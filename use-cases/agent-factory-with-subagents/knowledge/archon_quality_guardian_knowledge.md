# Archon Quality Guardian Knowledge Base

## –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Archon Quality Guardian

```
–¢—ã –≥–ª–∞–≤–Ω—ã–π —Å—Ç—Ä–∞–∂ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–º–∞–Ω–¥—ã Archon - —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç –ø–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—é –≤—ã—Å–æ—á–∞–π—à–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞, —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ production-ready —Ä–µ—à–µ–Ω–∏–π. –¢–≤–æ—è –º–∏—Å—Å–∏—è - –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑—É–ø—Ä–µ—á–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø—Ä–æ–¥—É–∫—Ç–∞.

**–¢–≤–æ—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞:**
- Comprehensive testing strategies (Unit, Integration, E2E, Performance)
- Code review –∏ static analysis
- Quality assurance –ø—Ä–æ—Ü–µ—Å—Å—ã –∏ –º–µ—Ç—Ä–∏–∫–∏
- Test automation –∏ CI/CD integration
- Performance testing –∏ optimization
- Security testing –∏ vulnerability assessment
- Documentation quality –∏ completeness

**–ö–ª—é—á–µ–≤—ã–µ –æ–±–ª–∞—Å—Ç–∏ –∫–∞—á–µ—Å—Ç–≤–∞:**

1. **Testing Excellence:**
   - Pytest, Jest, Playwright –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
   - Test-driven development (TDD) practices
   - Behavior-driven development (BDD)
   - Property-based testing –∏ fuzzing

2. **Code Quality Assurance:**
   - Static analysis (mypy, ESLint, SonarQube)
   - Code coverage analysis
   - Code complexity metrics
   - Architectural compliance checking

3. **Performance & Reliability:**
   - Load testing —Å k6, Artillery, JMeter
   - Memory profiling –∏ leak detection
   - Chaos engineering principles
   - SLA/SLO monitoring

4. **Security Quality:**
   - SAST/DAST security scanning
   - Dependency vulnerability scanning
   - Penetration testing coordination
   - Secure code review practices

**–ü–æ–¥—Ö–æ–¥ –∫ —Ä–∞–±–æ—Ç–µ:**
1. Quality-first mindset - –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–∫–ª–∞–¥—ã–≤–∞–µ—Ç—Å—è —Å –Ω–∞—á–∞–ª–∞
2. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –ø—Ä–æ–≤–µ—Ä–æ–∫
3. –ò–∑–º–µ—Ä–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (95%+ coverage, 0 critical issues)
4. –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
5. –û–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã best practices
```

## Testing Strategies & Frameworks

### Comprehensive Testing Pyramid
```python
import pytest
import asyncio
from unittest.mock import AsyncMock, MagicMock, patch
from pydantic_ai.models.test import TestModel
from pydantic_ai import Agent, RunContext
import hypothesis
from hypothesis import strategies as st
import time

class TestingFramework:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""

    # Unit Tests (Base of pyramid - 70%)
    class UnitTests:
        """Unit —Ç–µ—Å—Ç—ã –¥–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""

        @pytest.fixture
        def mock_dependencies(self):
            """–ú–æ–∫–∞–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏."""
            return {
                'database': AsyncMock(),
                'cache': AsyncMock(),
                'external_api': AsyncMock(),
                'logger': MagicMock()
            }

        @pytest.mark.asyncio
        async def test_agent_tool_functionality(self, mock_dependencies):
            """–¢–µ—Å—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –∞–≥–µ–Ω—Ç–∞."""
            from your_app.tools import search_knowledge

            # Arrange
            mock_context = MagicMock()
            mock_context.deps = mock_dependencies['database']
            mock_dependencies['database'].search.return_value = [
                {"content": "test result", "score": 0.95}
            ]

            # Act
            result = await search_knowledge(mock_context, "test query")

            # Assert
            assert "test result" in result
            mock_dependencies['database'].search.assert_called_once_with("test query")

        @hypothesis.given(st.text(min_size=1, max_size=100))
        def test_input_validation_property_based(self, input_text):
            """Property-based —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
            from your_app.validation import validate_input

            # Property: –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–æ–ª–∂–Ω–∞ –ª–∏–±–æ –ø—Ä–æ–π—Ç–∏, –ª–∏–±–æ –≤—ã–±—Ä–æ—Å–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            try:
                result = validate_input(input_text)
                assert isinstance(result, str)
                assert len(result) > 0
            except ValueError as e:
                assert str(e)  # –û—à–∏–±–∫–∞ –¥–æ–ª–∂–Ω–∞ –∏–º–µ—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ

    # Integration Tests (Middle of pyramid - 20%)
    class IntegrationTests:
        """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤."""

        @pytest.mark.integration
        @pytest.mark.asyncio
        async def test_agent_with_real_database(self, test_database):
            """–¢–µ—Å—Ç –∞–≥–µ–Ω—Ç–∞ —Å —Ä–µ–∞–ª—å–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""
            from your_app.agent import create_agent
            from your_app.dependencies import AgentDependencies

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
            deps = AgentDependencies(database_url=test_database.url)
            await deps.initialize()

            try:
                agent = create_agent(deps)
                result = await agent.run("Find information about testing", deps=deps)

                assert result.data is not None
                assert isinstance(result.data, str)
                assert len(result.data) > 0

            finally:
                await deps.cleanup()

        @pytest.mark.integration
        async def test_external_api_integration(self):
            """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å –≤–Ω–µ—à–Ω–∏–º–∏ API."""
            import aiohttp

            async with aiohttp.ClientSession() as session:
                # –¢–µ—Å—Ç —Å —Ä–µ–∞–ª—å–Ω—ã–º API endpoint
                response = await session.get("https://api.external-service.com/health")
                assert response.status == 200

                data = await response.json()
                assert "status" in data
                assert data["status"] == "healthy"

    # E2E Tests (Top of pyramid - 10%)
    class EndToEndTests:
        """End-to-end —Ç–µ—Å—Ç—ã –ø–æ–ª–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤."""

        @pytest.mark.e2e
        @pytest.mark.asyncio
        async def test_complete_user_workflow(self, test_client):
            """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ workflow."""
            # 1. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å
            response = await test_client.post("/api/agent/query", json={
                "query": "Analyze the latest market trends",
                "user_id": "test_user_123"
            })

            assert response.status_code == 200
            data = response.json()

            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
            assert "response" in data
            assert "metadata" in data
            assert data["metadata"]["status"] == "success"

            # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏–∏
            history_response = await test_client.get(
                "/api/user/test_user_123/history"
            )
            assert history_response.status_code == 200
            history = history_response.json()
            assert len(history["queries"]) > 0

        @pytest.mark.e2e
        async def test_error_handling_workflow(self, test_client):
            """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫ –≤ –ø–æ–ª–Ω–æ–º workflow."""
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –∑–∞–ø—Ä–æ—Å
            response = await test_client.post("/api/agent/query", json={
                "query": "",  # –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
                "user_id": "test_user_123"
            })

            assert response.status_code == 400
            data = response.json()
            assert "error" in data
            assert "validation" in data["error"]["type"]
```

### Performance Testing Framework
```python
import asyncio
import time
import psutil
import os
from typing import List, Dict, Any
import pytest
from concurrent.futures import ThreadPoolExecutor

class PerformanceTestSuite:
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""

    @pytest.mark.performance
    async def test_response_time_sla(self, test_agent):
        """–¢–µ—Å—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è SLA –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞."""
        agent, deps = test_agent

        response_times = []

        # –í—ã–ø–æ–ª–Ω—è–µ–º 100 –∑–∞–ø—Ä–æ—Å–æ–≤
        for i in range(100):
            start_time = time.time()
            await agent.run(f"Test query {i}", deps=deps)
            end_time = time.time()

            response_times.append(end_time - start_time)

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        avg_response_time = sum(response_times) / len(response_times)
        p95_response_time = sorted(response_times)[int(0.95 * len(response_times))]
        p99_response_time = sorted(response_times)[int(0.99 * len(response_times))]

        # SLA —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
        assert avg_response_time < 1.0, f"Average response time {avg_response_time:.2f}s exceeds 1s SLA"
        assert p95_response_time < 2.0, f"95th percentile {p95_response_time:.2f}s exceeds 2s SLA"
        assert p99_response_time < 5.0, f"99th percentile {p99_response_time:.2f}s exceeds 5s SLA"

    @pytest.mark.performance
    async def test_concurrent_load_handling(self, test_agent):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏."""
        agent, deps = test_agent

        async def single_request(request_id: int):
            """–û–¥–∏–Ω–æ—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º ID."""
            return await agent.run(f"Concurrent request {request_id}", deps=deps)

        # –¢–µ—Å—Ç —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        concurrency_levels = [10, 50, 100, 200]

        for concurrency in concurrency_levels:
            start_time = time.time()

            # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏
            tasks = [single_request(i) for i in range(concurrency)]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            end_time = time.time()
            total_time = end_time - start_time

            # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            successful_requests = sum(1 for r in results if not isinstance(r, Exception))
            error_rate = (concurrency - successful_requests) / concurrency * 100
            throughput = successful_requests / total_time

            print(f"Concurrency: {concurrency}, Success Rate: {successful_requests}/{concurrency}, "
                  f"Error Rate: {error_rate:.1f}%, Throughput: {throughput:.1f} req/s")

            # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            assert error_rate < 5.0, f"Error rate {error_rate:.1f}% too high for concurrency {concurrency}"
            assert throughput > concurrency * 0.5, f"Throughput {throughput:.1f} too low for concurrency {concurrency}"

    @pytest.mark.performance
    async def test_memory_usage_stability(self, test_agent):
        """–¢–µ—Å—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏."""
        agent, deps = test_agent

        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        memory_measurements = [initial_memory]

        # –í—ã–ø–æ–ª–Ω—è–µ–º 1000 –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
        for i in range(1000):
            await agent.run(f"Memory test query {i}", deps=deps)

            if i % 100 == 0:  # –ò–∑–º–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 100 –∑–∞–ø—Ä–æ—Å–æ–≤
                current_memory = process.memory_info().rss / 1024 / 1024
                memory_measurements.append(current_memory)

        # –ê–Ω–∞–ª–∏–∑ —É—Ç–µ—á–µ–∫ –ø–∞–º—è—Ç–∏
        memory_growth = memory_measurements[-1] - memory_measurements[0]
        max_memory = max(memory_measurements)

        print(f"Memory usage: Initial: {initial_memory:.1f}MB, "
              f"Final: {memory_measurements[-1]:.1f}MB, "
              f"Growth: {memory_growth:.1f}MB, Max: {max_memory:.1f}MB")

        # –ü—Ä–æ–≤–µ—Ä–∫–∏
        assert memory_growth < 100, f"Memory growth {memory_growth:.1f}MB indicates memory leak"
        assert max_memory < initial_memory + 200, f"Peak memory usage {max_memory:.1f}MB too high"

    @pytest.mark.performance
    async def test_database_query_performance(self, test_database):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
        from your_app.database import DatabaseService

        db_service = DatabaseService(test_database.url)
        await db_service.initialize()

        try:
            # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            start_time = time.time()
            for i in range(100):
                await db_service.get_user_by_id(f"user_{i}")
            simple_query_time = (time.time() - start_time) / 100

            # –¢–µ—Å—Ç —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            start_time = time.time()
            for i in range(10):
                await db_service.get_user_analytics(f"user_{i}")
            complex_query_time = (time.time() - start_time) / 10

            print(f"DB Performance: Simple queries: {simple_query_time*1000:.1f}ms, "
                  f"Complex queries: {complex_query_time*1000:.1f}ms")

            # SLA –ø—Ä–æ–≤–µ—Ä–∫–∏
            assert simple_query_time < 0.01, f"Simple query time {simple_query_time*1000:.1f}ms exceeds 10ms SLA"
            assert complex_query_time < 0.1, f"Complex query time {complex_query_time*1000:.1f}ms exceeds 100ms SLA"

        finally:
            await db_service.cleanup()
```

## Code Quality Analysis

### Static Analysis & Code Metrics
```python
import ast
import subprocess
import json
from typing import Dict, List, Any
from pathlib import Path

class CodeQualityAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞."""

    def __init__(self, project_path: str):
        self.project_path = Path(project_path)

    def run_mypy_analysis(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ mypy –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ç–∏–ø–æ–≤."""
        result = subprocess.run([
            "mypy", "--json-report", "/tmp/mypy-report",
            str(self.project_path)
        ], capture_output=True, text=True)

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        error_count = result.stdout.count('"severity": "error"')
        warning_count = result.stdout.count('"severity": "warning"')

        return {
            "tool": "mypy",
            "errors": error_count,
            "warnings": warning_count,
            "status": "passed" if error_count == 0 else "failed",
            "details": result.stdout
        }

    def run_pylint_analysis(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ pylint –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞."""
        result = subprocess.run([
            "pylint", "--output-format=json",
            str(self.project_path)
        ], capture_output=True, text=True)

        try:
            pylint_data = json.loads(result.stdout)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Å–æ–æ–±—â–µ–Ω–∏–π
            messages_by_type = {}
            for message in pylint_data:
                msg_type = message.get("type", "unknown")
                if msg_type not in messages_by_type:
                    messages_by_type[msg_type] = 0
                messages_by_type[msg_type] += 1

            return {
                "tool": "pylint",
                "messages": messages_by_type,
                "total_issues": len(pylint_data),
                "status": "passed" if len(pylint_data) == 0 else "review_needed",
                "details": pylint_data[:10]  # –ü–µ—Ä–≤—ã–µ 10 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
            }
        except json.JSONDecodeError:
            return {
                "tool": "pylint",
                "status": "error",
                "error": "Could not parse pylint output"
            }

    def analyze_code_complexity(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞."""
        complexity_data = []

        for py_file in self.project_path.rglob("*.py"):
            if "test_" in py_file.name or py_file.name.startswith("test_"):
                continue

            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    tree = ast.parse(f.read())

                file_complexity = self._calculate_cyclomatic_complexity(tree)
                complexity_data.append({
                    "file": str(py_file.relative_to(self.project_path)),
                    "complexity": file_complexity
                })
            except Exception as e:
                print(f"Error analyzing {py_file}: {e}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        complexities = [item["complexity"] for item in complexity_data]
        avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        max_complexity = max(complexities) if complexities else 0

        return {
            "tool": "complexity_analyzer",
            "average_complexity": avg_complexity,
            "max_complexity": max_complexity,
            "files_analyzed": len(complexity_data),
            "high_complexity_files": [
                item for item in complexity_data if item["complexity"] > 10
            ],
            "status": "passed" if max_complexity <= 15 else "review_needed"
        }

    def _calculate_cyclomatic_complexity(self, tree: ast.AST) -> int:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ü–∏–∫–ª–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏."""
        complexity = 1  # –ë–∞–∑–æ–≤–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å

        for node in ast.walk(tree):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(node, ast.ExceptHandler):
                complexity += 1
            elif isinstance(node, (ast.And, ast.Or)):
                complexity += 1

        return complexity

    def run_security_scan(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å bandit."""
        result = subprocess.run([
            "bandit", "-r", str(self.project_path),
            "-f", "json", "-o", "/tmp/bandit-report.json"
        ], capture_output=True, text=True)

        try:
            with open("/tmp/bandit-report.json", 'r') as f:
                bandit_data = json.load(f)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç–∏
            severity_counts = {"LOW": 0, "MEDIUM": 0, "HIGH": 0}
            for result in bandit_data.get("results", []):
                severity = result.get("issue_severity", "UNKNOWN")
                if severity in severity_counts:
                    severity_counts[severity] += 1

            return {
                "tool": "bandit",
                "severity_counts": severity_counts,
                "total_issues": len(bandit_data.get("results", [])),
                "critical_issues": severity_counts["HIGH"],
                "status": "passed" if severity_counts["HIGH"] == 0 else "failed"
            }
        except Exception as e:
            return {
                "tool": "bandit",
                "status": "error",
                "error": str(e)
            }

    def analyze_test_coverage(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ—Å—Ç–∞–º–∏."""
        # –ó–∞–ø—É—Å–∫ pytest —Å coverage
        result = subprocess.run([
            "pytest", "--cov=.", "--cov-report=json:/tmp/coverage.json",
            str(self.project_path / "tests")
        ], capture_output=True, text=True)

        try:
            with open("/tmp/coverage.json", 'r') as f:
                coverage_data = json.load(f)

            total_coverage = coverage_data["totals"]["percent_covered"]

            # –§–∞–π–ª—ã —Å –Ω–∏–∑–∫–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º
            low_coverage_files = []
            for filename, file_data in coverage_data["files"].items():
                if file_data["summary"]["percent_covered"] < 80:
                    low_coverage_files.append({
                        "file": filename,
                        "coverage": file_data["summary"]["percent_covered"]
                    })

            return {
                "tool": "coverage",
                "total_coverage": total_coverage,
                "line_coverage": coverage_data["totals"]["percent_covered_display"],
                "low_coverage_files": low_coverage_files,
                "status": "passed" if total_coverage >= 80 else "review_needed"
            }
        except Exception as e:
            return {
                "tool": "coverage",
                "status": "error",
                "error": str(e)
            }

    def generate_quality_report(self) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ."""
        return {
            "timestamp": time.time(),
            "project_path": str(self.project_path),
            "analyses": {
                "type_checking": self.run_mypy_analysis(),
                "code_quality": self.run_pylint_analysis(),
                "complexity": self.analyze_code_complexity(),
                "security": self.run_security_scan(),
                "test_coverage": self.analyze_test_coverage()
            }
        }
```

### Automated Code Review
```python
import re
from typing import List, Dict, Any, Tuple
from pathlib import Path

class AutomatedCodeReviewer:
    """–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–¥ —Ä–µ–≤—å—é–µ—Ä."""

    def __init__(self):
        self.review_rules = {
            "naming_conventions": self._check_naming_conventions,
            "function_length": self._check_function_length,
            "import_organization": self._check_import_organization,
            "documentation": self._check_documentation,
            "error_handling": self._check_error_handling,
            "performance_patterns": self._check_performance_patterns
        }

    def review_file(self, file_path: Path) -> Dict[str, Any]:
        """–†–µ–≤—å—é –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞."""
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        issues = []
        suggestions = []

        for rule_name, rule_func in self.review_rules.items():
            rule_issues, rule_suggestions = rule_func(content, file_path)
            issues.extend(rule_issues)
            suggestions.extend(rule_suggestions)

        return {
            "file": str(file_path),
            "issues": issues,
            "suggestions": suggestions,
            "score": self._calculate_file_score(issues)
        }

    def _check_naming_conventions(self, content: str, file_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—à–µ–Ω–∏–π –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è."""
        issues = []
        suggestions = []

        lines = content.split('\n')

        for i, line in enumerate(lines, 1):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Å–æ–≤ (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ PascalCase)
            class_match = re.search(r'class\s+([a-z_][a-zA-Z0-9_]*)', line)
            if class_match:
                class_name = class_match.group(1)
                if not class_name[0].isupper():
                    issues.append({
                        "type": "naming_convention",
                        "severity": "medium",
                        "line": i,
                        "message": f"Class '{class_name}' should be in PascalCase",
                        "suggestion": f"Rename to '{self._to_pascal_case(class_name)}'"
                    })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ—É–Ω–∫—Ü–∏–π (–¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ snake_case)
            func_match = re.search(r'def\s+([A-Z][a-zA-Z0-9_]*)', line)
            if func_match:
                func_name = func_match.group(1)
                issues.append({
                    "type": "naming_convention",
                    "severity": "medium",
                    "line": i,
                    "message": f"Function '{func_name}' should be in snake_case",
                    "suggestion": f"Rename to '{self._to_snake_case(func_name)}'"
                })

        return issues, suggestions

    def _check_function_length(self, content: str, file_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã —Ñ—É–Ω–∫—Ü–∏–π."""
        issues = []
        suggestions = []

        lines = content.split('\n')
        in_function = False
        function_start = 0
        function_name = ""

        for i, line in enumerate(lines, 1):
            if re.match(r'\s*def\s+', line):
                in_function = True
                function_start = i
                function_name = re.search(r'def\s+(\w+)', line).group(1)
            elif in_function and not line.startswith(' ') and not line.startswith('\t') and line.strip():
                # –ö–æ–Ω–µ—Ü —Ñ—É–Ω–∫—Ü–∏–∏
                function_length = i - function_start
                if function_length > 50:
                    issues.append({
                        "type": "function_length",
                        "severity": "high" if function_length > 100 else "medium",
                        "line": function_start,
                        "message": f"Function '{function_name}' is too long ({function_length} lines)",
                        "suggestion": "Consider breaking this function into smaller functions"
                    })
                in_function = False

        return issues, suggestions

    def _check_documentation(self, content: str, file_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏."""
        issues = []
        suggestions = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ docstrings –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–π
        function_pattern = r'def\s+(\w+)\s*\([^)]*\):'
        functions = re.finditer(function_pattern, content)

        for func_match in functions:
            func_name = func_match.group(1)
            func_start = func_match.start()

            # –ü–æ–∏—Å–∫ docstring –ø–æ—Å–ª–µ —Ñ—É–Ω–∫—Ü–∏–∏
            remaining_content = content[func_match.end():]
            docstring_match = re.search(r'^\s*""".*?"""', remaining_content, re.DOTALL | re.MULTILINE)

            if not docstring_match and not func_name.startswith('_'):
                line_num = content[:func_start].count('\n') + 1
                issues.append({
                    "type": "documentation",
                    "severity": "medium",
                    "line": line_num,
                    "message": f"Public function '{func_name}' lacks docstring",
                    "suggestion": "Add a descriptive docstring explaining the function's purpose"
                })

        return issues, suggestions

    def _check_error_handling(self, content: str, file_path: Path) -> Tuple[List[Dict], List[Dict]]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–æ–∫."""
        issues = []
        suggestions = []

        lines = content.split('\n')

        for i, line in enumerate(lines, 1):
            # Bare except clauses
            if re.search(r'except\s*:', line):
                issues.append({
                    "type": "error_handling",
                    "severity": "high",
                    "line": i,
                    "message": "Bare 'except:' clause catches all exceptions",
                    "suggestion": "Specify the exception type or use 'except Exception:'"
                })

            # Exception –±–µ–∑ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            if 'except' in line and 'pass' in lines[i] if i < len(lines) else False:
                issues.append({
                    "type": "error_handling",
                    "severity": "medium",
                    "line": i,
                    "message": "Exception silently ignored",
                    "suggestion": "Add logging or proper error handling"
                })

        return issues, suggestions

    def _calculate_file_score(self, issues: List[Dict]) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Ñ–∞–π–ª–∞."""
        score = 100.0

        for issue in issues:
            if issue["severity"] == "high":
                score -= 10
            elif issue["severity"] == "medium":
                score -= 5
            elif issue["severity"] == "low":
                score -= 2

        return max(0.0, score)

    def _to_pascal_case(self, snake_str: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ PascalCase."""
        return ''.join(word.capitalize() for word in snake_str.split('_'))

    def _to_snake_case(self, pascal_str: str) -> str:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ snake_case."""
        return re.sub(r'(?<!^)(?=[A-Z])', '_', pascal_str).lower()
```

## Quality Metrics & Monitoring

### Comprehensive Quality Dashboard
```python
import json
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Any
import matplotlib.pyplot as plt
import pandas as pd

class QualityMetricsCollector:
    """–°–±–æ—Ä—â–∏–∫ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞."""

    def __init__(self, db_path: str = "quality_metrics.db"):
        self.db_path = db_path
        self.init_database()

    def init_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS quality_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                project_name TEXT,
                metric_type TEXT,
                metric_name TEXT,
                metric_value REAL,
                metadata TEXT
            )
        """)

        conn.commit()
        conn.close()

    def record_metric(self, project: str, metric_type: str, name: str, value: float, metadata: Dict = None):
        """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO quality_metrics (project_name, metric_type, metric_name, metric_value, metadata)
            VALUES (?, ?, ?, ?, ?)
        """, (project, metric_type, name, value, json.dumps(metadata or {})))

        conn.commit()
        conn.close()

    def collect_test_metrics(self, test_results: Dict) -> None:
        """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
        project = test_results.get("project", "unknown")

        # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        self.record_metric(project, "testing", "total_tests", test_results.get("total", 0))
        self.record_metric(project, "testing", "passed_tests", test_results.get("passed", 0))
        self.record_metric(project, "testing", "failed_tests", test_results.get("failed", 0))
        self.record_metric(project, "testing", "test_coverage", test_results.get("coverage", 0))

        # –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–µ—Å—Ç–æ–≤
        if "duration" in test_results:
            self.record_metric(project, "testing", "test_duration", test_results["duration"])

        # –ö–∞—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤
        pass_rate = (test_results.get("passed", 0) / test_results.get("total", 1)) * 100
        self.record_metric(project, "testing", "pass_rate", pass_rate)

    def collect_code_quality_metrics(self, analysis_results: Dict) -> None:
        """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–¥–∞."""
        project = analysis_results.get("project", "unknown")

        for analysis_type, results in analysis_results.get("analyses", {}).items():
            if analysis_type == "complexity":
                self.record_metric(project, "code_quality", "avg_complexity",
                                 results.get("average_complexity", 0))
                self.record_metric(project, "code_quality", "max_complexity",
                                 results.get("max_complexity", 0))

            elif analysis_type == "security":
                critical_issues = results.get("critical_issues", 0)
                self.record_metric(project, "security", "critical_issues", critical_issues)

            elif analysis_type == "test_coverage":
                coverage = results.get("total_coverage", 0)
                self.record_metric(project, "coverage", "line_coverage", coverage)

    def generate_quality_report(self, project: str, days: int = 30) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ."""
        conn = sqlite3.connect(self.db_path)

        # –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –¥–Ω–µ–π
        query = """
            SELECT metric_type, metric_name, metric_value, timestamp
            FROM quality_metrics
            WHERE project_name = ? AND timestamp >= datetime('now', '-{} days')
            ORDER BY timestamp DESC
        """.format(days)

        df = pd.read_sql_query(query, conn, params=(project,))
        conn.close()

        if df.empty:
            return {"error": "No data found for the specified period"}

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –º–µ—Ç—Ä–∏–∫
        report = {
            "project": project,
            "period_days": days,
            "generated_at": datetime.now().isoformat(),
            "metrics": {}
        }

        for metric_type in df['metric_type'].unique():
            type_data = df[df['metric_type'] == metric_type]

            metrics_summary = {}
            for metric_name in type_data['metric_name'].unique():
                metric_data = type_data[type_data['metric_name'] == metric_name]

                metrics_summary[metric_name] = {
                    "current_value": metric_data.iloc[0]['metric_value'],
                    "average": metric_data['metric_value'].mean(),
                    "min": metric_data['metric_value'].min(),
                    "max": metric_data['metric_value'].max(),
                    "trend": self._calculate_trend(metric_data['metric_value'].tolist()),
                    "data_points": len(metric_data)
                }

            report["metrics"][metric_type] = metrics_summary

        return report

    def _calculate_trend(self, values: List[float]) -> str:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ç—Ä–µ–Ω–¥–∞ –º–µ—Ç—Ä–∏–∫–∏."""
        if len(values) < 2:
            return "insufficient_data"

        recent_avg = sum(values[:len(values)//2]) / len(values[:len(values)//2])
        older_avg = sum(values[len(values)//2:]) / len(values[len(values)//2:])

        if recent_avg > older_avg * 1.05:
            return "improving"
        elif recent_avg < older_avg * 0.95:
            return "declining"
        else:
            return "stable"

    def generate_quality_charts(self, project: str, output_dir: str = "quality_charts"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞."""
        import os
        os.makedirs(output_dir, exist_ok=True)

        conn = sqlite3.connect(self.db_path)

        # –ì—Ä–∞—Ñ–∏–∫ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ—Å—Ç–∞–º–∏
        coverage_query = """
            SELECT DATE(timestamp) as date, metric_value as coverage
            FROM quality_metrics
            WHERE project_name = ? AND metric_name = 'line_coverage'
            ORDER BY timestamp
        """

        coverage_df = pd.read_sql_query(coverage_query, conn, params=(project,))

        if not coverage_df.empty:
            plt.figure(figsize=(12, 6))
            plt.plot(pd.to_datetime(coverage_df['date']), coverage_df['coverage'])
            plt.title(f'Test Coverage Trend - {project}')
            plt.xlabel('Date')
            plt.ylabel('Coverage %')
            plt.grid(True)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/coverage_trend.png')
            plt.close()

        # –ì—Ä–∞—Ñ–∏–∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞
        complexity_query = """
            SELECT DATE(timestamp) as date, metric_value as complexity
            FROM quality_metrics
            WHERE project_name = ? AND metric_name = 'avg_complexity'
            ORDER BY timestamp
        """

        complexity_df = pd.read_sql_query(complexity_query, conn, params=(project,))

        if not complexity_df.empty:
            plt.figure(figsize=(12, 6))
            plt.plot(pd.to_datetime(complexity_df['date']), complexity_df['complexity'])
            plt.title(f'Code Complexity Trend - {project}')
            plt.xlabel('Date')
            plt.ylabel('Average Complexity')
            plt.grid(True)
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f'{output_dir}/complexity_trend.png')
            plt.close()

        conn.close()
```

## Quality Gates & CI/CD Integration

### Quality Gates Configuration
```yaml
# .github/workflows/quality-gate.yml
name: Quality Gate

on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  quality-gate:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install poetry
        poetry install

    # Unit Tests
    - name: Run Unit Tests
      run: |
        poetry run pytest tests/unit --cov=. --cov-report=xml

    # Code Quality Checks
    - name: Type Checking (MyPy)
      run: |
        poetry run mypy .

    - name: Code Quality (Pylint)
      run: |
        poetry run pylint src/

    - name: Security Scan (Bandit)
      run: |
        poetry run bandit -r src/

    # Performance Tests
    - name: Performance Tests
      run: |
        poetry run pytest tests/performance -v

    # Quality Gate Evaluation
    - name: Evaluate Quality Gate
      run: |
        poetry run python scripts/quality_gate.py

    # Upload Results
    - name: Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml

    - name: Quality Gate Status
      if: failure()
      run: |
        echo "Quality gate failed! Check the logs above."
        exit 1
```

### Quality Gate Script
```python
#!/usr/bin/env python3
"""
Quality Gate evaluation script.
Checks if all quality metrics meet the required thresholds.
"""

import json
import sys
import subprocess
from typing import Dict, Any, Tuple

class QualityGate:
    """Quality Gate checker."""

    def __init__(self):
        self.thresholds = {
            "test_coverage": 80.0,          # –ú–∏–Ω–∏–º—É–º 80% –ø–æ–∫—Ä—ã—Ç–∏–µ
            "test_pass_rate": 100.0,        # 100% –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤
            "max_complexity": 15,           # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            "critical_security": 0,         # 0 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π
            "high_security": 3,             # –ú–∞–∫—Å–∏–º—É–º 3 –≤—ã—Å–æ–∫–∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–∏
            "type_errors": 0,               # 0 –æ—à–∏–±–æ–∫ —Ç–∏–ø–∏–∑–∞—Ü–∏–∏
            "performance_p95": 2000,        # P95 < 2 —Å–µ–∫—É–Ω–¥
        }

    def check_test_coverage(self) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ—Å—Ç–∞–º–∏."""
        try:
            result = subprocess.run([
                "coverage", "json", "--pretty-print"
            ], capture_output=True, text=True)

            coverage_data = json.loads(result.stdout)
            total_coverage = coverage_data["totals"]["percent_covered"]

            passed = total_coverage >= self.thresholds["test_coverage"]
            message = f"Test coverage: {total_coverage:.1f}% (threshold: {self.thresholds['test_coverage']}%)"

            return passed, message
        except Exception as e:
            return False, f"Failed to check test coverage: {e}"

    def check_type_errors(self) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—à–∏–±–æ–∫ —Ç–∏–ø–∏–∑–∞—Ü–∏–∏."""
        try:
            result = subprocess.run([
                "mypy", "--json-report", "/tmp/mypy-report", "."
            ], capture_output=True, text=True)

            error_count = result.stdout.count('"severity": "error"')
            passed = error_count <= self.thresholds["type_errors"]
            message = f"Type errors: {error_count} (threshold: {self.thresholds['type_errors']})"

            return passed, message
        except Exception as e:
            return False, f"Failed to check type errors: {e}"

    def check_security_issues(self) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—è–∑–≤–∏–º–æ—Å—Ç–µ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."""
        try:
            result = subprocess.run([
                "bandit", "-r", ".", "-f", "json"
            ], capture_output=True, text=True)

            if result.stdout:
                bandit_data = json.loads(result.stdout)
                results = bandit_data.get("results", [])

                critical_count = sum(1 for r in results if r.get("issue_severity") == "HIGH")
                high_count = sum(1 for r in results if r.get("issue_severity") == "MEDIUM")

                critical_passed = critical_count <= self.thresholds["critical_security"]
                high_passed = high_count <= self.thresholds["high_security"]

                passed = critical_passed and high_passed
                message = f"Security issues: {critical_count} critical, {high_count} high"

                return passed, message
            else:
                return True, "No security issues found"
        except Exception as e:
            return False, f"Failed to check security issues: {e}"

    def check_performance(self) -> Tuple[bool, str]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
        try:
            # –ó–∞–ø—É—Å–∫ performance —Ç–µ—Å—Ç–æ–≤
            result = subprocess.run([
                "pytest", "tests/performance", "--json-report", "--json-report-file=/tmp/performance.json"
            ], capture_output=True, text=True)

            if result.returncode == 0:
                return True, "Performance tests passed"
            else:
                return False, "Performance tests failed"
        except Exception as e:
            return False, f"Failed to check performance: {e}"

    def evaluate(self) -> bool:
        """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        checks = [
            ("Test Coverage", self.check_test_coverage),
            ("Type Checking", self.check_type_errors),
            ("Security", self.check_security_issues),
            ("Performance", self.check_performance),
        ]

        all_passed = True
        print("üîç Quality Gate Evaluation")
        print("=" * 50)

        for check_name, check_func in checks:
            passed, message = check_func()
            status = "‚úÖ PASS" if passed else "‚ùå FAIL"
            print(f"{check_name}: {status} - {message}")

            if not passed:
                all_passed = False

        print("=" * 50)

        if all_passed:
            print("üéâ Quality Gate: PASSED")
            return True
        else:
            print("üí• Quality Gate: FAILED")
            return False

if __name__ == "__main__":
    gate = QualityGate()
    success = gate.evaluate()
    sys.exit(0 if success else 1)
```

## Best Practices –¥–ª—è Quality Guardian

### 1. Testing Strategy
- **Test Pyramid**: 70% unit, 20% integration, 10% e2e
- **Coverage Target**: –ú–∏–Ω–∏–º—É–º 80%, —Å—Ç—Ä–µ–º–ª–µ–Ω–∏–µ –∫ 95%
- **Performance SLA**: P95 < 2s, P99 < 5s
- **Flaky Tests**: –ù–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤

### 2. Code Quality Standards
- **Zero Tolerance**: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ security issues
- **Type Safety**: 100% type coverage –¥–ª—è Python
- **Complexity**: –ú–∞–∫—Å–∏–º—É–º 15 cyclomatic complexity
- **Documentation**: Docstrings –¥–ª—è –≤—Å–µ—Ö –ø—É–±–ª–∏—á–Ω—ã—Ö API

### 3. Review Process
- **Automated First**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ–¥ human review
- **Blocking Issues**: Quality gate –±–ª–æ–∫–∏—Ä—É–µ—Ç merge
- **Continuous Monitoring**: –ü–æ—Å—Ç–æ—è–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞
- **Feedback Loop**: –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞–º

### 4. Quality Metrics Tracking
- **Daily Monitoring**: –ï–∂–µ–¥–Ω–µ–≤–Ω—ã–µ –æ—Ç—á–µ—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞
- **Trend Analysis**: –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
- **Team Training**: –û–±—É—á–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º
- **Process Improvement**: –ü–æ—Å—Ç–æ—è–Ω–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤