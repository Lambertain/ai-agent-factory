# Module 05: Monitoring & Observability

**–ù–∞–∑–∞–¥ –∫:** [Deployment Engineer Knowledge Base](../deployment_engineer_knowledge.md)

---

## üéØ –¢–†–ò–ì–ì–ï–†–ù–ê–Ø –°–ò–°–¢–ï–ú–ê - –ö–æ–≥–¥–∞ —á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç –º–æ–¥—É–ª—å

### –¢–∏–ø 1: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (Keywords Triggers)
**–ß–∏—Ç–∞–π —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –ï–°–õ–ò –∑–∞–¥–∞—á–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç:**
- `prometheus`, `grafana`, `monitoring`, `observability`
- `metrics`, `alerts`, `alerting`, `alert rules`
- `tracing`, `distributed tracing`, `opentelemetry`, `jaeger`
- `histogram`, `gauge`, `counter`, `prometheus_client`
- `golden signals`, `sli`, `slo`, `service level`
- `dashboard`, `visualization`, `grafana dashboard`

### –¢–∏–ø 2: –°—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (Scenario Triggers)
**–ß–∏—Ç–∞–π —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –ö–û–ì–î–ê –Ω—É–∂–Ω–æ:**
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Prometheus –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
- –°–æ–∑–¥–∞—Ç—å alert rules –¥–ª—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Grafana dashboard –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∞–º–∏ (FastAPI)
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å distributed tracing (OpenTelemetry + Jaeger)
- –°–æ–∑–¥–∞—Ç—å SLI/SLO –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Golden Signals monitoring
- –°–æ–∑–¥–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –¥–ª—è infrastructure –∏ application
- –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Prometheus middleware –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ CI/CD pipeline

### –¢–∏–ø 3: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã (Technical Terms Triggers)
**–ß–∏—Ç–∞–π —ç—Ç–æ—Ç –º–æ–¥—É–ª—å –ï–°–õ–ò –≤—Å—Ç—Ä–µ—á–∞–µ—à—å:**
- Golden Signals (Latency, Traffic, Errors, Saturation)
- SLI/SLO (Service Level Indicators/Objectives)
- Prometheus exporters –∏ scrape_configs
- histogram_quantile –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ percentiles
- OpenTelemetry spans –∏ —Ç—Ä–µ–π—Å–∏–Ω–≥
- Jaeger distributed tracing
- Alert rules (rate, for, labels, annotations)
- Prometheus middleware –¥–ª—è FastAPI
- Grafana datasources –∏ provisioning
- Counter, Histogram, Gauge –º–µ—Ç—Ä–∏–∫–∏
- Alert fatigue prevention
- Node exporter –∏ Kubernetes service discovery

---

## üìã –°–û–î–ï–†–ñ–ê–ù–ò–ï –ú–û–î–£–õ–Ø

**–û—Å–Ω–æ–≤–Ω—ã–µ —Ç–µ–º—ã:**
1. ‚úÖ Prometheus Configuration (complete setup —Å Kubernetes service discovery)
2. ‚úÖ Alert Rules (application + infrastructure alerts —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–æ—Ä–æ–≥–∞–º–∏)
3. ‚úÖ Grafana Dashboards (JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫)
4. ‚úÖ Application Instrumentation (FastAPI metrics - Counter, Histogram, Gauge)
5. ‚úÖ Distributed Tracing (OpenTelemetry + Jaeger –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è)
6. ‚úÖ Best Practices (Golden Signals, SLI/SLO, Alert Fatigue Prevention)

**–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:**
- Complete Prometheus configuration (global, alerting, scrape_configs)
- Production alert rules (HighErrorRate, HighResponseTime, ApplicationDown, HighMemoryUsage)
- Infrastructure alerts (NodeHighCPU, NodeHighMemory, DiskSpaceLow, PodCrashLooping)
- Grafana dashboard JSON (Request Rate, Response Time, Error Rate, Memory, DB Connections)
- FastAPI Prometheus middleware (automatic metrics collection)

**–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:**
- Prometheus (metrics collection, alerting)
- Grafana (visualization, dashboards)
- OpenTelemetry (distributed tracing)
- Jaeger (trace visualization)
- prometheus_client (Python library)

---

## Prometheus Configuration

### Complete Prometheus Setup

```yaml
# ========================================
# prometheus.yml - Main Configuration
# ========================================
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'archon-prod'
    environment: 'production'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules once and periodically evaluate them
rule_files:
  - "alerts/*.yml"

# Scrape configurations
scrape_configs:
  # ========================================
  # Prometheus self-monitoring
  # ========================================
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # ========================================
  # Kubernetes API server
  # ========================================
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
      - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

  # ========================================
  # Kubernetes nodes
  # ========================================
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

  # ========================================
  # Kubernetes pods
  # ========================================
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

  # ========================================
  # Application metrics
  # ========================================
  - job_name: 'archon-agent'
    static_configs:
      - targets: ['archon-agent-service:8000']
    metrics_path: /metrics
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
        replacement: 'archon-agent'
```

---

## Alert Rules

### Production Alert Rules

```yaml
# ========================================
# alerts/application.yml
# ========================================
groups:
  - name: application_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

      # High response time
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95,
            rate(http_request_duration_seconds_bucket[5m])
          ) > 1
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s"

      # Application down
      - alert: ApplicationDown
        expr: up{job="archon-agent"} == 0
        for: 2m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "Application is down"
          description: "{{ $labels.instance }} has been down for more than 2 minutes"

      # High memory usage
      - alert: HighMemoryUsage
        expr: |
          (process_resident_memory_bytes / 1024 / 1024 / 1024) > 0.8
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High memory usage"
          description: "Memory usage is {{ $value | humanize }}GB (threshold: 800MB)"

      # Database connection pool exhausted
      - alert: DatabasePoolExhausted
        expr: |
          db_pool_connections_active / db_pool_connections_max > 0.9
        for: 5m
        labels:
          severity: warning
          component: database
        annotations:
          summary: "Database connection pool nearly exhausted"
          description: "{{ $value | humanizePercentage }} of connections in use"

# ========================================
# alerts/infrastructure.yml
# ========================================
groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Node high CPU
      - alert: NodeHighCPU
        expr: |
          100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High CPU usage on node"
          description: "CPU usage on {{ $labels.instance }} is {{ $value | humanize }}%"

      # Node high memory
      - alert: NodeHighMemory
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High memory usage on node"
          description: "Memory usage on {{ $labels.instance }} is {{ $value | humanize }}%"

      # Disk space low
      - alert: DiskSpaceLow
        expr: |
          (node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 < 15
        for: 5m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "Low disk space"
          description: "{{ $labels.instance }}:{{ $labels.mountpoint }} has {{ $value | humanize }}% free"

      # Pod crash looping
      - alert: PodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
          component: kubernetes
        annotations:
          summary: "Pod is crash looping"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
```

---

## Grafana Dashboards

### Application Dashboard JSON

```json
{
  "dashboard": {
    "title": "Archon Agent Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{ method }} {{ status }}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Response Time (95th percentile)",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "{{ path }}"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m])",
            "legendFormat": "Error Rate"
          }
        ]
      },
      {
        "id": 4,
        "title": "Memory Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "process_resident_memory_bytes / 1024 / 1024",
            "legendFormat": "Memory (MB)"
          }
        ]
      },
      {
        "id": 5,
        "title": "Active Database Connections",
        "type": "graph",
        "targets": [
          {
            "expr": "db_pool_connections_active",
            "legendFormat": "Active Connections"
          }
        ]
      }
    ]
  }
}
```

---

## Application Instrumentation

### FastAPI Metrics Example

```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import FastAPI, Request, Response
import time

# ========================================
# Metrics Definitions
# ========================================
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

ACTIVE_REQUESTS = Gauge(
    'http_requests_active',
    'Active HTTP requests',
    ['method', 'endpoint']
)

DB_CONNECTION_POOL = Gauge(
    'db_pool_connections_active',
    'Active database connections'
)

DB_CONNECTION_POOL_MAX = Gauge(
    'db_pool_connections_max',
    'Maximum database connections'
)

# ========================================
# Middleware –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫
# ========================================
@app.middleware("http")
async def prometheus_middleware(request: Request, call_next):
    """Middleware –¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ –ø–æ HTTP –∑–∞–ø—Ä–æ—Å–∞–º."""
    method = request.method
    endpoint = request.url.path

    # –£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    ACTIVE_REQUESTS.labels(method=method, endpoint=endpoint).inc()

    # –ù–∞—á–∞—Ç—å –æ—Ç—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏
    start_time = time.time()

    try:
        # –í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å
        response = await call_next(request)
        status = response.status_code
    except Exception as e:
        status = 500
        raise
    finally:
        # –£–º–µ–Ω—å—à–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        ACTIVE_REQUESTS.labels(method=method, endpoint=endpoint).dec()

        # –ó–∞–ø–∏—Å–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
        duration = time.time() - start_time
        REQUEST_COUNT.labels(
            method=method,
            endpoint=endpoint,
            status=status
        ).inc()
        REQUEST_DURATION.labels(
            method=method,
            endpoint=endpoint
        ).observe(duration)

    return response

# ========================================
# Endpoint –¥–ª—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –º–µ—Ç—Ä–∏–∫
# ========================================
@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint."""
    # –û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
    pool = app.state.db_pool
    DB_CONNECTION_POOL.set(pool.get_active_connections())
    DB_CONNECTION_POOL_MAX.set(pool.max_connections)

    return Response(
        content=generate_latest(),
        media_type="text/plain"
    )

# ========================================
# Custom –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
# ========================================
AGENT_TASKS_COMPLETED = Counter(
    'agent_tasks_completed_total',
    'Total completed agent tasks',
    ['agent_type', 'status']
)

AGENT_TASK_DURATION = Histogram(
    'agent_task_duration_seconds',
    'Agent task execution duration',
    ['agent_type']
)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ
@app.post("/tasks")
async def execute_task(task: Task):
    start_time = time.time()
    try:
        result = await agent.execute(task)
        AGENT_TASKS_COMPLETED.labels(
            agent_type=task.agent_type,
            status="success"
        ).inc()
        return result
    except Exception as e:
        AGENT_TASKS_COMPLETED.labels(
            agent_type=task.agent_type,
            status="error"
        ).inc()
        raise
    finally:
        duration = time.time() - start_time
        AGENT_TASK_DURATION.labels(
            agent_type=task.agent_type
        ).observe(duration)
```

---

## Distributed Tracing

### OpenTelemetry Integration

```python
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor

# ========================================
# Setup OpenTelemetry
# ========================================
def setup_tracing(app: FastAPI):
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —Ç—Ä–µ–π—Å–∏–Ω–≥–∞."""
    # –°–æ–∑–¥–∞—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Ç—Ä–µ–π—Å–æ–≤
    trace.set_tracer_provider(TracerProvider())

    # –°–æ–∑–¥–∞—Ç—å Jaeger —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
    jaeger_exporter = JaegerExporter(
        agent_host_name="jaeger",
        agent_port=6831,
    )

    # –î–æ–±–∞–≤–∏—Ç—å —ç–∫—Å–ø–æ—Ä—Ç–µ—Ä
    trace.get_tracer_provider().add_span_processor(
        BatchSpanProcessor(jaeger_exporter)
    )

    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å FastAPI
    FastAPIInstrumentor.instrument_app(app)

# ========================================
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ
# ========================================
tracer = trace.get_tracer(__name__)

async def process_with_tracing(data: dict):
    """–§—É–Ω–∫—Ü–∏—è —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º."""
    with tracer.start_as_current_span("process_data") as span:
        # –î–æ–±–∞–≤–∏—Ç—å –∞—Ç—Ä–∏–±—É—Ç—ã
        span.set_attribute("data.size", len(data))
        span.set_attribute("data.type", type(data).__name__)

        # –í–ª–æ–∂–µ–Ω–Ω—ã–π span –¥–ª—è –ø–æ–¥–æ–ø–µ—Ä–∞—Ü–∏–∏
        with tracer.start_as_current_span("validate_data"):
            validated = validate(data)

        # –ï—â–µ –æ–¥–∏–Ω –≤–ª–æ–∂–µ–Ω–Ω—ã–π span
        with tracer.start_as_current_span("transform_data"):
            result = transform(validated)

        return result
```

---

## Best Practices –¥–ª—è Monitoring

### 1. Golden Signals

**–ß–µ—Ç—ã—Ä–µ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**
```python
# Latency - –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
REQUEST_LATENCY = Histogram(
    'request_latency_seconds',
    'Request latency',
    buckets=[0.01, 0.05, 0.1, 0.5, 1, 2, 5]
)

# Traffic - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_RATE = Counter(
    'requests_total',
    'Total requests'
)

# Errors - –æ—à–∏–±–∫–∏
ERROR_RATE = Counter(
    'errors_total',
    'Total errors',
    ['type']
)

# Saturation - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
RESOURCE_USAGE = Gauge(
    'resource_usage_percent',
    'Resource usage percentage',
    ['resource_type']
)
```

### 2. SLI/SLO Monitoring

**Service Level Indicators:**
```yaml
# SLI: 95% –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è < 500ms
- record: sli:request_latency:ratio
  expr: |
    histogram_quantile(0.95,
      rate(http_request_duration_seconds_bucket[5m])
    ) < 0.5

# SLO: 99.9% uptime
- record: slo:availability:ratio
  expr: |
    sum(rate(up[30d])) / count(up) >= 0.999
```

### 3. Alert Fatigue Prevention

**–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–æ—Ä–æ–≥–∏:**
```yaml
# ‚ùå –ü–õ–û–•–û - —Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–π
- alert: HighCPU
  expr: cpu_usage > 50
  for: 1m

# ‚úÖ –•–û–†–û–®–û - —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- alert: HighCPU
  expr: cpu_usage > 80
  for: 10m
```

---

**–ù–∞–≤–∏–≥–∞—Ü–∏—è:**
- [‚Üê –ü—Ä–µ–¥—ã–¥—É—â–∏–π –º–æ–¥—É–ª—å: Infrastructure as Code](04_infrastructure_as_code.md)
- [‚Üë –ù–∞–∑–∞–¥ –∫ Deployment Engineer Knowledge Base](../deployment_engineer_knowledge.md)
- [‚Üí –°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å: Security & Best Practices](06_security_best_practices.md)
