"""
Tools for Psychology Content Orchestrator Agent
Ð˜Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ð´Ð»Ñ ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸Ð¸ Ð¸ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°
"""

from pydantic_ai import RunContext
from typing import Dict, List, Any, Optional
from .dependencies import OrchestratorDependencies
import json
import asyncio

# ÐžÐ¡ÐÐžÐ’ÐÐ«Ð• Ð˜ÐÐ¡Ð¢Ð Ð£ÐœÐ•ÐÐ¢Ð« ÐžÐ ÐšÐ•Ð¡Ð¢Ð ÐÐ¦Ð˜Ð˜

async def orchestrate_test_creation(
    ctx: RunContext[OrchestratorDependencies],
    project_name: str,
    test_topics: List[str],
    complexity_level: str = "standard"  # simple, standard, advanced, expert
) -> str:
    """
    ÐŸÐ¾Ð»Ð½Ð°Ñ Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ‚ÐµÑÑ‚Ð¾Ð²

    ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ð²ÐµÑÑŒ Ð¿Ð°Ð¹Ð¿Ð»Ð°Ð¹Ð½ Ð¾Ñ‚ Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ñ Ð´Ð¾ Ð³Ð¾Ñ‚Ð¾Ð²Ñ‹Ñ… Ñ‚ÐµÑÑ‚Ð¾Ð²:
    1. Psychology Research Agent - Ð¸ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ‚ÐµÐ¼Ñ‹
    2. Psychology Content Architect - ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð¾Ð²
    3. Psychology Test Generator - Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ð¾Ð²
    4. Psychology Quality Guardian - ÐºÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
    5. Psychology Transformation Planner - Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
    """
    try:
        orchestration_log = []
        created_deliverables = {}

        # Ð­Ñ‚Ð°Ð¿ 1: ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸
        planning_result = await plan_orchestration_workflow(
            ctx,
            project_name=project_name,
            test_topics=test_topics,
            complexity_level=complexity_level
        )
        orchestration_log.append(f"âœ… ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾: {planning_result}")

        # Ð­Ñ‚Ð°Ð¿ 2: Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ (ÐµÑÐ»Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ)
        if complexity_level in ["advanced", "expert"]:
            research_result = await delegate_to_specialist(
                ctx,
                agent_type="psychology_research_agent",
                task_title=f"Research for {project_name}",
                task_description=f"Comprehensive research for topics: {', '.join(test_topics)}",
                priority="high"
            )
            orchestration_log.append(f"âœ… Ð˜ÑÑÐ»ÐµÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ: {research_result}")
            created_deliverables["research"] = "Comprehensive research completed"

        # Ð­Ñ‚Ð°Ð¿ 3: Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ñ‚ÐµÑÑ‚Ð¾Ð² (Psychology Content Architect)
        architect_result = await delegate_to_specialist(
            ctx,
            agent_type="psychology_content_architect",
            task_title=f"Create tests for {project_name}",
            task_description=f"Create PatternShift methodology tests for: {', '.join(test_topics)}. Complexity: {complexity_level}",
            priority="high",
            context_data={
                "project_name": project_name,
                "test_topics": test_topics,
                "complexity_level": complexity_level,
                "methodology": "patternshift_full"
            }
        )
        orchestration_log.append(f"âœ… Ð¢ÐµÑÑ‚Ñ‹ ÑÐ¾Ð·Ð´Ð°Ð½Ñ‹: {architect_result}")
        created_deliverables["tests"] = f"Tests created for {len(test_topics)} topics"

        # Ð­Ñ‚Ð°Ð¿ 4: Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ð¾Ð² (Psychology Test Generator)
        generator_result = await delegate_to_specialist(
            ctx,
            agent_type="psychology_test_generator",
            task_title=f"Generate instances for {project_name}",
            task_description=f"Generate specific test instances based on created tests",
            priority="medium",
            context_data={
                "source_tests": "from_content_architect",
                "instance_count": len(test_topics) * 2,
                "target_language": ctx.deps.target_language
            }
        )
        orchestration_log.append(f"âœ… Ð­ÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€Ñ‹ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹: {generator_result}")
        created_deliverables["instances"] = "Test instances generated"

        # Ð­Ñ‚Ð°Ð¿ 5: ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° (Psychology Quality Guardian)
        quality_result = await delegate_to_specialist(
            ctx,
            agent_type="psychology_quality_guardian",
            task_title=f"Quality assurance for {project_name}",
            task_description="Comprehensive quality check of created tests and instances",
            priority="high",
            context_data={
                "validation_scope": "full_project",
                "quality_level": "comprehensive",
                "patternshift_compliance": True
            }
        )
        orchestration_log.append(f"âœ… ÐšÐ¾Ð½Ñ‚Ñ€Ð¾Ð»ÑŒ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°: {quality_result}")
        created_deliverables["quality_reports"] = "Quality validation completed"

        # Ð­Ñ‚Ð°Ð¿ 6: ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ (ÐµÑÐ»Ð¸ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ)
        if complexity_level in ["standard", "advanced", "expert"]:
            transformation_result = await delegate_to_specialist(
                ctx,
                agent_type="psychology_transformation_planner",
                task_title=f"Transformation programs for {project_name}",
                task_description="Create transformation and intervention programs based on test results",
                priority="medium",
                context_data={
                    "based_on_tests": test_topics,
                    "program_complexity": complexity_level,
                    "target_outcomes": "behavioral_change"
                }
            )
            orchestration_log.append(f"âœ… ÐŸÑ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸: {transformation_result}")
            created_deliverables["transformation_programs"] = "Transformation programs created"

        # Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ ÑÐ²Ð¾Ð´ÐºÐ°
        orchestration_summary = f"""
ðŸŽ¯ **ÐžÑ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: {project_name}**

ðŸ“Š **Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¾:**
{chr(10).join([f"- {key}: {value}" for key, value in created_deliverables.items()])}

ðŸ”„ **Ð­Ñ‚Ð°Ð¿Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ:**
{chr(10).join(orchestration_log)}

âœ… **ÐÐ³ÐµÐ½Ñ‚Ñ‹ Ð·Ð°Ð´ÐµÐ¹ÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ñ‹:** {len(orchestration_log)} ÑÑ‚Ð°Ð¿Ð¾Ð²
ðŸ“ˆ **Ð¡Ð»Ð¾Ð¶Ð½Ð¾ÑÑ‚ÑŒ:** {complexity_level}
ðŸŽ¯ **Ð¢ÐµÐ¼Ñ‹ Ñ‚ÐµÑÑ‚Ð¾Ð²:** {len(test_topics)}

ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾ÐµÐºÑ‚ Ð³Ð¾Ñ‚Ð¾Ð² Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÑŽ!
"""

        return orchestration_summary

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸: {e}"

async def coordinate_agent_workflow(
    ctx: RunContext[OrchestratorDependencies],
    workflow_type: str,  # sequential, parallel, conditional
    agent_sequence: List[str],
    coordination_params: Dict[str, Any] = None
) -> str:
    """
    ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€Ð°Ð±Ð¾Ñ‡Ð¸Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÑÑ Ð¼ÐµÐ¶Ð´Ñƒ Ð°Ð³ÐµÐ½Ñ‚Ð°Ð¼Ð¸

    ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ:
    - sequential: Ð¿Ð¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
    - parallel: Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
    - conditional: ÑƒÑÐ»Ð¾Ð²Ð½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
    """
    if coordination_params is None:
        coordination_params = {}

    try:
        coordination_results = []

        if workflow_type == "sequential":
            # ÐŸÐ¾ÑÐ»ÐµÐ´Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
            for i, agent in enumerate(agent_sequence):
                step_result = await coordinate_single_agent(
                    ctx,
                    agent_name=agent,
                    step_number=i + 1,
                    previous_results=coordination_results,
                    params=coordination_params
                )
                coordination_results.append({
                    "agent": agent,
                    "step": i + 1,
                    "result": step_result,
                    "status": "completed"
                })

        elif workflow_type == "parallel":
            # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
            parallel_tasks = []
            for i, agent in enumerate(agent_sequence):
                task = coordinate_single_agent(
                    ctx,
                    agent_name=agent,
                    step_number=i + 1,
                    previous_results=[],
                    params=coordination_params
                )
                parallel_tasks.append(task)

            # Ð–Ð´ÐµÐ¼ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… Ð·Ð°Ð´Ð°Ñ‡
            parallel_results = await asyncio.gather(*parallel_tasks, return_exceptions=True)

            for i, (agent, result) in enumerate(zip(agent_sequence, parallel_results)):
                status = "completed" if not isinstance(result, Exception) else "failed"
                coordination_results.append({
                    "agent": agent,
                    "step": i + 1,
                    "result": str(result),
                    "status": status
                })

        elif workflow_type == "conditional":
            # Ð£ÑÐ»Ð¾Ð²Ð½Ð¾Ðµ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ
            for i, agent in enumerate(agent_sequence):
                # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ
                should_execute = check_execution_condition(
                    agent,
                    coordination_results,
                    coordination_params
                )

                if should_execute:
                    step_result = await coordinate_single_agent(
                        ctx,
                        agent_name=agent,
                        step_number=i + 1,
                        previous_results=coordination_results,
                        params=coordination_params
                    )
                    coordination_results.append({
                        "agent": agent,
                        "step": i + 1,
                        "result": step_result,
                        "status": "completed"
                    })
                else:
                    coordination_results.append({
                        "agent": agent,
                        "step": i + 1,
                        "result": "Skipped due to conditions",
                        "status": "skipped"
                    })

        return f"""
ðŸ”„ **ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸Ñ Ð°Ð³ÐµÐ½Ñ‚Ð¾Ð² Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°**

**Ð¢Ð¸Ð¿ workflow:** {workflow_type}
**ÐÐ³ÐµÐ½Ñ‚Ñ‹:** {len(agent_sequence)}

**Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹:**
{chr(10).join([f"- {r['agent']}: {r['status']}" for r in coordination_results])}

**Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ°:**
- Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¾: {sum(1 for r in coordination_results if r['status'] == 'completed')}
- ÐŸÑ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð¾: {sum(1 for r in coordination_results if r['status'] == 'skipped')}
- ÐžÑˆÐ¸Ð±ÐºÐ¸: {sum(1 for r in coordination_results if r['status'] == 'failed')}
"""

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² ÐºÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸Ð¸ workflow: {e}"

async def manage_test_lifecycle(
    ctx: RunContext[OrchestratorDependencies],
    lifecycle_stage: str,  # creation, validation, enhancement, deployment
    test_data: Dict[str, Any],
    lifecycle_params: Dict[str, Any] = None
) -> str:
    """
    Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ñ‹Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼ Ð¿ÑÐ¸Ñ…Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ñ‚ÐµÑÑ‚Ð¾Ð²

    ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð¸Ñ€ÑƒÐµÑ‚ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ðµ ÑÑ‚Ð°Ð¿Ñ‹ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ†Ð¸ÐºÐ»Ð° Ñ‚ÐµÑÑ‚Ð¾Ð²
    """
    if lifecycle_params is None:
        lifecycle_params = {}

    try:
        lifecycle_log = []

        if lifecycle_stage == "creation":
            # Ð­Ñ‚Ð°Ð¿ ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ
            creation_result = await orchestrate_test_creation(
                ctx,
                project_name=test_data.get("project_name", "New Test Project"),
                test_topics=test_data.get("topics", ["general"]),
                complexity_level=test_data.get("complexity", "standard")
            )
            lifecycle_log.append(f"Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ: {creation_result}")

        elif lifecycle_stage == "validation":
            # Ð­Ñ‚Ð°Ð¿ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸
            validation_result = await delegate_to_specialist(
                ctx,
                agent_type="psychology_quality_guardian",
                task_title="Lifecycle validation",
                task_description="Validate test in lifecycle context",
                context_data=test_data
            )
            lifecycle_log.append(f"Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ: {validation_result}")

        elif lifecycle_stage == "enhancement":
            # Ð­Ñ‚Ð°Ð¿ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ
            enhancement_result = await delegate_to_specialist(
                ctx,
                agent_type="psychology_content_architect",
                task_title="Test enhancement",
                task_description="Enhance existing test based on feedback",
                context_data=test_data
            )
            lifecycle_log.append(f"Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ: {enhancement_result}")

        elif lifecycle_stage == "deployment":
            # Ð­Ñ‚Ð°Ð¿ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ
            deployment_log = [
                "Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°",
                "ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸",
                "Ð£Ð¿Ð°ÐºÐ¾Ð²ÐºÐ° Ð´Ð»Ñ Ñ€Ð°Ð·Ð²ÐµÑ€Ñ‚Ñ‹Ð²Ð°Ð½Ð¸Ñ",
                "Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐºÑ†Ð¸Ð¹ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ"
            ]
            lifecycle_log.extend(deployment_log)

        return f"""
ðŸ“‹ **Ð£Ð¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ñ‹Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼: {lifecycle_stage}**

**Ð­Ñ‚Ð°Ð¿Ñ‹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ:**
{chr(10).join([f"âœ… {log}" for log in lifecycle_log])}

**Ð¡Ñ‚Ð°Ñ‚ÑƒÑ:** Ð­Ñ‚Ð°Ð¿ {lifecycle_stage} Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾
"""

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² ÑƒÐ¿Ñ€Ð°Ð²Ð»ÐµÐ½Ð¸Ð¸ Ð¶Ð¸Ð·Ð½ÐµÐ½Ð½Ñ‹Ð¼ Ñ†Ð¸ÐºÐ»Ð¾Ð¼: {e}"

async def validate_final_output(
    ctx: RunContext[OrchestratorDependencies],
    project_deliverables: Dict[str, Any],
    validation_level: str = "comprehensive"  # basic, standard, comprehensive
) -> str:
    """
    Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð²ÑÐµÑ… deliverables Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°

    ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÑ‚ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ Ð²ÑÐµÑ… ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð¾Ð² Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
    """
    try:
        validation_results = {}

        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ñ‚ÐµÑÑ‚Ð¾Ð²
        if "tests" in project_deliverables:
            test_validation = await validate_component(
                ctx,
                component_type="tests",
                component_data=project_deliverables["tests"],
                validation_level=validation_level
            )
            validation_results["tests"] = test_validation

        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼ Ñ‚Ñ€Ð°Ð½ÑÑ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
        if "transformation_programs" in project_deliverables:
            program_validation = await validate_component(
                ctx,
                component_type="programs",
                component_data=project_deliverables["transformation_programs"],
                validation_level=validation_level
            )
            validation_results["programs"] = program_validation

        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ð¸
        if "documentation" in project_deliverables:
            docs_validation = await validate_component(
                ctx,
                component_type="documentation",
                component_data=project_deliverables["documentation"],
                validation_level=validation_level
            )
            validation_results["documentation"] = docs_validation

        # ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°
        overall_score = calculate_overall_validation_score(validation_results)

        return f"""
ðŸ” **Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ð°Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°**

**Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸:** {validation_level}

**Ð ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°Ð¼:**
{chr(10).join([f"- {comp}: {result}" for comp, result in validation_results.items()])}

**ÐžÐ±Ñ‰Ð°Ñ Ð¾Ñ†ÐµÐ½ÐºÐ°:** {overall_score:.2f}/1.00

**Ð¡Ñ‚Ð°Ñ‚ÑƒÑ:** {'âœ… ÐŸÑ€Ð¾ÐµÐºÑ‚ Ð³Ð¾Ñ‚Ð¾Ð² Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÑŽ' if overall_score > 0.8 else 'âš ï¸ Ð¢Ñ€ÐµÐ±ÑƒÑŽÑ‚ÑÑ Ð´Ð¾Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸'}
"""

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸: {e}"

async def track_project_progress(
    ctx: RunContext[OrchestratorDependencies],
    project_id: str,
    tracking_scope: str = "all_stages"  # current_stage, all_stages, quality_only
) -> str:
    """
    ÐžÑ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°

    ÐœÐ¾Ð½Ð¸Ñ‚Ð¾Ñ€Ð¸Ð½Ð³ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð·Ð°Ð´Ð°Ñ‡ Ð¸ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° deliverables
    """
    try:
        progress_data = {}

        # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð· Archon
        if tracking_scope in ["all_stages", "current_stage"]:
            # Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°
            tasks_status = await get_project_tasks_status(ctx, project_id)
            progress_data["tasks"] = tasks_status

        if tracking_scope in ["all_stages", "quality_only"]:
            # Ð¡Ñ‚Ð°Ñ‚ÑƒÑ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð°
            quality_status = await get_project_quality_status(ctx, project_id)
            progress_data["quality"] = quality_status

        # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°
        overall_progress = calculate_project_progress(progress_data)

        return f"""
ðŸ“Š **ÐŸÑ€Ð¾Ð³Ñ€ÐµÑÑ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð° {project_id}**

**ÐžÐ±Ð»Ð°ÑÑ‚ÑŒ Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ñ:** {tracking_scope}

**Ð¡Ñ‚Ð°Ñ‚ÑƒÑ Ð·Ð°Ð´Ð°Ñ‡:**
- Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¾: {progress_data.get('tasks', {}).get('completed', 0)}
- Ð’ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ: {progress_data.get('tasks', {}).get('in_progress', 0)}
- ÐžÐ¶Ð¸Ð´Ð°ÑŽÑ‚: {progress_data.get('tasks', {}).get('pending', 0)}

**ÐšÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾:**
- ÐŸÑ€Ð¾ÑˆÐ»Ð¸ Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÑƒ: {progress_data.get('quality', {}).get('passed', 0)}
- Ð¢Ñ€ÐµÐ±ÑƒÑŽÑ‚ Ð´Ð¾Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸: {progress_data.get('quality', {}).get('needs_work', 0)}

**ÐžÐ±Ñ‰Ð¸Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ:** {overall_progress:.1f}%
"""

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð½Ð¸Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {e}"

async def delegate_to_specialist(
    ctx: RunContext[OrchestratorDependencies],
    agent_type: str,
    task_title: str,
    task_description: str,
    priority: str = "medium",
    context_data: Dict[str, Any] = None
) -> str:
    """
    Ð”ÐµÐ»ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð·Ð°Ð´Ð°Ñ‡Ñƒ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ð°Ð³ÐµÐ½Ñ‚Ñƒ

    Ð¡Ð¾Ð·Ð´Ð°ÐµÑ‚ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð² Archon Ð´Ð»Ñ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÐµÐ³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð°
    """
    if context_data is None:
        context_data = {}

    try:
        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÐ¼ Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¸ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»Ñ
        agent_assignee_map = {
            "psychology_content_architect": "Online Support Content Architect Agent",
            "psychology_test_generator": "Online Support Test Generator Agent",
            "psychology_quality_guardian": "Online Support Quality Guardian Agent",
            "psychology_research_agent": "Online Support Research Agent",
            "psychology_transformation_planner": "Psychology Transformation Planner Agent"
        }

        assignee = agent_assignee_map.get(agent_type, "Archon Analysis Lead")

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ Ð·Ð°Ð´Ð°Ñ‡Ñƒ Ð² Archon
        task_result = {
            "task_id": f"orchestrated_{agent_type}_{hash(task_title) % 1000}",
            "status": "delegated",
            "assignee": assignee,
            "title": task_title,
            "description": task_description,
            "priority": priority,
            "context": context_data
        }

        return f"""
âœ… **Ð—Ð°Ð´Ð°Ñ‡Ð° Ð´ÐµÐ»ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð° Ð°Ð³ÐµÐ½Ñ‚Ñƒ {agent_type}**

**ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ:** {task_title}
**Ð˜ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒ:** {assignee}
**ÐŸÑ€Ð¸Ð¾Ñ€Ð¸Ñ‚ÐµÑ‚:** {priority}
**ID Ð·Ð°Ð´Ð°Ñ‡Ð¸:** {task_result['task_id']}

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½, Ð·Ð°Ð´Ð°Ñ‡Ð° ÑÐ¾Ð·Ð´Ð°Ð½Ð° Ð² Archon Ð´Ð»Ñ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ.
"""

    except Exception as e:
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð² Ð´ÐµÐ»ÐµÐ³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸: {e}"

# Ð’Ð¡ÐŸÐžÐœÐžÐ“ÐÐ¢Ð•Ð›Ð¬ÐÐ«Ð• Ð¤Ð£ÐÐšÐ¦Ð˜Ð˜

async def plan_orchestration_workflow(
    ctx: RunContext[OrchestratorDependencies],
    project_name: str,
    test_topics: List[str],
    complexity_level: str
) -> str:
    """ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ workflow Ð¾Ñ€ÐºÐµÑÑ‚Ñ€Ð°Ñ†Ð¸Ð¸"""
    workflow_steps = []

    if complexity_level in ["advanced", "expert"]:
        workflow_steps.append("Research phase")

    workflow_steps.extend([
        "Test creation phase",
        "Test generation phase",
        "Quality assurance phase"
    ])

    if complexity_level in ["standard", "advanced", "expert"]:
        workflow_steps.append("Transformation planning phase")

    return f"Planned {len(workflow_steps)} steps for {complexity_level} complexity"

async def coordinate_single_agent(
    ctx: RunContext[OrchestratorDependencies],
    agent_name: str,
    step_number: int,
    previous_results: List[Dict],
    params: Dict[str, Any]
) -> str:
    """ÐšÐ¾Ð¾Ñ€Ð´Ð¸Ð½Ð°Ñ†Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ð°Ð³ÐµÐ½Ñ‚Ð°"""
    return f"Agent {agent_name} completed step {step_number}"

def check_execution_condition(
    agent: str,
    previous_results: List[Dict],
    params: Dict[str, Any]
) -> bool:
    """ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÑƒÑÐ»Ð¾Ð²Ð¸Ð¹ Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ñ Ð´Ð»Ñ Ð°Ð³ÐµÐ½Ñ‚Ð°"""
    # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ð»Ð¾Ð³Ð¸ÐºÐ° - Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°ÑÑˆÐ¸Ñ€Ð¸Ñ‚ÑŒ
    return True

async def validate_component(
    ctx: RunContext[OrchestratorDependencies],
    component_type: str,
    component_data: Any,
    validation_level: str
) -> str:
    """Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð¼Ð¿Ð¾Ð½ÐµÐ½Ñ‚Ð°"""
    scores = {
        "basic": 0.7,
        "standard": 0.8,
        "comprehensive": 0.9
    }
    return f"Validated with score {scores.get(validation_level, 0.8)}"

def calculate_overall_validation_score(validation_results: Dict[str, str]) -> float:
    """Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐ¹ Ð¾Ñ†ÐµÐ½ÐºÐ¸ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸"""
    return 0.85  # Ð—Ð°Ð³Ð»ÑƒÑˆÐºÐ°

async def get_project_tasks_status(ctx: RunContext[OrchestratorDependencies], project_id: str) -> Dict[str, int]:
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° Ð·Ð°Ð´Ð°Ñ‡ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"""
    return {"completed": 5, "in_progress": 2, "pending": 1}

async def get_project_quality_status(ctx: RunContext[OrchestratorDependencies], project_id: str) -> Dict[str, int]:
    """ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚Ð°Ñ‚ÑƒÑÐ° ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"""
    return {"passed": 4, "needs_work": 1}

def calculate_project_progress(progress_data: Dict[str, Any]) -> float:
    """Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ð±Ñ‰ÐµÐ³Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð°"""
    tasks = progress_data.get("tasks", {})
    total_tasks = sum(tasks.values()) if tasks else 1
    completed_tasks = tasks.get("completed", 0)
    return (completed_tasks / total_tasks) * 100 if total_tasks > 0 else 0