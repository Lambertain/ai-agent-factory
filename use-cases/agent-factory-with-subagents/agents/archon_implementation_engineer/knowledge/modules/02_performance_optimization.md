# Module 02: Performance Optimization

**–í–µ—Ä—Å–∏—è:** 1.0
**–î–∞—Ç–∞:** 2025-10-17
**–ê–≤—Ç–æ—Ä:** Archon Implementation Engineer

**–ù–∞–∑–∞–¥ –∫:** [Implementation Engineer Knowledge Base](../archon_implementation_engineer_knowledge.md)

---

## üîß –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ò–ì–ì–ï–†–´ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∑–∞–¥–∞—á Archon)

**–ö–æ–≥–¥–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç –º–æ–¥—É–ª—å:**
- Async/await patterns –¥–ª—è parallel API calls
- Batching strategies –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ overhead (embeddings, API requests)
- Multi-level caching implementation (Memory ‚Üí Redis ‚Üí Database)
- Token Bucket –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è rate limiting –≤–Ω–µ—à–Ω–∏—Ö API
- Connection pooling –¥–ª—è PostgreSQL –∏ Redis
- Performance profiling –∏ bottleneck analysis
- CPU-intensive –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ thread pool (–∏–∑–±–µ–∂–∞–Ω–∏–µ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ event loop)

---

## üîç –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (–¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)

**–†—É—Å—Å–∫–∏–µ:** –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å, async, –±–∞—Ç—á–∏–Ω–≥, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ, connection pool, rate limiting, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º, thread pool, exponential backoff, throughput, latency

**English:** performance, async, batching, caching, connection pool, rate limiting, optimization, parallelism, thread pool, exponential backoff, throughput, latency

---

## üìå –ö–û–ì–î–ê –ß–ò–¢–ê–¢–¨ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)

- –í—ã—Å–æ–∫–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è response time –∫—Ä–∏—Ç–∏—á–Ω–∞
- –†–∞–±–æ—Ç–∞ —Å –≤–Ω–µ—à–Ω–∏–º–∏ API —Å rate limits
- –ê–≥–µ–Ω—Ç –¥–µ–ª–∞–µ—Ç –º–Ω–æ–∂–µ—Å—Ç–≤–æ parallel –æ–ø–µ—Ä–∞—Ü–∏–π
- CPU-intensive –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å latency
- Throughput –∞–≥–µ–Ω—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω

---

## Async Programming Best Practices

### Performance Optimized Agent

```python
import asyncio
import aiohttp
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor
import time

class PerformanceOptimizedAgent:
    """–ê–≥–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""

    def __init__(self):
        self.session = None
        self.executor = ThreadPoolExecutor(max_workers=4)

    async def __aenter__(self):
        """Context manager entry."""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        await self.session.close()
        self.executor.shutdown()

    async def parallel_api_calls(self, urls: List[str]) -> List[Dict]:
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –≤—ã–∑–æ–≤—ã API —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫."""
        tasks = [self.fetch_data(url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful = [r for r in results if not isinstance(r, Exception)]

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
        errors = [r for r in results if isinstance(r, Exception)]
        if errors:
            print(f"[WARNING] {len(errors)} requests failed")

        return successful

    async def fetch_data(self, url: str) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å retry –ª–æ–≥–∏–∫–æ–π –∏ exponential backoff."""
        for attempt in range(3):
            try:
                async with self.session.get(url, timeout=5) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=response.status
                        )
            except Exception as e:
                if attempt == 2:  # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                    raise e
                # Exponential backoff: 1s, 2s, 4s
                await asyncio.sleep(2 ** attempt)

    async def cpu_intensive_task(self, data: List[Dict]) -> List[Dict]:
        """CPU-–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∞—è –∑–∞–¥–∞—á–∞ –≤ thread pool –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ event loop."""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self.executor,
            self.process_data_sync,
            data
        )

    def process_data_sync(self, data: List[Dict]) -> List[Dict]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        # –°–ª–æ–∂–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ –±–ª–æ–∫–∏—Ä—É—é—Ç CPU
        return [self.transform_item(item) for item in data]

    def transform_item(self, item: Dict) -> Dict:
        """–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞."""
        # –ò–º–∏—Ç–∞—Ü–∏—è —Ç—è–∂–µ–ª—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        time.sleep(0.01)
        return {"processed": True, **item}

# Usage with context manager
async def main():
    async with PerformanceOptimizedAgent() as agent:
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ API –≤—ã–∑–æ–≤—ã
        urls = ["http://api1.com", "http://api2.com", "http://api3.com"]
        results = await agent.parallel_api_calls(urls)

        # CPU-–∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ thread pool
        processed = await agent.cpu_intensive_task(results)

        return processed
```

### Batching –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤

```python
import asyncio
from typing import List, Dict, Callable, Any
from collections import defaultdict
import time

class BatchProcessor:
    """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ overhead."""

    def __init__(self, batch_size: int = 10, max_wait_ms: int = 100):
        self.batch_size = batch_size
        self.max_wait_ms = max_wait_ms
        self.pending_requests: List[Dict] = []
        self.batch_lock = asyncio.Lock()

    async def add_to_batch(
        self,
        request_data: Any,
        processor: Callable
    ) -> Any:
        """–î–æ–±–∞–≤–∏—Ç—å –∑–∞–ø—Ä–æ—Å –≤ batch –∏ –¥–æ–∂–¥–∞—Ç—å—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
        future = asyncio.Future()

        async with self.batch_lock:
            self.pending_requests.append({
                "data": request_data,
                "future": future
            })

            # –ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É, –µ—Å–ª–∏ batch –ø–æ–ª–Ω—ã–π
            if len(self.pending_requests) >= self.batch_size:
                await self._process_batch(processor)

        # –ï—Å–ª–∏ batch –Ω–µ –ø–æ–ª–Ω—ã–π, –∑–∞–ø—É—Å—Ç–∏—Ç—å —Ç–∞–π–º–µ—Ä
        if not future.done():
            asyncio.create_task(
                self._wait_and_process(processor)
            )

        return await future

    async def _wait_and_process(self, processor: Callable):
        """–ü–æ–¥–æ–∂–¥–∞—Ç—å —Ç–∞–π–º–∞—É—Ç –∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å partial batch."""
        await asyncio.sleep(self.max_wait_ms / 1000)

        async with self.batch_lock:
            if self.pending_requests:
                await self._process_batch(processor)

    async def _process_batch(self, processor: Callable):
        """–û–±—Ä–∞–±–æ—Ç–∞—Ç—å batch –∑–∞–ø—Ä–æ—Å–æ–≤."""
        if not self.pending_requests:
            return

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—É—â–∏–π batch
        batch = self.pending_requests[:]
        self.pending_requests.clear()

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ batch
        try:
            batch_data = [req["data"] for req in batch]
            results = await processor(batch_data)

            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            for req, result in zip(batch, results):
                req["future"].set_result(result)

        except Exception as e:
            # –ü—Ä–∏ –æ—à–∏–±–∫–µ - propagate –∫–æ –≤—Å–µ–º futures
            for req in batch:
                req["future"].set_exception(e)

# Usage Example
class EmbeddingService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings —Å batching."""

    def __init__(self):
        self.batch_processor = BatchProcessor(batch_size=32, max_wait_ms=50)

    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∏—Ç—å embedding –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        return await self.batch_processor.add_to_batch(
            text,
            self._batch_embed
        )

    async def _batch_embed(self, texts: List[str]) -> List[List[float]]:
        """Batch –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings."""
        # –û–¥–∏–Ω API –≤—ã–∑–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç–µ–∫—Å—Ç–æ–≤
        # –í–º–µ—Å—Ç–æ N –≤—ã–∑–æ–≤–æ–≤ - —Ç–æ–ª—å–∫–æ 1
        print(f"[OK] Batch embedding {len(texts)} texts")

        # –†–µ–∞–ª—å–Ω—ã–π –≤—ã–∑–æ–≤ –∫ embedding API
        # response = await embedding_api.embed(texts)
        # return response.embeddings

        # Mock –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        return [[0.1, 0.2, 0.3] for _ in texts]

# Usage
async def process_documents(docs: List[str]):
    service = EmbeddingService()

    # –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –±—É–¥—É—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ batch'–∏—Ç—å—Å—è
    embeddings = await asyncio.gather(*[
        service.get_embedding(doc) for doc in docs
    ])

    return embeddings
```

---

## Caching Strategies

### Redis Cache Manager

```python
import redis.asyncio as redis
import json
import hashlib
from functools import wraps
from typing import Optional, Any, Callable
import pickle

class CacheManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å Redis."""

    def __init__(self, redis_url: str):
        self.redis = redis.from_url(redis_url)

    async def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞."""
        value = await self.redis.get(key)
        if value:
            return pickle.loads(value)
        return None

    async def set(self, key: str, value: Any, ttl: int = 3600):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à —Å TTL."""
        await self.redis.setex(key, ttl, pickle.dumps(value))

    async def delete(self, key: str):
        """–£–¥–∞–ª–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞."""
        await self.redis.delete(key)

    async def clear_pattern(self, pattern: str):
        """–û—á–∏—Å—Ç–∏—Ç—å –∫—ç—à –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: "user:*")."""
        keys = await self.redis.keys(pattern)
        if keys:
            await self.redis.delete(*keys)

    async def get_or_set(
        self,
        key: str,
        factory: Callable,
        ttl: int = 3600
    ) -> Any:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∏ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞—Ç—å."""
        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
        cached = await self.get(key)
        if cached is not None:
            return cached

        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
        value = await factory()

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        await self.set(key, value, ttl)

        return value

def cached(ttl: int = 3600, key_prefix: str = ""):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π."""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞
            cache_key = f"{key_prefix}:{func.__name__}:{_generate_cache_key(args, kwargs)}"

            # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
            cache_manager = kwargs.get('cache_manager')
            if cache_manager:
                cached_result = await cache_manager.get(cache_key)
                if cached_result is not None:
                    return cached_result

            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
            result = await func(*args, **kwargs)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if cache_manager:
                await cache_manager.set(cache_key, result, ttl)

            return result
        return wrapper
    return decorator

def _generate_cache_key(args, kwargs) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤."""
    key_data = f"{args}:{sorted(kwargs.items())}"
    return hashlib.md5(key_data.encode()).hexdigest()

# Usage Example
class DataService:
    """–°–µ—Ä–≤–∏—Å —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""

    def __init__(self, cache_manager: CacheManager):
        self.cache_manager = cache_manager

    @cached(ttl=1800, key_prefix="data_service")
    async def expensive_computation(
        self,
        param1: str,
        param2: int,
        cache_manager=None
    ) -> Dict:
        """–î–æ—Ä–æ–≥–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        # –°–∏–º—É–ª—è—Ü–∏—è —Ç—è–∂–µ–ª—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
        await asyncio.sleep(2)
        return {"result": f"processed_{param1}_{param2}"}

    async def get_user_data(self, user_id: str) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        return await self.cache_manager.get_or_set(
            f"user:{user_id}",
            lambda: self._fetch_user_from_db(user_id),
            ttl=3600
        )

    async def _fetch_user_from_db(self, user_id: str) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –ë–î."""
        # –†–µ–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∫ –ë–î
        return {"id": user_id, "name": "User"}
```

### Multi-level Caching Strategy

```python
from typing import Optional
import asyncio

class MultiLevelCache:
    """–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ (Memory -> Redis -> DB)."""

    def __init__(self, redis_cache: CacheManager):
        self.memory_cache: Dict[str, Any] = {}
        self.redis_cache = redis_cache
        self.memory_ttl = 60  # 1 –º–∏–Ω—É—Ç–∞ –≤ –ø–∞–º—è—Ç–∏
        self.redis_ttl = 3600  # 1 —á–∞—Å –≤ Redis

    async def get(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–≥–æ –∫—ç—à–∞."""
        # L1: Memory cache
        if key in self.memory_cache:
            print(f"[OK] Cache hit: Memory - {key}")
            return self.memory_cache[key]

        # L2: Redis cache
        redis_value = await self.redis_cache.get(key)
        if redis_value:
            print(f"[OK] Cache hit: Redis - {key}")
            # Promote to memory cache
            self.memory_cache[key] = redis_value
            asyncio.create_task(self._expire_memory(key))
            return redis_value

        print(f"[WARNING] Cache miss - {key}")
        return None

    async def set(self, key: str, value: Any):
        """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –≤–æ –≤—Å–µ —É—Ä–æ–≤–Ω–∏ –∫—ç—à–∞."""
        # L1: Memory
        self.memory_cache[key] = value
        asyncio.create_task(self._expire_memory(key))

        # L2: Redis
        await self.redis_cache.set(key, value, self.redis_ttl)

    async def _expire_memory(self, key: str):
        """Expire memory cache entry."""
        await asyncio.sleep(self.memory_ttl)
        self.memory_cache.pop(key, None)

    async def invalidate(self, key: str):
        """–ò–Ω–≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –∫—ç—à –Ω–∞ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω—è—Ö."""
        self.memory_cache.pop(key, None)
        await self.redis_cache.delete(key)
```

---

## Connection Pooling

### Database Connection Pool Management

```python
import asyncpg
from contextlib import asynccontextmanager

class PoolManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π."""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=5,        # –ú–∏–Ω–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            max_size=20,       # –ú–∞–∫—Å–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            max_queries=50000, # –ú–∞–∫—Å–∏–º—É–º –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            max_inactive_connection_lifetime=300,  # 5 –º–∏–Ω—É—Ç
            command_timeout=60,
            server_settings={
                'jit': 'off',  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ JIT –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                'application_name': 'archon_agent'
            }
        )

    @asynccontextmanager
    async def acquire(self):
        """–ü–æ–ª—É—á–∏—Ç—å —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ –ø—É–ª–∞."""
        async with self.pool.acquire() as conn:
            yield conn

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π."""
        await self.pool.close()

# Usage
async def main():
    pool_manager = PoolManager("postgresql://...")
    await pool_manager.initialize()

    try:
        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—É–ª–∞
        async with pool_manager.acquire() as conn:
            result = await conn.fetchval("SELECT version()")
            print(result)
    finally:
        await pool_manager.close()
```

---

## Rate Limiting

### Token Bucket Rate Limiter

```python
import time
import asyncio
from typing import Optional

class TokenBucket:
    """Rate limiter —Å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–º Token Bucket."""

    def __init__(self, rate: float, capacity: int):
        """
        Args:
            rate: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
            capacity: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å bucket
        """
        self.rate = rate
        self.capacity = capacity
        self.tokens = capacity
        self.last_update = time.time()
        self.lock = asyncio.Lock()

    async def acquire(self, tokens: int = 1) -> bool:
        """–ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã."""
        async with self.lock:
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤
            now = time.time()
            elapsed = now - self.last_update
            self.tokens = min(
                self.capacity,
                self.tokens + elapsed * self.rate
            )
            self.last_update = now

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–æ–≤
            if self.tokens >= tokens:
                self.tokens -= tokens
                return True

            return False

    async def wait_and_acquire(self, tokens: int = 1):
        """–ü–æ–¥–æ–∂–¥–∞—Ç—å –∏ –ø–æ–ª—É—á–∏—Ç—å —Ç–æ–∫–µ–Ω—ã."""
        while True:
            if await self.acquire(tokens):
                return

            # –ü–æ–¥–æ–∂–¥–∞—Ç—å –≤—Ä–µ–º—è –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞
            wait_time = tokens / self.rate
            await asyncio.sleep(wait_time)

# Usage
class RateLimitedAPI:
    """API —Å rate limiting."""

    def __init__(self, rate_limit: int = 100):
        # 100 –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
        self.limiter = TokenBucket(rate=rate_limit, capacity=rate_limit)

    async def make_request(self, url: str) -> Dict:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å —Å rate limiting."""
        await self.limiter.wait_and_acquire()

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        print(f"[OK] Making request to {url}")
        # response = await http_client.get(url)
        # return response.json()
```

---

**–ù–∞–≤–∏–≥–∞—Ü–∏—è:**
- [‚Üê –ü—Ä–µ–¥—ã–¥—É—â–∏–π –º–æ–¥—É–ª—å: Clean Architecture](01_clean_architecture_design_patterns.md)
- [‚Üë –ù–∞–∑–∞–¥ –∫ Implementation Engineer Knowledge Base](../archon_implementation_engineer_knowledge.md)
- [‚Üí –°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å: Database Optimization](03_database_optimization.md)
