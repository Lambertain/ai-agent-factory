# Module 03: Database Optimization

**–í–µ—Ä—Å–∏—è:** 1.0
**–î–∞—Ç–∞:** 2025-10-17
**–ê–≤—Ç–æ—Ä:** Archon Implementation Engineer

**–ù–∞–∑–∞–¥ –∫:** [Implementation Engineer Knowledge Base](../archon_implementation_engineer_knowledge.md)

---

## üîß –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –¢–†–ò–ì–ì–ï–†–´ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∑–∞–¥–∞—á Archon)

**–ö–æ–≥–¥–∞ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û —á–∏—Ç–∞—Ç—å —ç—Ç–æ—Ç –º–æ–¥—É–ª—å:**
- COPY –¥–ª—è bulk inserts –≤ PostgreSQL (fastest method)
- –°–æ–∑–¥–∞–Ω–∏–µ GIN/BRIN/Covering indexes –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
- FAISS IndexIVFFlat/HNSW –¥–ª—è vector similarity search
- –†–µ—à–µ–Ω–∏–µ N+1 query problem —Å JOIN –∏–ª–∏ batch fetch
- Hybrid search implementation (vector + full-text PostgreSQL)
- EXPLAIN ANALYZE –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
- Batch upsert –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å ON CONFLICT UPDATE
- Vector database –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ —Ä–∞–∑–º–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö (<10k, <100k, >100k)

---

## üîç –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê (–¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º)

**–†—É—Å—Å–∫–∏–µ:** –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö, –∏–Ω–¥–µ–∫—Å—ã, vector search, FAISS, N+1 –ø—Ä–æ–±–ª–µ–º–∞, bulk –æ–ø–µ—Ä–∞—Ü–∏–∏, PostgreSQL, –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è, similarity search, embeddings, hybrid search, full-text search, query optimization, batch operations

**English:** database, indexes, vector search, FAISS, N+1 problem, bulk operations, PostgreSQL, optimization, similarity search, embeddings, hybrid search, full-text search, query optimization, batch operations

---

## üìå –ö–û–ì–î–ê –ß–ò–¢–ê–¢–¨ (–∫–æ–Ω—Ç–µ–∫—Å—Ç)

- –ë–æ–ª—å—à–∏–µ –æ–±—ä–µ–º—ã –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ (—Ç—ã—Å—è—á–∏ –∑–∞–ø–∏—Å–µ–π)
- Vector databases –∏ semantic similarity search
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö database –∑–∞–ø—Ä–æ—Å–æ–≤
- –†–∞–±–æ—Ç–∞ —Å embeddings –∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –ø–æ–∏—Å–∫–æ–º
- N+1 query problem –æ–±–Ω–∞—Ä—É–∂–µ–Ω –≤ –ª–æ–≥–∞—Ö
- –ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å bulk inserts/updates —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º overhead
- Hybrid –ø–æ–∏—Å–∫ (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π + —Ç–µ–∫—Å—Ç–æ–≤—ã–π)
- –ê–Ω–∞–ª–∏–∑ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è query execution plans

---

## Efficient Database Operations

### Optimized Database Class

```python
import asyncpg
from typing import List, Dict, Any
from contextlib import asynccontextmanager

class OptimizedDatabase:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""

    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É–ª–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=5,
            max_size=20,
            command_timeout=60,
            server_settings={
                'jit': 'off',  # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ JIT –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                'application_name': 'archon_agent',
                'work_mem': '64MB',  # –ü–∞–º—è—Ç—å –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
                'effective_cache_size': '4GB'  # –†–∞–∑–º–µ—Ä –∫—ç—à–∞ –û–°
            }
        )

    @asynccontextmanager
    async def transaction(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π."""
        async with self.pool.acquire() as conn:
            async with conn.transaction():
                yield conn

    async def bulk_insert(
        self,
        table: str,
        data: List[Dict[str, Any]]
    ) -> int:
        """–ú–∞—Å—Å–æ–≤–∞—è –≤—Å—Ç–∞–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ COPY (fastest method)."""
        if not data:
            return 0

        columns = list(data[0].keys())
        values = [list(row.values()) for row in data]

        async with self.pool.acquire() as conn:
            # COPY - —Å–∞–º—ã–π –±—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –≤—Å—Ç–∞–≤–∫–∏
            result = await conn.copy_records_to_table(
                table,
                records=values,
                columns=columns
            )
            return len(values)

    async def execute_query_with_pagination(
        self,
        base_query: str,
        params: List[Any],
        page_size: int = 1000
    ) -> List[Dict]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
        all_results = []
        offset = 0

        async with self.pool.acquire() as conn:
            while True:
                paginated_query = f"{base_query} LIMIT {page_size} OFFSET {offset}"
                rows = await conn.fetch(paginated_query, *params)

                if not rows:
                    break

                all_results.extend([dict(row) for row in rows])
                offset += page_size

                if len(rows) < page_size:
                    break

        return all_results

    async def explain_query(
        self,
        query: str,
        params: List[Any] = None
    ) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –ø–ª–∞–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ (EXPLAIN ANALYZE)."""
        explain_query = f"EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) {query}"

        async with self.pool.acquire() as conn:
            result = await conn.fetchval(explain_query, *(params or []))
            return result[0]  # JSON —Ä–µ–∑—É–ª—å—Ç–∞—Ç

    async def optimize_table(self, table: str):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ç–∞–±–ª–∏—Ü—ã (VACUUM ANALYZE)."""
        async with self.pool.acquire() as conn:
            await conn.execute(f"VACUUM ANALYZE {table}")
```

### Batch Operations –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

```python
class BatchOperations:
    """Batch –æ–ø–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ round-trips."""

    def __init__(self, db: OptimizedDatabase):
        self.db = db

    async def batch_upsert(
        self,
        table: str,
        data: List[Dict],
        conflict_columns: List[str],
        update_columns: List[str]
    ):
        """Batch upsert (INSERT ... ON CONFLICT UPDATE)."""
        if not data:
            return

        columns = list(data[0].keys())
        values_placeholder = ', '.join([
            f"({', '.join(['$' + str(i*len(columns) + j + 1) for j in range(len(columns))])})"
            for i in range(len(data))
        ])

        conflict_clause = ', '.join(conflict_columns)
        update_clause = ', '.join([
            f"{col} = EXCLUDED.{col}" for col in update_columns
        ])

        query = f"""
            INSERT INTO {table} ({', '.join(columns)})
            VALUES {values_placeholder}
            ON CONFLICT ({conflict_clause})
            DO UPDATE SET {update_clause}
        """

        # Flatten data –¥–ª—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        flat_values = [
            value for row in data for value in row.values()
        ]

        async with self.db.pool.acquire() as conn:
            await conn.execute(query, *flat_values)

    async def batch_delete(
        self,
        table: str,
        ids: List[str]
    ) -> int:
        """Batch —É–¥–∞–ª–µ–Ω–∏–µ –ø–æ —Å–ø–∏—Å–∫—É ID."""
        if not ids:
            return 0

        placeholders = ', '.join(f'${i+1}' for i in range(len(ids)))
        query = f"DELETE FROM {table} WHERE id IN ({placeholders})"

        async with self.db.pool.acquire() as conn:
            result = await conn.execute(query, *ids)
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫
            return int(result.split()[-1])
```

---

## Indexing Strategies

### –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤

```sql
-- 1. –°–æ—Å—Ç–∞–≤–Ω–æ–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –ø–æ–ª—è–º
CREATE INDEX CONCURRENTLY idx_users_name_email
ON users (name, email)
WHERE active = true;

-- 2. –ß–∞—Å—Ç–∏—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π (—ç–∫–æ–Ω–æ–º–∏—è –º–µ—Å—Ç–∞)
CREATE INDEX CONCURRENTLY idx_users_active
ON users (created_at)
WHERE active = true;

-- 3. GIN –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
CREATE INDEX CONCURRENTLY idx_documents_search
ON documents USING gin(to_tsvector('english', title || ' ' || content));

-- 4. –ò–Ω–¥–µ–∫—Å –¥–ª—è JSON –ø–æ–ª–µ–π
CREATE INDEX CONCURRENTLY idx_metadata_tags
ON documents USING gin((metadata->>'tags'));

-- 5. Covering index –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è table lookup
CREATE INDEX CONCURRENTLY idx_users_covering
ON users (id) INCLUDE (name, email, created_at)
WHERE active = true;

-- 6. BRIN –∏–Ω–¥–µ–∫—Å –¥–ª—è timestamp columns (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏)
CREATE INDEX CONCURRENTLY idx_logs_created_at
ON logs USING brin(created_at);

-- 7. Hash –∏–Ω–¥–µ–∫—Å –¥–ª—è —Ç–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
CREATE INDEX CONCURRENTLY idx_sessions_token
ON sessions USING hash(session_token);
```

### Index Maintenance

```python
class IndexMaintenance:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞–º–∏ –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""

    def __init__(self, db: OptimizedDatabase):
        self.db = db

    async def check_index_usage(self, table: str) -> List[Dict]:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞ —Ç–∞–±–ª–∏—Ü–µ."""
        query = """
            SELECT
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch,
                pg_size_pretty(pg_relation_size(indexrelid)) as index_size
            FROM pg_stat_user_indexes
            WHERE tablename = $1
            ORDER BY idx_scan ASC
        """

        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(query, table)
            return [dict(row) for row in rows]

    async def find_missing_indexes(self, table: str) -> List[Dict]:
        """–ù–∞–π—Ç–∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø–æ–ª–µ–∑–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã."""
        query = """
            SELECT
                schemaname,
                tablename,
                attname,
                n_distinct,
                correlation
            FROM pg_stats
            WHERE tablename = $1
            AND n_distinct > 100  -- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            AND correlation < 0.8  -- –ù–∏–∑–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å —Ñ–∏–∑–∏—á–µ—Å–∫–∏–º –ø–æ—Ä—è–¥–∫–æ–º
        """

        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(query, table)
            return [dict(row) for row in rows]

    async def rebuild_index(self, index_name: str):
        """–ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å –∏–Ω–¥–µ–∫—Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ (–±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏)."""
        query = f"REINDEX INDEX CONCURRENTLY {index_name}"

        async with self.db.pool.acquire() as conn:
            await conn.execute(query)
```

---

## Vector Database Optimization

### Optimized Vector Store —Å FAISS

```python
import numpy as np
from typing import List, Tuple
import faiss
import pickle

class OptimizedVectorStore:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å FAISS."""

    def __init__(self, dimension: int):
        self.dimension = dimension
        self.index = None
        self.metadata = []

    def build_index(
        self,
        vectors: np.ndarray,
        use_gpu: bool = False
    ):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞."""
        n_vectors = vectors.shape[0]

        # –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ç–∏–ø–∞ –∏–Ω–¥–µ–∫—Å–∞
        if n_vectors < 10000:
            # –î–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö - –ø—Ä–æ—Å—Ç–æ–π L2 –ø–æ–∏—Å–∫
            self.index = faiss.IndexFlatL2(self.dimension)
            print(f"[OK] Using IndexFlatL2 for {n_vectors} vectors")

        elif n_vectors < 100000:
            # –°—Ä–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ - IVF —Å –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
            nlist = min(100, n_vectors // 100)
            quantizer = faiss.IndexFlatL2(self.dimension)
            self.index = faiss.IndexIVFFlat(
                quantizer,
                self.dimension,
                nlist
            )
            print(f"[OK] Using IndexIVFFlat with {nlist} clusters")

        else:
            # –ë–æ–ª—å—à–∏–µ –¥–∞–Ω–Ω—ã–µ - HNSW –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
            self.index = faiss.IndexHNSWFlat(self.dimension, 32)
            self.index.hnsw.efConstruction = 200
            self.index.hnsw.efSearch = 50
            print(f"[OK] Using IndexHNSWFlat for {n_vectors} vectors")

        # GPU acceleration –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
        if use_gpu and faiss.get_num_gpus() > 0:
            res = faiss.StandardGpuResources()
            self.index = faiss.index_cpu_to_gpu(res, 0, self.index)
            print("[OK] Moved index to GPU")

        # –û–±—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ (–¥–ª—è IVF)
        if hasattr(self.index, 'train'):
            print("[OK] Training index...")
            self.index.train(vectors)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤
        print("[OK] Adding vectors to index...")
        self.index.add(vectors)

    def search(
        self,
        query_vector: np.ndarray,
        k: int = 10
    ) -> Tuple[List[float], List[int]]:
        """–ü–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤."""
        query_vector = query_vector.reshape(1, -1).astype(np.float32)
        distances, indices = self.index.search(query_vector, k)
        return distances[0].tolist(), indices[0].tolist()

    def batch_search(
        self,
        query_vectors: np.ndarray,
        k: int = 10
    ) -> Tuple[np.ndarray, np.ndarray]:
        """–ü–∞–∫–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–ø—Ä–æ—Å–æ–≤."""
        return self.index.search(query_vectors.astype(np.float32), k)

    def save_index(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫."""
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        faiss.write_index(self.index, f"{filepath}.faiss")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        with open(f"{filepath}.metadata", 'wb') as f:
            pickle.dump(self.metadata, f)

        print(f"[OK] Index saved to {filepath}")

    def load_index(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞."""
        # –ó–∞–≥—Ä—É–∑–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞
        self.index = faiss.read_index(f"{filepath}.faiss")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        with open(f"{filepath}.metadata", 'rb') as f:
            self.metadata = pickle.load(f)

        print(f"[OK] Index loaded from {filepath}")

    def add_vectors_incremental(
        self,
        vectors: np.ndarray,
        metadata: List[Dict]
    ):
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤."""
        self.index.add(vectors.astype(np.float32))
        self.metadata.extend(metadata)
```

### Hybrid Search: Vector + Full-Text

```python
from typing import List, Dict
import asyncpg

class HybridSearchEngine:
    """Hybrid search: vector similarity + full-text search."""

    def __init__(
        self,
        vector_store: OptimizedVectorStore,
        db: OptimizedDatabase
    ):
        self.vector_store = vector_store
        self.db = db

    async def hybrid_search(
        self,
        query_text: str,
        query_embedding: np.ndarray,
        k: int = 20,
        alpha: float = 0.5  # –í–µ—Å vector search vs full-text
    ) -> List[Dict]:
        """
        Hybrid search —Å –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π vector similarity –∏ full-text search.

        Args:
            query_text: –¢–µ–∫—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            query_embedding: –í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            alpha: –í–µ—Å vector search (0-1), (1-alpha) = –≤–µ—Å full-text
        """
        # 1. Vector search
        vector_distances, vector_indices = self.vector_store.search(
            query_embedding, k=k*2
        )

        # 2. Full-text search
        fulltext_results = await self._fulltext_search(query_text, k=k*2)

        # 3. –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ–º
        combined_scores = {}

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è vector scores
        max_vector_dist = max(vector_distances) if vector_distances else 1
        for idx, distance in zip(vector_indices, vector_distances):
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º distance –≤ similarity score (0-1)
            similarity = 1 - (distance / max_vector_dist)
            combined_scores[idx] = alpha * similarity

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ full-text scores
        for result in fulltext_results:
            doc_id = result['id']
            rank_score = result['rank']

            if doc_id in combined_scores:
                combined_scores[doc_id] += (1 - alpha) * rank_score
            else:
                combined_scores[doc_id] = (1 - alpha) * rank_score

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É score
        sorted_results = sorted(
            combined_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )[:k]

        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_ids = [doc_id for doc_id, _ in sorted_results]
        documents = await self._get_documents_by_ids(doc_ids)

        return documents

    async def _fulltext_search(
        self,
        query: str,
        k: int
    ) -> List[Dict]:
        """Full-text –ø–æ–∏—Å–∫ –≤ PostgreSQL."""
        sql = """
            SELECT
                id,
                title,
                content,
                ts_rank(
                    to_tsvector('english', title || ' ' || content),
                    plainto_tsquery('english', $1)
                ) as rank
            FROM documents
            WHERE to_tsvector('english', title || ' ' || content) @@
                  plainto_tsquery('english', $1)
            ORDER BY rank DESC
            LIMIT $2
        """

        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(sql, query, k)
            return [dict(row) for row in rows]

    async def _get_documents_by_ids(
        self,
        doc_ids: List[int]
    ) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–ø–∏—Å–∫—É ID."""
        placeholders = ', '.join(f'${i+1}' for i in range(len(doc_ids)))
        sql = f"""
            SELECT id, title, content, metadata
            FROM documents
            WHERE id IN ({placeholders})
        """

        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(sql, *doc_ids)
            return [dict(row) for row in rows]
```

---

## Query Optimization Patterns

### N+1 Query Problem Solution

```python
class OptimizedQueries:
    """–†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã N+1 –∑–∞–ø—Ä–æ—Å–æ–≤."""

    def __init__(self, db: OptimizedDatabase):
        self.db = db

    async def get_users_with_posts_bad(self) -> List[Dict]:
        """‚ùå –ü–ª–æ—Ö–æ: N+1 queries."""
        users = await self._get_all_users()

        for user in users:
            # N –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤!
            user['posts'] = await self._get_user_posts(user['id'])

        return users

    async def get_users_with_posts_good(self) -> List[Dict]:
        """‚úÖ –•–æ—Ä–æ—à–æ: 2 queries —Å JOIN –∏–ª–∏ batch fetch."""
        # –í–∞—Ä–∏–∞–Ω—Ç 1: JOIN –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ
        query = """
            SELECT
                u.id as user_id,
                u.name as user_name,
                u.email as user_email,
                p.id as post_id,
                p.title as post_title,
                p.content as post_content
            FROM users u
            LEFT JOIN posts p ON u.id = p.user_id
        """

        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(query)

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        users_dict = {}
        for row in rows:
            user_id = row['user_id']

            if user_id not in users_dict:
                users_dict[user_id] = {
                    'id': user_id,
                    'name': row['user_name'],
                    'email': row['user_email'],
                    'posts': []
                }

            if row['post_id']:
                users_dict[user_id]['posts'].append({
                    'id': row['post_id'],
                    'title': row['post_title'],
                    'content': row['post_content']
                })

        return list(users_dict.values())

    async def _get_all_users(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch("SELECT * FROM users")
            return [dict(row) for row in rows]

    async def _get_user_posts(self, user_id: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        async with self.db.pool.acquire() as conn:
            rows = await conn.fetch(
                "SELECT * FROM posts WHERE user_id = $1", user_id
            )
            return [dict(row) for row in rows]
```

---

**–ù–∞–≤–∏–≥–∞—Ü–∏—è:**
- [‚Üê –ü—Ä–µ–¥—ã–¥—É—â–∏–π –º–æ–¥—É–ª—å: Performance Optimization](02_performance_optimization.md)
- [‚Üë –ù–∞–∑–∞–¥ –∫ Implementation Engineer Knowledge Base](../archon_implementation_engineer_knowledge.md)
- [‚Üí –°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å: Testing & Quality Assurance](04_testing_quality_assurance.md)
