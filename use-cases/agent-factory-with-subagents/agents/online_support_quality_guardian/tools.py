"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è Psychology Quality Guardian Agent

–ù–∞–±–æ—Ä —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞,
—ç—Ç–∏—á–µ—Å–∫–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –Ω–∞—É—á–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.
"""

from typing import Any, Dict, List, Optional, Union
from pydantic_ai import RunContext
import json
import re
from datetime import datetime

try:
    from mcp__archon__rag_search_knowledge_base import mcp__archon__rag_search_knowledge_base
    ARCHON_AVAILABLE = True
except ImportError:
    ARCHON_AVAILABLE = False

from .dependencies import QualityGuardianDependencies


async def search_quality_guardian_knowledge(
    ctx: RunContext[QualityGuardianDependencies],
    query: str,
    focus_area: str = "general",
    match_count: int = 5
) -> str:
    """
    –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π Psychology Quality Guardian –ø–æ –∫–æ–Ω—Ç—Ä–æ–ª—é –∫–∞—á–µ—Å—Ç–≤–∞.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        focus_area: –û–±–ª–∞—Å—Ç—å —Ñ–æ–∫—É—Å–∞ (ethical, scientific, safety, cultural)
        match_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ –∫–æ–Ω—Ç—Ä–æ–ª—é –∫–∞—á–µ—Å—Ç–≤–∞
    """
    enhanced_query = f"{query} psychology quality {focus_area} validation"

    if ARCHON_AVAILABLE:
        try:
            result = await mcp__archon__rag_search_knowledge_base(
                query=enhanced_query,
                match_count=match_count
            )

            if result.get("success") and result.get("results"):
                knowledge = "\n".join([
                    f"**{r['metadata']['title']}:**\n{r['content']}"
                    for r in result["results"]
                ])
                return f"üìö –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π Quality Guardian:\n{knowledge}"
            else:
                return f"‚ö†Ô∏è –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –¥–ª—è '{query}' –≤ –æ–±–ª–∞—Å—Ç–∏ '{focus_area}'"

        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}"
    else:
        return f"üìñ –õ–æ–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –ø–æ '{query}' –≤ –æ–±–ª–∞—Å—Ç–∏ '{focus_area}': –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã APA, ITC, –∏ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏"


async def evaluate_ethical_compliance(
    ctx: RunContext[QualityGuardianDependencies],
    content_data: Dict[str, Any],
    ethical_standards: List[str] = None,
    target_population: str = "adults"
) -> str:
    """
    –û—Ü–µ–Ω–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        content_data: –î–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        ethical_standards: –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ (APA, BPS, ITC –∏ —Ç.–¥.)
        target_population: –¶–µ–ª–µ–≤–∞—è –ø–æ–ø—É–ª—è—Ü–∏—è

    Returns:
        –î–µ—Ç–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
    """
    if ethical_standards is None:
        ethical_standards = ["APA", "informed_consent", "beneficence", "justice", "respect"]

    ethical_analysis = {
        "compliance_score": 0,
        "ethical_issues": [],
        "recommendations": [],
        "risk_level": "low"
    }

    # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è
    if "informed_consent" in ethical_standards:
        consent_issues = _check_informed_consent(content_data, target_population)
        ethical_analysis["ethical_issues"].extend(consent_issues)

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–Ω—Ü–∏–ø–∞ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è
    if "beneficence" in ethical_standards:
        beneficence_issues = _check_beneficence_principle(content_data)
        ethical_analysis["ethical_issues"].extend(beneficence_issues)

    # –ê–Ω–∞–ª–∏–∑ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏ –∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞
    if "justice" in ethical_standards:
        justice_issues = _check_justice_principle(content_data, target_population)
        ethical_analysis["ethical_issues"].extend(justice_issues)

    # –ê–Ω–∞–ª–∏–∑ —É–≤–∞–∂–µ–Ω–∏—è –∫ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–º—É –¥–æ—Å—Ç–æ–∏–Ω—Å—Ç–≤—É
    if "respect" in ethical_standards:
        respect_issues = _check_respect_principle(content_data)
        ethical_analysis["ethical_issues"].extend(respect_issues)

    # –†–∞—Å—á–µ—Ç –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è
    total_checks = len(ethical_standards) * 10  # –ú–∞–∫—Å–∏–º—É–º 10 –±–∞–ª–ª–æ–≤ –∑–∞ –∫–∞–∂–¥—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç
    issues_penalty = len(ethical_analysis["ethical_issues"]) * 2
    ethical_analysis["compliance_score"] = max(0, total_checks - issues_penalty)

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Ä–æ–≤–Ω—è —Ä–∏—Å–∫–∞
    if len(ethical_analysis["ethical_issues"]) == 0:
        ethical_analysis["risk_level"] = "low"
    elif len(ethical_analysis["ethical_issues"]) <= 3:
        ethical_analysis["risk_level"] = "medium"
    else:
        ethical_analysis["risk_level"] = "high"

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    ethical_analysis["recommendations"] = _generate_ethical_recommendations(
        ethical_analysis["ethical_issues"]
    )

    return f"""
üîç **–û—Ü–µ–Ω–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:**

**–û–±—â–∏–π –±–∞–ª–ª —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è:** {ethical_analysis['compliance_score']}/{total_checks}
**–£—Ä–æ–≤–µ–Ω—å —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞:** {ethical_analysis['risk_level'].upper()}
**–ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã:** {', '.join(ethical_standards)}

**üìã –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã ({len(ethical_analysis['ethical_issues'])}):**
{_format_issues_list(ethical_analysis['ethical_issues'])}

**üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:**
{_format_recommendations_list(ethical_analysis['recommendations'])}

**üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
1. –£—Å—Ç—Ä–∞–Ω–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ —ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
2. –í–Ω–µ–¥—Ä–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É
3. –ü—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω—É—é —ç—Ç–∏—á–µ—Å–∫—É—é –æ—Ü–µ–Ω–∫—É
4. –ü–æ–ª—É—á–∏—Ç—å –æ–¥–æ–±—Ä–µ–Ω–∏–µ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞ (–ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
"""


async def assess_scientific_validity(
    ctx: RunContext[QualityGuardianDependencies],
    content_data: Dict[str, Any],
    validation_criteria: List[str] = None,
    evidence_level: str = "standard"
) -> str:
    """
    –û—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –∏ –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤.

    Args:
        content_data: –î–∞–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        validation_criteria: –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        evidence_level: –£—Ä–æ–≤–µ–Ω—å —Ç—Ä–µ–±—É–µ–º—ã—Ö –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤

    Returns:
        –ê–Ω–∞–ª–∏–∑ –Ω–∞—É—á–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
    """
    if validation_criteria is None:
        validation_criteria = ["content_validity", "construct_validity", "reliability", "theoretical_basis"]

    validity_analysis = {
        "overall_validity": "pending",
        "validity_scores": {},
        "scientific_issues": [],
        "evidence_gaps": [],
        "recommendations": []
    }

    # –û—Ü–µ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    if "content_validity" in validation_criteria:
        content_validity_score = _assess_content_validity(content_data)
        validity_analysis["validity_scores"]["content_validity"] = content_validity_score

    # –û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    if "construct_validity" in validation_criteria:
        construct_validity_score = _assess_construct_validity(content_data)
        validity_analysis["validity_scores"]["construct_validity"] = construct_validity_score

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    if "reliability" in validation_criteria:
        reliability_score = _assess_reliability(content_data)
        validity_analysis["validity_scores"]["reliability"] = reliability_score

    # –û—Ü–µ–Ω–∫–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
    if "theoretical_basis" in validation_criteria:
        theoretical_score = _assess_theoretical_basis(content_data)
        validity_analysis["validity_scores"]["theoretical_basis"] = theoretical_score

    # –ê–Ω–∞–ª–∏–∑ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    empirical_support = _analyze_empirical_support(content_data, evidence_level)
    validity_analysis["evidence_gaps"] = empirical_support["gaps"]

    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    avg_score = sum(validity_analysis["validity_scores"].values()) / len(validity_analysis["validity_scores"])
    if avg_score >= 80:
        validity_analysis["overall_validity"] = "high"
    elif avg_score >= 60:
        validity_analysis["overall_validity"] = "adequate"
    else:
        validity_analysis["overall_validity"] = "insufficient"

    return f"""
üî¨ **–û—Ü–µ–Ω–∫–∞ –Ω–∞—É—á–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏:**

**–û–±—â–∞—è –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å:** {validity_analysis['overall_validity'].upper()}
**–£—Ä–æ–≤–µ–Ω—å –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤:** {evidence_level}

**üìä –û—Ü–µ–Ω–∫–∏ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏:**
{_format_validity_scores(validity_analysis['validity_scores'])}

**‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:**
{_format_issues_list(validity_analysis['scientific_issues'])}

**üìà –ü—Ä–æ–±–µ–ª—ã –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –±–∞–∑–µ:**
{_format_evidence_gaps(validity_analysis['evidence_gaps'])}

**üîß –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏:**
1. –ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
2. –£—Å–∏–ª–∏—Ç—å —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ
3. –£–ª—É—á—à–∏—Ç—å –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
4. –†–∞—Å—à–∏—Ä–∏—Ç—å —ç–º–ø–∏—Ä–∏—á–µ—Å–∫—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É

**üìã –°–ª–µ–¥—É—é—â–∏–µ –Ω–∞—É—á–Ω—ã–µ —à–∞–≥–∏:**
- –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
- –°–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- Peer review –Ω–∞—É—á–Ω—ã—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
- –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
"""


async def analyze_content_safety(
    ctx: RunContext[QualityGuardianDependencies],
    content_data: Dict[str, Any],
    risk_categories: List[str] = None,
    sensitivity_level: str = "standard"
) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø—Å–∏—Ö–∏—á–µ—Å–∫–æ–≥–æ –∑–¥–æ—Ä–æ–≤—å—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤.

    Args:
        content_data: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
        risk_categories: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ä–∏—Å–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        sensitivity_level: –£—Ä–æ–≤–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –û—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ —Å –≤—ã—è–≤–ª–µ–Ω–Ω—ã–º–∏ —Ä–∏—Å–∫–∞–º–∏
    """
    if risk_categories is None:
        risk_categories = ["psychological_harm", "privacy_risk", "vulnerable_groups", "crisis_triggers"]

    safety_analysis = {
        "overall_safety": "safe",
        "risk_scores": {},
        "safety_issues": [],
        "protection_measures": [],
        "crisis_protocols": []
    }

    # –ê–Ω–∞–ª–∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤
    if "psychological_harm" in risk_categories:
        psych_risks = _analyze_psychological_risks(content_data, sensitivity_level)
        safety_analysis["risk_scores"]["psychological_harm"] = psych_risks["score"]
        safety_analysis["safety_issues"].extend(psych_risks["issues"])

    # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏
    if "privacy_risk" in risk_categories:
        privacy_risks = _analyze_privacy_risks(content_data)
        safety_analysis["risk_scores"]["privacy_risk"] = privacy_risks["score"]
        safety_analysis["safety_issues"].extend(privacy_risks["issues"])

    # –ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –¥–ª—è —É—è–∑–≤–∏–º—ã—Ö –≥—Ä—É–ø–ø
    if "vulnerable_groups" in risk_categories:
        vulnerability_risks = _analyze_vulnerability_risks(content_data)
        safety_analysis["risk_scores"]["vulnerable_groups"] = vulnerability_risks["score"]
        safety_analysis["safety_issues"].extend(vulnerability_risks["issues"])

    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –∫—Ä–∏–∑–∏—Å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π
    if "crisis_triggers" in risk_categories:
        crisis_risks = _analyze_crisis_triggers(content_data)
        safety_analysis["risk_scores"]["crisis_triggers"] = crisis_risks["score"]
        safety_analysis["safety_issues"].extend(crisis_risks["issues"])

    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    max_risk_score = max(safety_analysis["risk_scores"].values()) if safety_analysis["risk_scores"] else 0
    if max_risk_score >= 80:
        safety_analysis["overall_safety"] = "high_risk"
    elif max_risk_score >= 50:
        safety_analysis["overall_safety"] = "moderate_risk"
    else:
        safety_analysis["overall_safety"] = "safe"

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∑–∞—â–∏—Ç–Ω—ã–º –º–µ—Ä–∞–º
    safety_analysis["protection_measures"] = _generate_protection_measures(safety_analysis["safety_issues"])

    return f"""
üõ°Ô∏è **–ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:**

**–û–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:** {safety_analysis['overall_safety'].upper()}
**–£—Ä–æ–≤–µ–Ω—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:** {sensitivity_level}

**‚ö†Ô∏è –û—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:**
{_format_risk_scores(safety_analysis['risk_scores'])}

**üö® –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:**
{_format_issues_list(safety_analysis['safety_issues'])}

**üõ°Ô∏è –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∑–∞—â–∏—Ç–Ω—ã–µ –º–µ—Ä—ã:**
{_format_protection_measures(safety_analysis['protection_measures'])}

**üÜò –ü—Ä–æ—Ç–æ–∫–æ–ª—ã –∫—Ä–∏–∑–∏—Å–Ω–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**
1. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
2. –¢—Ä–∏–≥–≥–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
3. –î–æ—Å—Ç—É–ø –∫ –∫—Ä–∏–∑–∏—Å–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ
4. –ü—Ä–æ—Ü–µ–¥—É—Ä—ã —ç—Å–∫–∞–ª–∞—Ü–∏–∏ –ø—Ä–∏ –≤—ã—è–≤–ª–µ–Ω–∏–∏ —Ä–∏—Å–∫–∞
5. –ö–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–ª—É–∂–± —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –ø–æ–º–æ—â–∏

**üìû –ö–æ–Ω—Ç–∞–∫—Ç—ã —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π –ø–æ–º–æ—â–∏:**
- –¢–µ–ª–µ—Ñ–æ–Ω –¥–æ–≤–µ—Ä–∏—è: 8-800-2000-122
- –ö—Ä–∏–∑–∏—Å–Ω–∞—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –ø–æ–º–æ—â—å: 051
- –°–ª—É–∂–±–∞ —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–≥–æ —Ä–µ–∞–≥–∏—Ä–æ–≤–∞–Ω–∏—è: 112
"""


async def validate_psychometric_properties(
    ctx: RunContext[QualityGuardianDependencies],
    test_data: Dict[str, Any],
    psychometric_standards: Dict[str, float] = None
) -> str:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ —Ç–µ—Å—Ç–∞ –∏–ª–∏ –æ—Ü–µ–Ω–æ—á–Ω–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.

    Args:
        test_data: –î–∞–Ω–Ω—ã–µ —Ç–µ—Å—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        psychometric_standards: –°—Ç–∞–Ω–¥–∞—Ä—Ç—ã –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π

    Returns:
        –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    """
    if psychometric_standards is None:
        psychometric_standards = {
            "reliability_threshold": 0.80,
            "validity_threshold": 0.70,
            "factor_loading_min": 0.30,
            "item_total_correlation_min": 0.30
        }

    psychometric_analysis = {
        "reliability": {},
        "validity": {},
        "factor_structure": {},
        "item_analysis": {},
        "overall_quality": "pending",
        "recommendations": []
    }

    # –ê–Ω–∞–ª–∏–∑ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    reliability_data = _analyze_reliability_indicators(test_data, psychometric_standards)
    psychometric_analysis["reliability"] = reliability_data

    # –ê–Ω–∞–ª–∏–∑ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    validity_data = _analyze_validity_indicators(test_data, psychometric_standards)
    psychometric_analysis["validity"] = validity_data

    # –ê–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    if "factor_structure" in test_data:
        factor_data = _analyze_factor_structure(test_data["factor_structure"], psychometric_standards)
        psychometric_analysis["factor_structure"] = factor_data

    # –ê–Ω–∞–ª–∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –∑–∞–¥–∞–Ω–∏–π
    if "items" in test_data or "questions" in test_data:
        items = test_data.get("items", test_data.get("questions", []))
        item_data = _analyze_item_characteristics(items, psychometric_standards)
        psychometric_analysis["item_analysis"] = item_data

    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
    psychometric_analysis["overall_quality"] = _calculate_overall_psychometric_quality(psychometric_analysis)

    return f"""
üìä **–í–∞–ª–∏–¥–∞—Ü–∏—è –ø—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤:**

**–û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞:** {psychometric_analysis['overall_quality'].upper()}

**üîÑ –ê–Ω–∞–ª–∏–∑ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏:**
{_format_reliability_analysis(psychometric_analysis['reliability'])}

**‚úÖ –ê–Ω–∞–ª–∏–∑ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏:**
{_format_validity_analysis(psychometric_analysis['validity'])}

**üß© –§–∞–∫—Ç–æ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**
{_format_factor_analysis(psychometric_analysis['factor_structure'])}

**üìù –ê–Ω–∞–ª–∏–∑ –∑–∞–¥–∞–Ω–∏–π:**
{_format_item_analysis(psychometric_analysis['item_analysis'])}

**üéØ –ü—Å–∏—Ö–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**
1. –£–ª—É—á—à–∏—Ç—å –∑–∞–¥–∞–Ω–∏—è —Å –Ω–∏–∑–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
2. –ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
3. –†–∞—Å—à–∏—Ä–∏—Ç—å –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—É—é –≤—ã–±–æ—Ä–∫—É
4. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–∫—Ç–æ—Ä–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É

**üìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏:**
- –°–±–æ—Ä –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–π —Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
- –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
- –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ—Ä–º –∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–æ–Ω–Ω—ã—Ö guidelines
"""


async def check_cultural_sensitivity(
    ctx: RunContext[QualityGuardianDependencies],
    content_data: Dict[str, Any],
    target_cultures: List[str] = None,
    sensitivity_areas: List[str] = None
) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∏–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        content_data: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        target_cultures: –¶–µ–ª–µ–≤—ã–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –≥—Ä—É–ø–ø—ã
        sensitivity_areas: –û–±–ª–∞—Å—Ç–∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –ê–Ω–∞–ª–∏–∑ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç–∏ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
    """
    if target_cultures is None:
        target_cultures = ["multiethnic", "multicultural", "global"]

    if sensitivity_areas is None:
        sensitivity_areas = ["language", "examples", "values", "accessibility", "representation"]

    cultural_analysis = {
        "overall_sensitivity": "adequate",
        "sensitivity_scores": {},
        "cultural_issues": [],
        "inclusion_gaps": [],
        "adaptation_recommendations": []
    }

    # –ê–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤–æ–π –∏–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç–∏
    if "language" in sensitivity_areas:
        language_analysis = _analyze_language_inclusivity(content_data)
        cultural_analysis["sensitivity_scores"]["language"] = language_analysis["score"]
        cultural_analysis["cultural_issues"].extend(language_analysis["issues"])

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ –∏ –∫–µ–π—Å–æ–≤
    if "examples" in sensitivity_areas:
        examples_analysis = _analyze_cultural_examples(content_data, target_cultures)
        cultural_analysis["sensitivity_scores"]["examples"] = examples_analysis["score"]
        cultural_analysis["cultural_issues"].extend(examples_analysis["issues"])

    # –ê–Ω–∞–ª–∏–∑ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç–µ–π
    if "values" in sensitivity_areas:
        values_analysis = _analyze_cultural_values(content_data)
        cultural_analysis["sensitivity_scores"]["values"] = values_analysis["score"]
        cultural_analysis["cultural_issues"].extend(values_analysis["issues"])

    # –ê–Ω–∞–ª–∏–∑ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    if "accessibility" in sensitivity_areas:
        accessibility_analysis = _analyze_accessibility_features(content_data)
        cultural_analysis["sensitivity_scores"]["accessibility"] = accessibility_analysis["score"]
        cultural_analysis["cultural_issues"].extend(accessibility_analysis["issues"])

    # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –≥—Ä—É–ø–ø
    if "representation" in sensitivity_areas:
        representation_analysis = _analyze_group_representation(content_data)
        cultural_analysis["sensitivity_scores"]["representation"] = representation_analysis["score"]
        cultural_analysis["cultural_issues"].extend(representation_analysis["issues"])

    # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    avg_sensitivity = sum(cultural_analysis["sensitivity_scores"].values()) / len(cultural_analysis["sensitivity_scores"])
    if avg_sensitivity >= 80:
        cultural_analysis["overall_sensitivity"] = "excellent"
    elif avg_sensitivity >= 60:
        cultural_analysis["overall_sensitivity"] = "adequate"
    else:
        cultural_analysis["overall_sensitivity"] = "needs_improvement"

    return f"""
üåç **–ê–Ω–∞–ª–∏–∑ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**

**–û–±—â–∞—è –∫—É–ª—å—Ç—É—Ä–Ω–∞—è –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å:** {cultural_analysis['overall_sensitivity'].upper()}
**–¶–µ–ª–µ–≤—ã–µ –≥—Ä—É–ø–ø—ã:** {', '.join(target_cultures)}

**üìä –û—Ü–µ–Ω–∫–∏ –ø–æ –æ–±–ª–∞—Å—Ç—è–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:**
{_format_sensitivity_scores(cultural_analysis['sensitivity_scores'])}

**‚ö†Ô∏è –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:**
{_format_issues_list(cultural_analysis['cultural_issues'])}

**üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏:**
1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω–∫–ª—é–∑–∏–≤–Ω—ã–π —è–∑—ã–∫
2. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏ –∫–µ–π—Å—ã
3. –£—á–µ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏—è –≤ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ü–µ–Ω–Ω–æ—Å—Ç—è—Ö
4. –û–±–µ—Å–ø–µ—á–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø
5. –°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç—å

**üîÑ –ü—Ä–æ—Ü–µ—Å—Å –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏:**
- –ü—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π —Ü–µ–ª–µ–≤—ã—Ö –∫—É–ª—å—Ç—É—Ä
- –ö—É–ª—å—Ç—É—Ä–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ —Ñ–æ–∫—É—Å-–≥—Ä—É–ø–ø–∞—Ö
- –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""


async def generate_quality_report(
    ctx: RunContext[QualityGuardianDependencies],
    evaluation_results: Dict[str, Any],
    report_type: str = "comprehensive"
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        evaluation_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫
        report_type: –¢–∏–ø –æ—Ç—á–µ—Ç–∞ (summary, detailed, comprehensive)

    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ
    """
    report_sections = []

    # –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ
    executive_summary = _generate_executive_summary(evaluation_results)
    report_sections.append(f"## üìã –ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ\n{executive_summary}")

    # –°–≤–æ–¥–∫–∞ –æ—Ü–µ–Ω–æ–∫
    if report_type in ["detailed", "comprehensive"]:
        assessment_summary = _generate_assessment_summary(evaluation_results)
        report_sections.append(f"## üìä –°–≤–æ–¥–∫–∞ –æ—Ü–µ–Ω–æ–∫\n{assessment_summary}")

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    if report_type == "comprehensive":
        detailed_results = _generate_detailed_results(evaluation_results)
        report_sections.append(f"## üîç –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n{detailed_results}")

    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    priority_recommendations = _generate_priority_recommendations(evaluation_results)
    report_sections.append(f"## üéØ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n{priority_recommendations}")

    # –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
    action_plan = _generate_action_plan(evaluation_results)
    report_sections.append(f"## üìã –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π\n{action_plan}")

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç–∞
    report_metadata = _generate_report_metadata(ctx.deps, evaluation_results)
    report_sections.append(f"## üìÑ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç–∞\n{report_metadata}")

    return "\n\n".join(report_sections)


async def create_improvement_recommendations(
    ctx: RunContext[QualityGuardianDependencies],
    quality_issues: List[Dict[str, Any]],
    priority_level: str = "high"
) -> str:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞.

    Args:
        quality_issues: –°–ø–∏—Å–æ–∫ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞
        priority_level: –£—Ä–æ–≤–µ–Ω—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –¥–ª—è —Ñ–æ–∫—É—Å–∞

    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –ø–ª–∞–Ω–æ–º –≤–Ω–µ–¥—Ä–µ–Ω–∏—è
    """
    recommendations = {
        "immediate_actions": [],
        "short_term_improvements": [],
        "long_term_enhancements": [],
        "resource_requirements": {},
        "success_metrics": {}
    }

    # –ê–Ω–∞–ª–∏–∑ –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–ª–µ–º
    prioritized_issues = _prioritize_quality_issues(quality_issues, priority_level)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
    recommendations["immediate_actions"] = _generate_immediate_actions(prioritized_issues["critical"])

    # –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è
    recommendations["short_term_improvements"] = _generate_short_term_improvements(prioritized_issues["high"])

    # –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è
    recommendations["long_term_enhancements"] = _generate_long_term_enhancements(prioritized_issues["medium"])

    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º
    recommendations["resource_requirements"] = _estimate_resource_requirements(recommendations)

    # –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞
    recommendations["success_metrics"] = _define_success_metrics(recommendations)

    return f"""
üöÄ **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞:**

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç —Ñ–æ–∫—É—Å–∞:** {priority_level.upper()}
**–í—Å–µ–≥–æ –ø—Ä–æ–±–ª–µ–º –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ:** {len(quality_issues)}

## ‚ö° –ù–µ–º–µ–¥–ª–µ–Ω–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ):
{_format_action_list(recommendations['immediate_actions'])}

## üìÖ –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è (1-3 –º–µ—Å—è—Ü–∞):
{_format_improvement_list(recommendations['short_term_improvements'])}

## üéØ –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏—è (3-12 –º–µ—Å—è—Ü–µ–≤):
{_format_enhancement_list(recommendations['long_term_enhancements'])}

## üí∞ –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ—Å—É—Ä—Å–∞–º:
{_format_resource_requirements(recommendations['resource_requirements'])}

## üìà –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞:
{_format_success_metrics(recommendations['success_metrics'])}

## üîÑ –ü—Ä–æ—Ü–µ—Å—Å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è:
1. **–ù–µ–¥–µ–ª—è 1-2:** –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
2. **–ú–µ—Å—è—Ü 1:** –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏–π
3. **–ú–µ—Å—è—Ü 2-3:** –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π
4. **–ú–µ—Å—è—Ü 4-12:** –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —É—Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞–Ω–∏–π
5. **–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ:** –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫ –∏ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–ª–∞–Ω–∞
"""


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

def _check_informed_consent(content_data: Dict, target_population: str) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç–∏ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è."""
    issues = []

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–æ—Ä–º—ã —Å–æ–≥–ª–∞—Å–∏—è
    if "informed_consent" not in content_data and "consent_form" not in content_data:
        issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ñ–æ—Ä–º–∞ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–æ–ø—É–ª—è—Ü–∏–∏
    if target_population in ["children", "adolescents"] and "parental_consent" not in str(content_data):
        issues.append("–¢—Ä–µ–±—É–µ—Ç—Å—è —Å–æ–≥–ª–∞—Å–∏–µ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –¥–ª—è –Ω–µ—Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ–ª–µ—Ç–Ω–∏—Ö")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤–∞ –Ω–∞ –æ—Ç–∫–∞–∑
    consent_text = str(content_data).lower()
    if "–ø—Ä–∞–≤–æ –Ω–∞ –æ—Ç–∫–∞–∑" not in consent_text and "right to withdraw" not in consent_text:
        issues.append("–ù–µ —É–∫–∞–∑–∞–Ω–æ –ø—Ä–∞–≤–æ –Ω–∞ –æ—Ç–∫–∞–∑ –æ—Ç —É—á–∞—Å—Ç–∏—è")

    return issues


def _check_beneficence_principle(content_data: Dict) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–∞ –±–ª–∞–≥–æ–ø–æ–ª—É—á–∏—è."""
    issues = []

    # –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    harmful_patterns = ["—Å—É–∏—Ü–∏–¥", "—Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ", "–Ω–∞—Å–∏–ª–∏–µ", "—Ç—Ä–∞–≤–º–∞"]
    content_text = str(content_data).lower()

    for pattern in harmful_patterns:
        if pattern in content_text and "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ" not in content_text:
            issues.append(f"–ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è —Ç–µ–º–∞ '{pattern}' –±–µ–∑ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏
    if "–ø–æ–¥–¥–µ—Ä–∂–∫–∞" not in content_text and "support" not in content_text:
        issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–µ")

    return issues


def _check_justice_principle(content_data: Dict, target_population: str) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–∞ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ—Å—Ç–∏."""
    issues = []

    content_text = str(content_data).lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω–∫–ª—é–∑–∏–≤–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–∞
    exclusive_terms = ["–Ω–æ—Ä–º–∞–ª—å–Ω—ã–π", "–æ–±—ã—á–Ω—ã–π", "—Ç–∏–ø–∏—á–Ω—ã–π"]
    for term in exclusive_terms:
        if term in content_text:
            issues.append(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –∏—Å–∫–ª—é—á–∞—é—â–∏–π —Ç–µ—Ä–º–∏–Ω: '{term}'")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    if target_population == "multicultural" and "–∫—É–ª—å—Ç—É—Ä–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è" not in content_text:
        issues.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π —É—á–µ—Ç –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π")

    return issues


def _check_respect_principle(content_data: Dict) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–∞ —É–≤–∞–∂–µ–Ω–∏—è."""
    issues = []

    content_text = str(content_data).lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ —è–∑—ã–∫–∞
    disrespectful_patterns = ["–¥–µ—Ñ–µ–∫—Ç", "–Ω–µ–Ω–æ—Ä–º–∞–ª—å–Ω", "–æ—Ç–∫–ª–æ–Ω–µ–Ω"]
    for pattern in disrespectful_patterns:
        if pattern in content_text:
            issues.append(f"–ù–µ—É–≤–∞–∂–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞: —Å–æ–¥–µ—Ä–∂–∏—Ç '{pattern}'")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏
    if "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å" not in content_text and "privacy" not in content_text:
        issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏")

    return issues


def _generate_ethical_recommendations(issues: List[str]) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —ç—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–æ–±–ª–µ–º–∞–º."""
    recommendations = []

    for issue in issues:
        if "—Å–æ–≥–ª–∞—Å–∏–µ" in issue:
            recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è")
        elif "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ" in issue:
            recommendations.append("–í–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–µ")
        elif "–ø–æ–¥–¥–µ—Ä–∂–∫–∞" in issue:
            recommendations.append("–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–ª—É–∂–±–∞—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∏")
        elif "—è–∑—ã–∫" in issue or "—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞" in issue:
            recommendations.append("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ —É–≤–∞–∂–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏")
        else:
            recommendations.append("–ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é —ç—Ç–∏—á–µ—Å–∫—É—é —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—É")

    return list(set(recommendations))  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã


def _assess_content_validity(content_data: Dict) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏."""
    score = 70.0  # –ë–∞–∑–æ–≤—ã–π –±–∞–ª–ª

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è
    if "theoretical_basis" in content_data or "theory" in str(content_data):
        score += 10

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∞
    if "subscales" in content_data or "dimensions" in content_data:
        score += 10

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –æ—Ü–µ–Ω–∫–∏
    if "expert_review" in content_data or "validation" in str(content_data):
        score += 10

    return min(score, 100.0)


def _assess_construct_validity(content_data: Dict) -> float:
    """–û—Ü–µ–Ω–∫–∞ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏."""
    score = 60.0

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–∞–∫—Ç–æ—Ä–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    if "factor_structure" in content_data:
        score += 15

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω–≤–µ—Ä–≥–µ–Ω—Ç–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    if "convergent_validity" in content_data:
        score += 10

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç–Ω–æ–π –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏
    if "discriminant_validity" in content_data:
        score += 15

    return min(score, 100.0)


def _assess_reliability(content_data: Dict) -> float:
    """–û—Ü–µ–Ω–∫–∞ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏."""
    score = 60.0

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
    if "cronbach_alpha" in content_data:
        alpha = content_data.get("cronbach_alpha", 0.0)
        if alpha >= 0.90:
            score += 20
        elif alpha >= 0.80:
            score += 15
        elif alpha >= 0.70:
            score += 10

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Ç–µ—Å—Ç–æ–≤–æ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
    if "test_retest" in content_data:
        score += 10

    return min(score, 100.0)


def _assess_theoretical_basis(content_data: Dict) -> float:
    """–û—Ü–µ–Ω–∫–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏—è."""
    score = 50.0

    content_text = str(content_data).lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–µ–æ—Ä–∏–∏
    if "—Ç–µ–æ—Ä–∏—è" in content_text or "theory" in content_text:
        score += 20

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–≥–æ –æ–±–∑–æ—Ä–∞
    if "literature" in content_text or "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è" in content_text:
        score += 15

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    if any(year in content_text for year in ["2020", "2021", "2022", "2023", "2024"]):
        score += 15

    return min(score, 100.0)


def _analyze_empirical_support(content_data: Dict, evidence_level: str) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏."""
    gaps = []

    # –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –ø–æ —É—Ä–æ–≤–Ω—é –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤
    if evidence_level == "comprehensive":
        required_studies = ["RCT", "meta_analysis", "longitudinal"]
        for study_type in required_studies:
            if study_type not in str(content_data):
                gaps.append(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç {study_type} –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")

    elif evidence_level == "standard":
        if "research" not in str(content_data).lower():
            gaps.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

    return {"gaps": gaps}


def _analyze_psychological_risks(content_data: Dict, sensitivity_level: str) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–æ–≤."""
    risks = {
        "score": 0,
        "issues": []
    }

    content_text = str(content_data).lower()

    # –í—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–º—ã
    high_risk_topics = ["—Å—É–∏—Ü–∏–¥", "—Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ", "–Ω–∞—Å–∏–ª–∏–µ", "trauma", "abuse"]
    for topic in high_risk_topics:
        if topic in content_text:
            risks["score"] += 30
            risks["issues"].append(f"–í—ã—Å–æ–∫–æ—Ä–∏—Å–∫–æ–≤–∞–Ω–Ω–∞—è —Ç–µ–º–∞: {topic}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π
    if risks["score"] > 0 and "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ" not in content_text:
        risks["score"] += 20
        risks["issues"].append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ —Ä–∏—Å–∫–∞—Ö")

    return risks


def _analyze_privacy_risks(content_data: Dict) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏."""
    risks = {
        "score": 0,
        "issues": []
    }

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–±–æ—Ä–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    personal_data_patterns = ["–∏–º—è", "–∞–¥—Ä–µ—Å", "—Ç–µ–ª–µ—Ñ–æ–Ω", "email", "–¥–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è"]
    content_text = str(content_data).lower()

    for pattern in personal_data_patterns:
        if pattern in content_text and "—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ" not in content_text:
            risks["score"] += 15
            risks["issues"].append(f"–°–±–æ—Ä {pattern} –±–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∑–∞—â–∏—Ç—ã –¥–∞–Ω–Ω—ã—Ö")

    return risks


def _analyze_vulnerability_risks(content_data: Dict) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤ –¥–ª—è —É—è–∑–≤–∏–º—ã—Ö –≥—Ä—É–ø–ø."""
    risks = {
        "score": 0,
        "issues": []
    }

    content_text = str(content_data).lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—á–µ—Ç–∞ –æ—Å–æ–±—ã—Ö –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π
    vulnerable_groups = ["–¥–µ—Ç–∏", "–ø–æ–∂–∏–ª—ã–µ", "–∏–Ω–≤–∞–ª–∏–¥—ã", "–ø—Å–∏—Ö–∏—á–µ—Å–∫–∏–µ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"]
    for group in vulnerable_groups:
        if group in content_text and "–∞–¥–∞–ø—Ç–∞—Ü–∏—è" not in content_text:
            risks["score"] += 20
            risks["issues"].append(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π —É—á–µ—Ç –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–µ–π –≥—Ä—É–ø–ø—ã: {group}")

    return risks


def _analyze_crisis_triggers(content_data: Dict) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –∫—Ä–∏–∑–∏—Å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π."""
    risks = {
        "score": 0,
        "issues": []
    }

    content_text = str(content_data).lower()

    # –ü–æ–∏—Å–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
    crisis_triggers = ["—Å–º–µ—Ä—Ç—å", "—Ä–∞–∑–≤–æ–¥", "—É–≤–æ–ª—å–Ω–µ–Ω–∏–µ", "–±–æ–ª–µ–∑–Ω—å", "–ø–æ—Ç–µ—Ä—è"]
    for trigger in crisis_triggers:
        if trigger in content_text and "–ø–æ–¥–¥–µ—Ä–∂–∫–∞" not in content_text:
            risks["score"] += 25
            risks["issues"].append(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä –±–µ–∑ –ø–æ–¥–¥–µ—Ä–∂–∫–∏: {trigger}")

    return risks


def _generate_protection_measures(safety_issues: List[str]) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞—â–∏—Ç–Ω—ã—Ö –º–µ—Ä."""
    measures = []

    for issue in safety_issues:
        if "–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ" in issue:
            measures.append("–î–æ–±–∞–≤–∏—Ç—å —Ç—Ä–∏–≥–≥–µ—Ä–Ω—ã–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è")
        elif "–ø–æ–¥–¥–µ—Ä–∂–∫–∞" in issue:
            measures.append("–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç—ã —Å–ª—É–∂–± –ø–æ–¥–¥–µ—Ä–∂–∫–∏")
        elif "–¥–∞–Ω–Ω—ã—Ö" in issue:
            measures.append("–£—Å–∏–ª–∏—Ç—å –∑–∞—â–∏—Ç—É –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
        elif "–∞–¥–∞–ø—Ç–∞—Ü–∏—è" in issue:
            measures.append("–°–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è —É—è–∑–≤–∏–º—ã—Ö –≥—Ä—É–ø–ø")

    return list(set(measures))


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

def _format_issues_list(issues: List[str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–±–ª–µ–º."""
    if not issues:
        return "‚úÖ –ü—Ä–æ–±–ª–µ–º—ã –Ω–µ –≤—ã—è–≤–ª–µ–Ω—ã"
    return "\n".join([f"‚ùå {issue}" for issue in issues])


def _format_recommendations_list(recommendations: List[str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
    if not recommendations:
        return "‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è"
    return "\n".join([f"üí° {rec}" for rec in recommendations])


def _format_validity_scores(scores: Dict[str, float]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏."""
    formatted = []
    for criterion, score in scores.items():
        emoji = "‚úÖ" if score >= 80 else "‚ö†Ô∏è" if score >= 60 else "‚ùå"
        formatted.append(f"{emoji} {criterion.replace('_', ' ').title()}: {score:.1f}/100")
    return "\n".join(formatted)


def _format_evidence_gaps(gaps: List[str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –≤ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞—Ö."""
    if not gaps:
        return "‚úÖ –ó–Ω–∞—á–∏–º—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–µ –≤—ã—è–≤–ª–µ–Ω—ã"
    return "\n".join([f"üìà {gap}" for gap in gaps])


def _format_risk_scores(scores: Dict[str, float]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫ —Ä–∏—Å–∫–æ–≤."""
    formatted = []
    for category, score in scores.items():
        if score >= 80:
            emoji = "üö®"
            level = "–í–´–°–û–ö–ò–ô"
        elif score >= 50:
            emoji = "‚ö†Ô∏è"
            level = "–°–†–ï–î–ù–ò–ô"
        else:
            emoji = "‚úÖ"
            level = "–ù–ò–ó–ö–ò–ô"
        formatted.append(f"{emoji} {category.replace('_', ' ').title()}: {score:.1f}/100 ({level})")
    return "\n".join(formatted)


def _format_protection_measures(measures: List[str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞—â–∏—Ç–Ω—ã—Ö –º–µ—Ä."""
    if not measures:
        return "‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ä—ã –∑–∞—â–∏—Ç—ã –Ω–µ —Ç—Ä–µ–±—É—é—Ç—Å—è"
    return "\n".join([f"üõ°Ô∏è {measure}" for measure in measures])


# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∞ –±—É–¥—É—Ç –¥–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ –º–µ—Ä–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏...