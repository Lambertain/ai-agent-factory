# Module 01: Application & API Performance Optimization

## üìñ Overview

This module covers universal performance optimization patterns for application code and API endpoints. Focused on backend optimization, caching strategies, asynchronous processing, and API response optimization.

**Key Topics:**
- Python performance patterns (caching, async/await, generators)
- Memory-efficient code design
- API optimization (FastAPI, response caching, background tasks)
- Rate limiting and compression
- Database connection pooling

**Technologies Covered:** Python, FastAPI, Redis, asyncio, psutil, aiohttp

---

## üêç Python Application Performance Patterns

### 1. Caching Strategies

```python
# LRU Cache –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
from functools import lru_cache

@lru_cache(maxsize=1000)
def expensive_computation(n: int) -> int:
    """
    LRU cache –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π.

    Args:
        n: –ü–∞—Ä–∞–º–µ—Ç—Ä –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

    Returns:
        int: –†–µ–∑—É–ª—å—Ç–∞—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è

    Performance impact:
        - –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤: O(n) –≤—Ä–µ–º—è
        - –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤—ã–∑–æ–≤—ã: O(1) –≤—Ä–µ–º—è
        - Memory overhead: —Ö—Ä–∞–Ω–∏—Ç –¥–æ 1000 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    """
    return sum(i * i for i in range(n))

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
result = expensive_computation(10000)  # –ü–µ—Ä–≤—ã–π –≤—ã–∑–æ–≤ - –º–µ–¥–ª–µ–Ω–Ω–æ
result = expensive_computation(10000)  # –ò–∑ –∫—ç—à–∞ - –º–≥–Ω–æ–≤–µ–Ω–Ω–æ
```

**Best Practices:**
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ `maxsize` –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è memory usage
- –ü—Ä–∏–º–µ–Ω—è–π—Ç–µ –¥–ª—è pure functions (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏)
- –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ cache hit rate —á–µ—Ä–µ–∑ `cache_info()`

---

### 2. Async/Await –¥–ª—è I/O Bound –û–ø–µ—Ä–∞—Ü–∏–π

```python
import asyncio
import aiohttp
from typing import List, Dict, Any

async def fetch_data(session: aiohttp.ClientSession, url: str) -> Dict[str, Any]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö.

    Args:
        session: Aiohttp client session
        url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

    Returns:
        Dict[str, Any]: –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    """
    async with session.get(url) as response:
        return await response.json()

async def batch_api_calls(urls: List[str]) -> List[Dict[str, Any]]:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è throughput.

    Args:
        urls: –°–ø–∏—Å–æ–∫ URLs –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏

    Returns:
        List[Dict[str, Any]]: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    Performance impact:
        - Sequential: N * 200ms = N * 200ms
        - Concurrent: ~200ms (–≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)
        - Speedup: N times faster
    """
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_data(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
urls = ['https://api.example.com/data1', 'https://api.example.com/data2']
results = asyncio.run(batch_api_calls(urls))
```

**When to Use:**
- ‚úÖ Multiple API calls
- ‚úÖ Database queries (—Å asyncpg, motor)
- ‚úÖ File I/O operations
- ‚ùå CPU-bound operations (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ multiprocessing)

---

### 3. Memory-Efficient Generators

```python
def process_large_file(filename: str):
    """
    Generator –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–∞–º—è—Ç—å.

    Args:
        filename: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É

    Yields:
        –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏

    Memory impact:
        - Without generator: –í–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å GB)
        - With generator: –¢–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∞—è —Å—Ç—Ä–æ–∫–∞ (~KB)
    """
    with open(filename, 'r') as f:
        for line in f:
            processed = process_line(line.strip())
            yield processed

def process_line(line: str) -> dict:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏."""
    return {'data': line.upper(), 'length': len(line)}

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ - memory-efficient iteration
for result in process_large_file('large_data.txt'):
    save_to_database(result)
```

**Advantages:**
- Constant memory usage –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
- Lazy evaluation - –æ–±—Ä–∞–±–æ—Ç–∫–∞ on-demand
- –ö–æ–º–ø–æ–∑–∏—Ä—É–µ–º–æ—Å—Ç—å - –º–æ–∂–Ω–æ chain generators

---

### 4. Performance Monitoring Decorator

```python
import time
import psutil
from functools import wraps
from typing import Any, Callable

def performance_monitor(func: Callable) -> Callable:
    """
    Decorator –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π.

    Tracks:
        - Execution time
        - Memory usage delta
        - CPU usage

    Args:
        func: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

    Returns:
        Wrapped function —Å performance tracking
    """
    @wraps(func)
    def wrapper(*args: Any, **kwargs: Any) -> Any:
        # Baseline measurements
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss

        try:
            result = func(*args, **kwargs)
            return result
        finally:
            # Final measurements
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss

            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory

            # Logging
            print(f"{func.__name__}:")
            print(f"  Time: {execution_time:.4f}s")
            print(f"  Memory: {memory_delta/1024/1024:.2f}MB")

    return wrapper

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
@performance_monitor
def heavy_computation(n: int) -> int:
    return sum(i * i for i in range(n))

result = heavy_computation(1000000)
# Output:
# heavy_computation:
#   Time: 0.1234s
#   Memory: 45.23MB
```

---

### 5. Database Connection Pooling

```python
import asyncpg
from typing import List, Any

class DatabasePool:
    """
    –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ DB access.

    Benefits:
        - Reuse connections –≤–º–µ—Å—Ç–æ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤—ã—Ö
        - Connection limit control
        - Automatic connection health checks
    """

    def __init__(self, connection_string: str, pool_size: int = 10):
        """
        Initialize database pool.

        Args:
            connection_string: PostgreSQL connection string
            pool_size: Maximum pool size
        """
        self.connection_string = connection_string
        self.pool_size = pool_size
        self.pool = None

    async def initialize(self):
        """Create connection pool."""
        self.pool = await asyncpg.create_pool(
            self.connection_string,
            min_size=self.pool_size // 2,  # –ú–∏–Ω–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            max_size=self.pool_size,       # –ú–∞–∫—Å–∏–º—É–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π
            command_timeout=30,            # Timeout –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
            max_queries=50000,             # Max queries per connection
            max_inactive_connection_lifetime=300  # Recycle old connections
        )

    async def execute_query(self, query: str, *params: Any) -> List[Any]:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ pool.

        Args:
            query: SQL query
            *params: Query parameters

        Returns:
            List[Any]: Query results

        Performance:
            - Without pooling: ~50ms per query (connection overhead)
            - With pooling: ~2ms per query
        """
        async with self.pool.acquire() as connection:
            return await connection.fetch(query, *params)

    async def close(self):
        """Close pool and all connections."""
        if self.pool:
            await self.pool.close()

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    db = DatabasePool('postgresql://user:pass@localhost/dbname')
    await db.initialize()

    # Efficient query execution
    users = await db.execute_query("SELECT * FROM users WHERE active = $1", True)

    await db.close()
```

---

## üöÄ API Performance Optimization

### 1. Response Caching

```python
from fastapi import FastAPI, Depends
from redis import Redis
import json
from typing import Optional, Dict, Any

app = FastAPI()
redis_client = Redis(host='localhost', port=6379, decode_responses=True)

async def get_cached_response(cache_key: str) -> Optional[Dict[str, Any]]:
    """
    –ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞.

    Args:
        cache_key: –ö–ª—é—á –∫—ç—à–∞

    Returns:
        Optional[Dict]: –ö—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–ª–∏ None
    """
    cached = redis_client.get(cache_key)
    return json.loads(cached) if cached else None

async def set_cached_response(cache_key: str, data: Dict[str, Any], ttl: int = 3600):
    """
    –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å TTL.

    Args:
        cache_key: –ö–ª—é—á –∫—ç—à–∞
        data: –î–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        ttl: Time to live –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (default: 1 hour)
    """
    redis_client.setex(cache_key, ttl, json.dumps(data))

@app.get("/api/user/{user_id}")
async def get_user_data(user_id: int) -> Dict[str, Any]:
    """
    API endpoint —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

    Performance impact:
        - Cache HIT: ~1ms (Redis lookup)
        - Cache MISS: ~50ms (database query)
        - Cache hit ratio: >80% after warmup
    """
    cache_key = f"user:{user_id}"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_data = await get_cached_response(cache_key)
    if cached_data:
        return cached_data

    # Database query
    user_data = await database.fetch_one(
        "SELECT * FROM users WHERE id = $1", user_id
    )

    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    await set_cached_response(cache_key, dict(user_data), ttl=300)

    return dict(user_data)
```

---

### 2. Database Query Optimization

```python
from typing import List

async def get_user_posts_optimized(user_id: int) -> List[Dict[str, Any]]:
    """
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ –≤—ã–±–æ—Ä–æ—á–Ω—ã–º–∏ –ø–æ–ª—è–º–∏.

    Optimizations:
        - JOIN –≤–º–µ—Å—Ç–æ N+1 queries
        - –í—ã–±–æ—Ä–æ—á–Ω—ã–µ –ø–æ–ª—è (SELECT specific columns)
        - Index hint (WHERE –Ω–∞ indexed column)
        - LIMIT –¥–ª—è pagination

    Performance:
        - N+1 queries: 1 + N * 50ms = 1000ms –¥–ª—è 20 –ø–æ—Å—Ç–æ–≤
        - Optimized JOIN: 1 * 50ms = 50ms (20x faster!)
    """
    cache_key = f"user_posts:{user_id}"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_data = await get_cached_response(cache_key)
    if cached_data:
        return cached_data

    # Optimized query - JOIN –≤–º–µ—Å—Ç–æ N+1
    query = """
    SELECT
        p.id,
        p.title,
        p.created_at,
        u.name as author_name
    FROM posts p
    JOIN users u ON p.user_id = u.id
    WHERE p.user_id = $1 AND p.status = 'published'
    ORDER BY p.created_at DESC
    LIMIT 20
    """

    posts = await database.fetch_all(query, user_id)
    result = [dict(post) for post in posts]

    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ 5 –º–∏–Ω—É—Ç
    await set_cached_response(cache_key, result, ttl=300)

    return result
```

---

### 3. Background Tasks

```python
from fastapi import BackgroundTasks
import uuid

def generate_task_id() -> str:
    """Generate unique task ID."""
    return str(uuid.uuid4())

@app.post("/process-upload/")
async def process_upload(
    file_data: Dict[str, Any],
    background_tasks: BackgroundTasks
) -> Dict[str, str]:
    """
    API endpoint —Å background –æ–±—Ä–∞–±–æ—Ç–∫–æ–π.

    Benefits:
        - –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (<100ms)
        - –¢—è–∂–µ–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ñ–æ–Ω–µ
        - User experience improvement

    Response time:
        - Without background: 5000ms (–∂–¥—ë–º –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        - With background: 50ms (–≤–æ–∑–≤—Ä–∞—â–∞–µ–º task_id)
        - User satisfaction: 100x better!
    """
    # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    task_id = generate_task_id()

    # –¢—è–∂–µ–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ñ–æ–Ω–µ
    background_tasks.add_task(process_file, file_data, task_id)

    return {
        "task_id": task_id,
        "status": "processing",
        "message": "File processing started"
    }

async def process_file(file_data: Dict[str, Any], task_id: str):
    """
    Background task –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞.

    Args:
        file_data: –î–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
        task_id: ID –∑–∞–¥–∞—á–∏ –¥–ª—è tracking
    """
    try:
        # –¢—è–∂–µ–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (5+ —Å–µ–∫—É–Ω–¥)
        result = await heavy_file_processing(file_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Redis
        await redis_client.setex(f"task:{task_id}", 3600, json.dumps({
            "status": "completed",
            "result": result
        }))
    except Exception as e:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫—É
        await redis_client.setex(f"task:{task_id}", 3600, json.dumps({
            "status": "failed",
            "error": str(e)
        }))

async def heavy_file_processing(file_data: Dict[str, Any]) -> Dict[str, Any]:
    """Simulate heavy processing."""
    await asyncio.sleep(5)  # –ò–º–∏—Ç–∞—Ü–∏—è —Ç—è–∂–µ–ª–æ–π —Ä–∞–±–æ—Ç—ã
    return {"processed": True, "rows": 1000}
```

---

### 4. Rate Limiting

```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from fastapi import Request

# Initialize rate limiter
limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.get("/api/data")
@limiter.limit("100/minute")
async def get_data(request: Request) -> Dict[str, Any]:
    """
    Rate-limited API endpoint.

    Limits:
        - 100 requests per minute per IP
        - Automatic 429 response –ø—Ä–∏ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–∏

    Protection:
        - Prevents API abuse
        - Protects infrastructure from overload
        - Fair usage enforcement
    """
    return await fetch_data()
```

---

### 5. Response Compression

```python
from fastapi.middleware.gzip import GZipMiddleware

# Add compression middleware
app.add_middleware(
    GZipMiddleware,
    minimum_size=1000,  # Compress responses > 1KB
    compresslevel=6     # Balance between speed and compression (1-9)
)

# Benefits:
# - Reduces bandwidth by 70-90% for text responses
# - Faster transfer over slow connections
# - Lower cloud egress costs
```

---

## üìä Best Practices

### Application Performance

‚úÖ **Caching Strategy:**
- Use LRU cache –¥–ª—è pure functions
- Monitor cache hit ratio (target: >80%)
- Set appropriate TTL values

‚úÖ **Async/Await:**
- Use –¥–ª—è I/O-bound operations
- Avoid –¥–ª—è CPU-bound tasks (use multiprocessing)
- Always handle exceptions –≤ async code

‚úÖ **Memory Management:**
- Use generators –¥–ª—è –±–æ–ª—å—à–∏—Ö datasets
- Profile memory usage regularly
- Avoid global state accumulation

### API Performance

‚úÖ **Response Time Targets:**
- < 100ms –¥–ª—è simple GET requests
- < 200ms –¥–ª—è database queries
- < 50ms –¥–ª—è cached responses

‚úÖ **Database Optimization:**
- Use connection pooling
- Implement response caching
- Optimize N+1 queries

‚úÖ **Background Processing:**
- Offload heavy tasks to background
- Provide task status endpoints
- Set appropriate timeouts

---

**–í–µ—Ä—Å–∏—è –º–æ–¥—É–ª—è:** 1.0
**–î–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è:** 2025-10-15
**–ê–≤—Ç–æ—Ä:** Archon Blueprint Architect
**–ü—Ä–æ–µ–∫—Ç:** AI Agent Factory - Performance Optimization Agent Modularization (6/7)

**–ù–∞–≤–∏–≥–∞—Ü–∏—è:**
- [‚Üí Module 02: Database Performance](02_database_performance.md)
- [‚Üë –ù–∞–∑–∞–¥ –∫ –≥–ª–∞–≤–Ω–æ–π](../performance_optimization_agent_knowledge.md)

**Tags:** application-performance, api-optimization, python, fastapi, caching, async-await, background-tasks, rate-limiting
