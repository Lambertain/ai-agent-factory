# Performance Optimization Agent Knowledge Base

## –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç

–¢—ã - —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π Performance Optimization Agent, —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –∏ —Å–∏—Å—Ç–µ–º. –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—à—å—Å—è –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ bottlenecks, –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–µ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è performance.

**–¢–≤–æ—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞:**
- –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –ª—é–±—ã—Ö –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö (SQL, NoSQL, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ)
- Frontend performance (bundle size, Core Web Vitals, lazy loading)
- Backend optimization (API performance, memory management, concurrency)
- Infrastructure optimization (CDN, caching, load balancing)
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ alerting —Å–∏—Å—Ç–µ–º –≤ real-time

**–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–∞:**
- –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ª—é–±—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Å—Ç–µ–∫–∏
- Cross-platform optimization (web, mobile, desktop)
- Cloud –∏ on-premise environments
- Microservices –∏ monolithic architectures
- Multi-language performance analysis (Python, Node.js, Java, Go, .NET, etc.)

## –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞–±–æ—Ç—ã

### üîÑ Reflection Pattern
–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:
1. –ò–∑–º–µ—Ä—è—é baseline –º–µ—Ç—Ä–∏–∫–∏ –ø–µ—Ä–µ–¥ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏
2. –ü—Ä–∏–º–µ–Ω—è—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –∏–∑–º–µ—Ä—è—é impact
3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é trade-offs –∏ –ø–æ–±–æ—á–Ω—ã–µ —ç—Ñ—Ñ–µ–∫—Ç—ã
4. –ü—Ä–µ–¥–ª–∞–≥–∞—é –¥–∞–ª—å–Ω–µ–π—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
5. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É—é best practices –¥–ª—è –±—É–¥—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

### üõ†Ô∏è Tool Use Pattern
- Performance profiling tools (cProfile, perf, Chrome DevTools)
- Monitoring systems (Prometheus, Grafana, DataDog, New Relic)
- Load testing tools (k6, Artillery, JMeter, locust)
- Database optimization tools (EXPLAIN PLAN, pg_stat_statements)
- Code analysis tools (SonarQube, CodeClimate)

### üìã Planning Pattern
1. Baseline measurement –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ performance goals
2. –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è bottlenecks —á–µ—Ä–µ–∑ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
3. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è optimizations –ø–æ impact/effort
4. –ü–æ—ç—Ç–∞–ø–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å continuous monitoring
5. A/B testing –¥–ª—è validation —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### üë• Multi-Agent Collaboration
- **–° Database Agent**: SQL query optimization –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ
- **–° Frontend Agent**: bundle optimization –∏ Core Web Vitals
- **–° DevOps Agent**: infrastructure tuning –∏ scaling strategies
- **–° Security Agent**: security-performance trade-offs –∞–Ω–∞–ª–∏–∑

## –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ Performance Optimization Patterns

### Application Performance Optimization

```python
# Python Performance Patterns
import asyncio
import cProfile
import psutil
from functools import lru_cache, wraps
from typing import Dict, Any, Optional
import time

# 1. Caching Strategies
@lru_cache(maxsize=1000)
def expensive_computation(n: int) -> int:
    """LRU cache –¥–ª—è –¥–æ—Ä–æ–≥–∏—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π."""
    return sum(i * i for i in range(n))

# 2. Async/Await –¥–ª—è I/O bound –æ–ø–µ—Ä–∞—Ü–∏–π
async def batch_api_calls(urls: list[str]) -> list[Dict[str, Any]]:
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ HTTP –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è throughput."""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_data(session, url) for url in urls]
        return await asyncio.gather(*tasks)

# 3. Memory-efficient generators
def process_large_file(filename: str):
    """Generator –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ –±–µ–∑ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ø–∞–º—è—Ç—å."""
    with open(filename, 'r') as f:
        for line in f:
            yield process_line(line.strip())

# 4. Performance monitoring decorator
def performance_monitor(func):
    """Decorator –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π."""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.perf_counter()
        start_memory = psutil.Process().memory_info().rss

        try:
            result = func(*args, **kwargs)
            return result
        finally:
            end_time = time.perf_counter()
            end_memory = psutil.Process().memory_info().rss

            execution_time = end_time - start_time
            memory_delta = end_memory - start_memory

            print(f"{func.__name__}: {execution_time:.4f}s, {memory_delta/1024/1024:.2f}MB")

    return wrapper

# 5. Database connection pooling
class DatabasePool:
    """–ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ DB access."""

    def __init__(self, connection_string: str, pool_size: int = 10):
        self.pool = asyncpg.create_pool(
            connection_string,
            min_size=pool_size//2,
            max_size=pool_size,
            command_timeout=30
        )

    async def execute_query(self, query: str, *params) -> list:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ pool."""
        async with self.pool.acquire() as connection:
            return await connection.fetch(query, *params)
```

### Database Performance Optimization

```sql
-- Universal Database Optimization Patterns

-- 1. Index Optimization
-- Composite indexes –¥–ª—è multi-column queries
CREATE INDEX CONCURRENTLY idx_orders_user_status_date
ON orders (user_id, status, created_at DESC)
WHERE status IN ('pending', 'processing');

-- Partial indexes –¥–ª—è filtered queries
CREATE INDEX CONCURRENTLY idx_active_users
ON users (last_login_at DESC)
WHERE is_active = true;

-- 2. Query Optimization
-- –ò–∑–±–µ–≥–∞–Ω–∏–µ N+1 queries —á–µ—Ä–µ–∑ JOINs
SELECT u.id, u.name, p.title, p.created_at
FROM users u
LEFT JOIN posts p ON p.user_id = u.id
WHERE u.is_active = true
ORDER BY p.created_at DESC;

-- Pagination —Å cursor-based approach –¥–ª—è –±–æ–ª—å—à–∏—Ö datasets
SELECT id, name, created_at
FROM users
WHERE created_at < $1  -- cursor
ORDER BY created_at DESC
LIMIT 20;

-- 3. Materialized Views –¥–ª—è complex aggregations
CREATE MATERIALIZED VIEW user_statistics AS
SELECT
    user_id,
    COUNT(*) as total_posts,
    AVG(view_count) as avg_views,
    MAX(created_at) as last_post_date
FROM posts
WHERE status = 'published'
GROUP BY user_id;

-- Refresh strategy
REFRESH MATERIALIZED VIEW CONCURRENTLY user_statistics;

-- 4. Partitioning –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ç–∞–±–ª–∏—Ü
CREATE TABLE events (
    id BIGSERIAL,
    user_id INTEGER,
    event_type VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
) PARTITION BY RANGE (created_at);

-- Monthly partitions
CREATE TABLE events_202501 PARTITION OF events
FOR VALUES FROM ('2025-01-01') TO ('2025-02-01');

-- 5. Connection pooling configuration
-- PgBouncer configuration
[databases]
app_db = host=localhost port=5432 dbname=app_production

[pgbouncer]
pool_mode = transaction
default_pool_size = 25
max_client_conn = 100
server_idle_timeout = 600
```

### Frontend Performance Optimization

```javascript
// Modern Frontend Performance Patterns

// 1. Code Splitting –∏ Lazy Loading
import { lazy, Suspense } from 'react';

const LazyComponent = lazy(() =>
  import('./HeavyComponent').then(module => ({
    default: module.HeavyComponent
  }))
);

function App() {
  return (
    <Suspense fallback={<LoadingSkeleton />}>
      <LazyComponent />
    </Suspense>
  );
}

// 2. Virtual Scrolling –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–ø–∏—Å–∫–æ–≤
import { FixedSizeList as List } from 'react-window';

function VirtualizedList({ items }) {
  const Row = ({ index, style }) => (
    <div style={style}>
      <ItemComponent item={items[index]} />
    </div>
  );

  return (
    <List
      height={600}
      itemCount={items.length}
      itemSize={80}
      width="100%"
    >
      {Row}
    </List>
  );
}

// 3. Image optimization —Å —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ñ–æ—Ä–º–∞—Ç–∞–º–∏
function OptimizedImage({ src, alt, ...props }) {
  return (
    <picture>
      <source srcSet={`${src}.avif`} type="image/avif" />
      <source srcSet={`${src}.webp`} type="image/webp" />
      <img src={`${src}.jpg`} alt={alt} loading="lazy" {...props} />
    </picture>
  );
}

// 4. Service Worker –¥–ª—è caching
// sw.js
const CACHE_NAME = 'app-v1';
const urlsToCache = [
  '/',
  '/static/css/main.css',
  '/static/js/main.js'
];

self.addEventListener('install', event => {
  event.waitUntil(
    caches.open(CACHE_NAME)
      .then(cache => cache.addAll(urlsToCache))
  );
});

self.addEventListener('fetch', event => {
  event.respondWith(
    caches.match(event.request)
      .then(response => {
        // Cache hit - return response
        if (response) {
          return response;
        }
        return fetch(event.request);
      })
  );
});

// 5. Bundle optimization
// webpack.config.js optimization
module.exports = {
  optimization: {
    splitChunks: {
      chunks: 'all',
      cacheGroups: {
        vendor: {
          test: /[\\/]node_modules[\\/]/,
          name: 'vendors',
          chunks: 'all',
        },
        common: {
          minChunks: 2,
          chunks: 'all',
          enforce: true
        }
      }
    },
    usedExports: true,
    sideEffects: false
  },
  resolve: {
    alias: {
      '@': path.resolve(__dirname, 'src')
    }
  }
};
```

### API Performance Optimization

```python
# Universal API Performance Patterns
from fastapi import FastAPI, Depends, BackgroundTasks
from redis import Redis
import asyncio
from typing import List, Optional
import json

app = FastAPI()
redis_client = Redis(host='localhost', port=6379, decode_responses=True)

# 1. Response Caching
async def get_cached_response(cache_key: str) -> Optional[dict]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
    cached = await redis_client.get(cache_key)
    return json.loads(cached) if cached else None

async def set_cached_response(cache_key: str, data: dict, ttl: int = 3600):
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ —Å TTL."""
    await redis_client.setex(cache_key, ttl, json.dumps(data))

# 2. Database Query Optimization
async def get_user_posts_optimized(user_id: int) -> List[dict]:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π –∏ –≤—ã–±–æ—Ä–æ—á–Ω—ã–º–∏ –ø–æ–ª—è–º–∏."""
    cache_key = f"user_posts:{user_id}"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached_data = await get_cached_response(cache_key)
    if cached_data:
        return cached_data

    # –í—ã–±–æ—Ä–æ—á–Ω—ã–µ –ø–æ–ª—è + JOIN –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
    query = """
    SELECT p.id, p.title, p.created_at, u.name as author_name
    FROM posts p
    JOIN users u ON p.user_id = u.id
    WHERE p.user_id = $1 AND p.status = 'published'
    ORDER BY p.created_at DESC
    LIMIT 20
    """

    posts = await database.fetch_all(query, user_id)
    result = [dict(post) for post in posts]

    # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    await set_cached_response(cache_key, result, ttl=300)
    return result

# 3. Background Tasks –¥–ª—è —Ç—è–∂–µ–ª—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
@app.post("/process-upload/")
async def process_upload(
    file_data: dict,
    background_tasks: BackgroundTasks
):
    """API endpoint —Å background –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
    # –ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
    task_id = generate_task_id()

    # –¢—è–∂–µ–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ñ–æ–Ω–µ
    background_tasks.add_task(process_file, file_data, task_id)

    return {"task_id": task_id, "status": "processing"}

async def process_file(file_data: dict, task_id: str):
    """Background task –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞."""
    try:
        # –¢—è–∂–µ–ª–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        result = await heavy_file_processing(file_data)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        await redis_client.setex(f"task:{task_id}", 3600, json.dumps({
            "status": "completed",
            "result": result
        }))
    except Exception as e:
        await redis_client.setex(f"task:{task_id}", 3600, json.dumps({
            "status": "failed",
            "error": str(e)
        }))

# 4. Rate Limiting –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.get("/api/data")
@limiter.limit("100/minute")
async def get_data(request: Request):
    """Rate-limited API endpoint."""
    return await fetch_data()

# 5. Response Compression
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(GZipMiddleware, minimum_size=1000)
```

### Infrastructure Performance Optimization

```yaml
# Docker Performance Optimization
# Dockerfile
FROM python:3.11-slim as builder

# Multi-stage build –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è image size
WORKDIR /app
COPY requirements.txt .
RUN pip wheel --no-cache-dir --no-deps --wheel-dir /app/wheels -r requirements.txt

FROM python:3.11-slim

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ runtime dependencies
COPY --from=builder /app/wheels /wheels
RUN pip install --no-cache /wheels/*

# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è production
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1

USER 1000
CMD ["gunicorn", "--bind", "0.0.0.0:8000", "--workers", "4", "app:app"]

---
# Kubernetes Performance Configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-deployment
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: app
        image: app:latest
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        env:
        - name: WORKERS
          value: "2"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 20

---
# Nginx Performance Configuration
server {
    listen 80;
    server_name app.example.com;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

    # Caching static assets
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
    }

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;

    location /api/ {
        limit_req zone=api burst=20 nodelay;
        proxy_pass http://backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    # Connection pooling
    upstream backend {
        least_conn;
        server app1:8000 max_fails=3 fail_timeout=30s;
        server app2:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }
}
```

## Domain-Specific Performance Patterns

### E-commerce Performance Optimization

```python
# E-commerce —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
class EcommercePerformanceOptimizer:
    """Performance optimization –¥–ª—è e-commerce –ø–ª–∞—Ç—Ñ–æ—Ä–º."""

    async def optimize_product_search(self, query: str, filters: dict) -> dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
        cache_key = f"search:{hash(query + str(sorted(filters.items())))}"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        cached_result = await self.redis.get(cache_key)
        if cached_result:
            return json.loads(cached_result)

        # Elasticsearch query —Å optimized aggregations
        search_body = {
            "query": {
                "bool": {
                    "must": [
                        {"multi_match": {
                            "query": query,
                            "fields": ["title^3", "description", "category"],
                            "type": "best_fields"
                        }}
                    ],
                    "filter": self._build_filters(filters)
                }
            },
            "aggs": {
                "categories": {"terms": {"field": "category.keyword", "size": 10}},
                "price_ranges": {"range": {"field": "price", "ranges": [
                    {"to": 50}, {"from": 50, "to": 100}, {"from": 100}
                ]}}
            },
            "size": 20,
            "_source": ["id", "title", "price", "image_url", "rating"]
        }

        result = await self.elasticsearch.search(index="products", body=search_body)

        # –ö—ç—à–∏—Ä—É–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ 5 –º–∏–Ω—É—Ç
        await self.redis.setex(cache_key, 300, json.dumps(result))
        return result

    async def optimize_cart_operations(self, user_id: str, item_id: str, quantity: int):
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –∫–æ—Ä–∑–∏–Ω–æ–π —á–µ—Ä–µ–∑ Redis."""
        cart_key = f"cart:{user_id}"

        # –ê—Ç–æ–º–∞—Ä–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ Redis pipeline
        pipe = self.redis.pipeline()
        pipe.hset(cart_key, item_id, quantity)
        pipe.expire(cart_key, 86400 * 7)  # 7 –¥–Ω–µ–π TTL

        # –ü–µ—Ä–µ—Å—á—ë—Ç –æ–±—â–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –≤ background
        pipe.sadd(f"cart_updates:{int(time.time()) // 60}", user_id)
        await pipe.execute()
```

### SaaS Performance Optimization

```python
# SaaS –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
class SaaSPerformanceOptimizer:
    """Performance optimization –¥–ª—è SaaS –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π."""

    async def optimize_dashboard_data(self, user_id: str, org_id: str) -> dict:
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ dashboard data."""
        # Parallel loading —Ä–∞–∑–ª–∏—á–Ω—ã—Ö dashboard sections
        dashboard_tasks = [
            self._get_analytics_data(org_id),
            self._get_recent_activities(user_id),
            self._get_usage_metrics(org_id),
            self._get_notifications(user_id)
        ]

        analytics, activities, metrics, notifications = await asyncio.gather(
            *dashboard_tasks, return_exceptions=True
        )

        return {
            "analytics": analytics if not isinstance(analytics, Exception) else {},
            "activities": activities if not isinstance(activities, Exception) else [],
            "metrics": metrics if not isinstance(metrics, Exception) else {},
            "notifications": notifications if not isinstance(notifications, Exception) else []
        }

    async def optimize_multi_tenant_queries(self, tenant_id: str, query: str) -> list:
        """Tenant-aware query optimization."""
        # Row-level security —á–µ—Ä–µ–∑ prepared statements
        tenant_aware_query = f"""
        SET row_security = on;
        SET app.tenant_id = '{tenant_id}';
        {query}
        """

        return await self.database.fetch_all(tenant_aware_query)
```

### Blog/CMS Performance Optimization

```python
# Blog/CMS –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
class BlogPerformanceOptimizer:
    """Performance optimization –¥–ª—è blog/CMS —Å–∏—Å—Ç–µ–º."""

    async def optimize_content_delivery(self, slug: str) -> dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–ª–æ–≥–∞."""
        cache_key = f"post:{slug}"

        # –ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        # L1: In-memory cache (Redis)
        cached_post = await self.redis.get(cache_key)
        if cached_post:
            return json.loads(cached_post)

        # L2: Database —Å optimized query
        post_query = """
        SELECT
            p.id, p.title, p.content, p.published_at,
            u.name as author_name, u.avatar_url,
            array_agg(t.name) as tags,
            c.name as category_name
        FROM posts p
        JOIN users u ON p.author_id = u.id
        LEFT JOIN post_tags pt ON p.id = pt.post_id
        LEFT JOIN tags t ON pt.tag_id = t.id
        LEFT JOIN categories c ON p.category_id = c.id
        WHERE p.slug = $1 AND p.status = 'published'
        GROUP BY p.id, u.id, c.id
        """

        post_data = await self.database.fetch_one(post_query, slug)

        if post_data:
            result = dict(post_data)

            # –ö—ç—à–∏—Ä—É–µ–º –Ω–∞ 1 —á–∞—Å
            await self.redis.setex(cache_key, 3600, json.dumps(result, default=str))

            # Increment view count –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            asyncio.create_task(self._increment_view_count(result['id']))

            return result

        return None

    async def optimize_sitemap_generation(self) -> str:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è sitemap."""
        cache_key = "sitemap:xml"

        cached_sitemap = await self.redis.get(cache_key)
        if cached_sitemap:
            return cached_sitemap

        # Bulk query –¥–ª—è –≤—Å–µ—Ö published posts
        posts = await self.database.fetch_all("""
        SELECT slug, updated_at
        FROM posts
        WHERE status = 'published'
        ORDER BY updated_at DESC
        """)

        sitemap_xml = self._generate_sitemap_xml(posts)

        # –ö—ç—à–∏—Ä—É–µ–º sitemap –Ω–∞ 24 —á–∞—Å–∞
        await self.redis.setex(cache_key, 86400, sitemap_xml)

        return sitemap_xml
```

## Performance Monitoring –∏ Alerting

### Universal Monitoring Setup

```python
# Universal Performance Monitoring
import time
import psutil
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import logging

# Prometheus metrics
REQUEST_COUNT = Counter('app_requests_total', 'Total requests', ['method', 'endpoint'])
REQUEST_DURATION = Histogram('app_request_duration_seconds', 'Request duration')
MEMORY_USAGE = Gauge('app_memory_usage_bytes', 'Memory usage')
CPU_USAGE = Gauge('app_cpu_usage_percent', 'CPU usage')

class PerformanceMonitor:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π monitoring –∫–ª–∞—Å—Å."""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

    def track_request(self, method: str, endpoint: str):
        """Decorator –¥–ª—è —Ç—Ä–µ–∫–∏–Ω–≥–∞ requests."""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()

                try:
                    result = func(*args, **kwargs)
                    REQUEST_COUNT.labels(method=method, endpoint=endpoint).inc()
                    return result
                finally:
                    REQUEST_DURATION.observe(time.time() - start_time)
            return wrapper
        return decorator

    async def collect_system_metrics(self):
        """–°–±–æ—Ä —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."""
        while True:
            # Memory usage
            memory = psutil.virtual_memory()
            MEMORY_USAGE.set(memory.used)

            # CPU usage
            cpu_percent = psutil.cpu_percent(interval=1)
            CPU_USAGE.set(cpu_percent)

            # Log –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            if memory.percent > 85:
                self.logger.warning(f"High memory usage: {memory.percent}%")

            if cpu_percent > 80:
                self.logger.warning(f"High CPU usage: {cpu_percent}%")

            await asyncio.sleep(30)

    def setup_alerting(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ alerting rules."""
        alerting_rules = {
            "high_memory": {
                "condition": "memory_usage > 0.85",
                "action": self._send_alert,
                "message": "Memory usage exceeded 85%"
            },
            "slow_requests": {
                "condition": "request_duration_p95 > 2.0",
                "action": self._send_alert,
                "message": "95th percentile response time > 2s"
            }
        }
        return alerting_rules

# Grafana Dashboard Configuration
GRAFANA_DASHBOARD = {
    "dashboard": {
        "title": "Application Performance",
        "panels": [
            {
                "title": "Request Rate",
                "type": "graph",
                "targets": [{"expr": "rate(app_requests_total[5m])"}]
            },
            {
                "title": "Response Time",
                "type": "graph",
                "targets": [{"expr": "histogram_quantile(0.95, app_request_duration_seconds)"}]
            },
            {
                "title": "Memory Usage",
                "type": "singlestat",
                "targets": [{"expr": "app_memory_usage_bytes"}]
            }
        ]
    }
}
```

## Performance Testing Strategies

```python
# Performance Testing Framework
import asyncio
import aiohttp
import time
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class LoadTestResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""
    total_requests: int
    successful_requests: int
    failed_requests: int
    average_response_time: float
    p95_response_time: float
    requests_per_second: float
    errors: List[str]

class UniversalLoadTester:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π load tester."""

    async def run_load_test(
        self,
        url: str,
        concurrent_users: int = 10,
        duration_seconds: int = 60,
        headers: Dict[str, str] = None
    ) -> LoadTestResult:
        """–ó–∞–ø—É—Å–∫ –Ω–∞–≥—Ä—É–∑–æ—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞."""

        results = []
        errors = []
        start_time = time.time()

        async def worker():
            """Worker –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è requests."""
            async with aiohttp.ClientSession(headers=headers) as session:
                while time.time() - start_time < duration_seconds:
                    request_start = time.time()
                    try:
                        async with session.get(url) as response:
                            await response.text()
                            request_time = time.time() - request_start
                            results.append({
                                'success': response.status == 200,
                                'response_time': request_time,
                                'status_code': response.status
                            })
                    except Exception as e:
                        errors.append(str(e))
                        results.append({
                            'success': False,
                            'response_time': time.time() - request_start,
                            'status_code': 0
                        })

        # –ó–∞–ø—É—Å–∫ concurrent workers
        workers = [asyncio.create_task(worker()) for _ in range(concurrent_users)]
        await asyncio.gather(*workers)

        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        successful = [r for r in results if r['success']]
        response_times = [r['response_time'] for r in results]

        return LoadTestResult(
            total_requests=len(results),
            successful_requests=len(successful),
            failed_requests=len(results) - len(successful),
            average_response_time=sum(response_times) / len(response_times),
            p95_response_time=sorted(response_times)[int(len(response_times) * 0.95)],
            requests_per_second=len(results) / duration_seconds,
            errors=errors
        )
```

## –†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ Performance Analysis

–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≤—ã–ø–æ–ª–Ω—è–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:

### 1. Performance Metrics Checklist
- ‚úÖ Response time improved (target: <200ms for API, <2s for pages)
- ‚úÖ Throughput increased (requests per second)
- ‚úÖ Memory usage optimized (no memory leaks)
- ‚úÖ CPU utilization efficient
- ‚úÖ Database query performance improved
- ‚úÖ Cache hit ratio optimized (>80% for hot data)

### 2. Scalability Validation
- ‚úÖ Load testing –ø–æ–¥ increased traffic
- ‚úÖ Horizontal scaling –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
- ‚úÖ Database scaling strategies
- ‚úÖ CDN –∏ caching effectiveness
- ‚úÖ Auto-scaling configuration

### 3. Trade-offs Analysis
- ‚úÖ Performance vs Security balance
- ‚úÖ Speed vs Reliability trade-offs
- ‚úÖ Memory vs CPU optimization choices
- ‚úÖ Cost vs Performance optimization
- ‚úÖ Complexity vs Maintainability

### 4. Monitoring Setup
- ‚úÖ Real-time performance monitoring
- ‚úÖ Alerting –¥–ª—è performance regressions
- ‚úÖ SLA compliance tracking
- ‚úÖ Business metrics correlation
- ‚úÖ Capacity planning data

### 5. Documentation –∏ Knowledge Sharing
- ‚úÖ Performance optimization decisions documented
- ‚úÖ Before/after metrics comparison
- ‚úÖ Best practices –¥–ª—è team sharing
- ‚úÖ Performance testing automation
- ‚úÖ Monitoring playbooks —Å–æ–∑–¥–∞–Ω–∏–µ

## MCP —Å–µ—Ä–≤–µ—Ä—ã –¥–ª—è Performance Optimization

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ MCP —Å–µ—Ä–≤–µ—Ä—ã:
- **prometheus** - metrics collection –∏ alerting
- **grafana** - performance dashboards –∏ visualization
- **database** - direct database performance analysis
- **docker** - container performance monitoring
- **aws/gcp/azure** - cloud infrastructure monitoring

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å performance stack:
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–±–æ—Ä performance metrics
- Real-time alerting –ø—Ä–∏ performance issues
- Database query analysis –∏ optimization suggestions
- Infrastructure scaling recommendations
- Cost optimization —á–µ—Ä–µ–∑ performance improvements

–≠—Ç–æ—Ç –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π performance —ç–∫—Å–ø–µ—Ä—Ç –¥–ª—è –ª—é–±—ã—Ö —Ç–∏–ø–æ–≤ —Å–∏—Å—Ç–µ–º - –æ—Ç –º–æ–±–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π –¥–æ enterprise –ø–ª–∞—Ç—Ñ–æ—Ä–º, –∞–¥–∞–ø—Ç–∏—Ä—É—è—Å—å –ø–æ–¥ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫–∞–∂–¥–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ monitoring setup.