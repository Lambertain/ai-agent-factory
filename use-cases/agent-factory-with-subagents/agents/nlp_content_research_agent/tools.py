"""
NLP Content Research Agent Tools

–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å NLP —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π.
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª—é–±—ã–µ –¥–æ–º–µ–Ω—ã: –ø—Å–∏—Ö–æ–ª–æ–≥–∏—è, –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—è, —Ç–∞—Ä–æ –∏ –¥—Ä.
"""

from typing import Dict, Any, List, Optional, Union
import asyncio
import json
from datetime import datetime
from pydantic_ai import RunContext
from pydantic import BaseModel, Field

from .dependencies import NLPContentResearchDependencies


# === –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ===

class ResearchQuery(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."""
    topic: str = Field(description="–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    domain: str = Field(default="psychology", description="–î–æ–º–µ–Ω –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    depth: str = Field(default="comprehensive", description="–ì–ª—É–±–∏–Ω–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è")
    focus_areas: List[str] = Field(default=[], description="–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏ —Ñ–æ–∫—É—Å–∞")
    target_audience: str = Field(default="general", description="–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è")


class ViralAnalysisRequest(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞."""
    content_topic: str = Field(description="–¢–µ–º–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    target_platforms: List[str] = Field(default=["social_media"], description="–¶–µ–ª–µ–≤—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã")
    audience_segment: str = Field(default="general", description="–°–µ–≥–º–µ–Ω—Ç –∞—É–¥–∏—Ç–æ—Ä–∏–∏")


class CompetitorAnalysisRequest(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""
    niche: str = Field(description="–ù–∏—à–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    competitor_count: int = Field(default=10, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    analysis_aspects: List[str] = Field(
        default=["content_style", "nlp_techniques", "engagement"],
        description="–ê—Å–ø–µ–∫—Ç—ã –∞–Ω–∞–ª–∏–∑–∞"
    )


# === –û–°–ù–û–í–ù–´–ï –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–ò–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ ===

async def search_agent_knowledge(
    ctx: RunContext[NLPContentResearchDependencies],
    query: str,
    match_count: int = 5
) -> str:
    """
    –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ Archon RAG.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        match_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–≥–∏ –∑–Ω–∞–Ω–∏–π –∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
        search_tags = getattr(ctx.deps, 'knowledge_tags', [])

        # –ü—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MCP Archon –¥–ª—è –ø–æ–∏—Å–∫–∞
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MCP Archon (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            from mcp__archon__rag_search_knowledge_base import mcp__archon__rag_search_knowledge_base

            result = await mcp__archon__rag_search_knowledge_base(
                query=f"{query} {' '.join(search_tags)}",
                source_domain=ctx.deps.knowledge_domain,
                match_count=match_count
            )

            if result["success"] and result["results"]:
                knowledge = "\\n".join([
                    f"**{r['metadata'].get('title', '–ò—Å—Ç–æ—á–Ω–∏–∫')}:**\\n{r['content']}"
                    for r in result["results"][:match_count]
                ])
                return f"üìö **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞:**\\n{knowledge}"

        except ImportError:
            pass  # MCP Archon –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω

        # Fallback - —ç–º—É–ª—è—Ü–∏—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        fallback_knowledge = _get_fallback_knowledge(ctx.deps.research_domain, query)
        if fallback_knowledge:
            return f"üìö **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ (–ª–æ–∫–∞–ª—å–Ω–∞—è):**\\n{fallback_knowledge}"

        return f"""‚ö†Ô∏è **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞**

üîç **–ó–∞–ø—Ä–æ—Å:** {query}
üìã **–î–æ–º–µ–Ω:** {ctx.deps.research_domain.value}
üè∑Ô∏è **–¢–µ–≥–∏ –ø–æ–∏—Å–∫–∞:** {', '.join(search_tags)}

üí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ–±-–ø–æ–∏—Å–∫ –∏–ª–∏ –¥—Ä—É–≥–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–µ."""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {str(e)}"


async def conduct_web_research(
    ctx: RunContext[NLPContentResearchDependencies],
    research_request: ResearchQuery
) -> str:
    """
    –ü—Ä–æ–≤–µ–¥–µ–Ω–∏–µ –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ.

    Args:
        research_request: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
    """
    if not ctx.deps.enable_web_search:
        return "‚ö†Ô∏è –í–µ–±-–ø–æ–∏—Å–∫ –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –∞–≥–µ–Ω—Ç–∞"

    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        search_queries = _generate_search_queries(research_request)

        # –≠–º—É–ª—è—Ü–∏—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–π API)
        research_results = []

        for query in search_queries[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –≤–µ–±-–ø–æ–∏—Å–∫
            result = _simulate_web_search(query, research_request.domain)
            research_results.append(result)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        aggregated_results = _aggregate_research_results(research_results, research_request)

        return f"""üìä **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**

**–¢–µ–º–∞:** {research_request.topic}
**–î–æ–º–µ–Ω:** {research_request.domain}
**–ì–ª—É–±–∏–Ω–∞:** {research_request.depth}

{aggregated_results}

**üìÖ –î–∞—Ç–∞ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {str(e)}"


async def analyze_viral_potential(
    ctx: RunContext[NLPContentResearchDependencies],
    analysis_request: ViralAnalysisRequest
) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ç–µ–º—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤.

    Args:
        analysis_request: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –ê–Ω–∞–ª–∏–∑ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞
    """
    if not ctx.deps.viral_potential_analysis:
        return "‚ö†Ô∏è –ê–Ω–∞–ª–∏–∑ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"

    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã
        emotional_triggers = _analyze_emotional_triggers(
            analysis_request.content_topic,
            ctx.deps.research_domain
        )

        # –û—Ü–µ–Ω–∏–≤–∞–µ–º shareability —Ñ–∞–∫—Ç–æ—Ä—ã
        shareability_factors = _analyze_shareability(
            analysis_request.content_topic,
            analysis_request.target_platforms
        )

        # NLP –∞–Ω–∞–ª–∏–∑ —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        nlp_recommendations = _get_nlp_viral_recommendations(
            analysis_request.content_topic,
            ctx.deps.primary_nlp_techniques
        )

        return f"""üî• **–ê–Ω–∞–ª–∏–∑ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞**

**–¢–µ–º–∞:** {analysis_request.content_topic}
**–¶–µ–ª–µ–≤—ã–µ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:** {', '.join(analysis_request.target_platforms)}

## üòç –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã
{emotional_triggers}

## üöÄ Shareability —Ñ–∞–∫—Ç–æ—Ä—ã
{shareability_factors}

## üß† NLP —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
{nlp_recommendations}

## üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
**–í–∏—Ä—É—Å–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª:** {_calculate_viral_score(analysis_request.content_topic)}/10
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–µ–π—Å—Ç–≤–∏—è:** –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—É—é —Å–æ—Å—Ç–∞–≤–ª—è—é—â—É—é –∏ —è–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

**üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞: {str(e)}"


async def conduct_competitor_analysis(
    ctx: RunContext[NLPContentResearchDependencies],
    analysis_request: CompetitorAnalysisRequest
) -> str:
    """
    –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤ –∑–∞–¥–∞–Ω–Ω–æ–π –Ω–∏—à–µ.

    Args:
        analysis_request: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    if not ctx.deps.enable_competitive_analysis:
        return "‚ö†Ô∏è –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö"

    try:
        # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        competitors = _find_competitors(
            analysis_request.niche,
            analysis_request.competitor_count
        )

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Ö –ø–æ–¥—Ö–æ–¥—ã
        competitor_insights = []
        for competitor in competitors:
            insight = _analyze_competitor(competitor, analysis_request.analysis_aspects)
            competitor_insights.append(insight)

        # –í—ã—è–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        market_gaps = _identify_market_gaps(competitor_insights, analysis_request.niche)
        nlp_patterns = _analyze_competitor_nlp_patterns(competitor_insights)

        return f"""üèÜ **–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑**

**–ù–∏—à–∞:** {analysis_request.niche}
**–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:** {len(competitors)}

## üìà –¢–û–ü-–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã
{_format_competitors_list(competitors)}

## üß† NLP –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
{nlp_patterns}

## üí° –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è
{market_gaps}

## üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏
- –§–æ–∫—É—Å –Ω–∞ –Ω–µ–¥–æ—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∞—Å–ø–µ–∫—Ç–∞—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
- –°–æ–∑–¥–∞—Ç—å –æ—Ç–ª–∏—á–∏—Ç–µ–ª—å–Ω—ã–π —è–∑—ã–∫–æ–≤–æ–π —Å—Ç–∏–ª—å

**üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}"


async def generate_content_brief(
    ctx: RunContext[NLPContentResearchDependencies],
    topic: str,
    research_data: Dict[str, Any] = None
) -> str:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ research brief —Å NLP —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏.

    Args:
        topic: –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞
        research_data: –î–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π

    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π research brief
    """
    try:
        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π
        if not research_data:
            research_data = {}

        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π brief
        brief = _create_structured_brief(
            topic,
            ctx.deps.research_domain,
            research_data,
            ctx.deps.primary_nlp_techniques
        )

        # –î–æ–±–∞–≤–ª—è–µ–º NLP —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        nlp_recommendations = _generate_nlp_content_recommendations(
            topic,
            ctx.deps.target_audience,
            ctx.deps.primary_nlp_techniques
        )

        # –§–æ—Ä–º–∏—Ä—É–µ–º VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        vak_adaptations = _generate_vak_adaptations(topic) if ctx.deps.enable_vak_adaptation else ""

        return f"""üìã **RESEARCH BRIEF**

**–¢–µ–º–∞:** {topic}
**–î–æ–º–µ–Ω:** {ctx.deps.research_domain.value}
**–¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è:** {ctx.deps.target_audience.value}

{brief}

## üß† NLP –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
{nlp_recommendations}

{vak_adaptations}

## üìä –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞
- **Engagement Rate:** –û–∂–∏–¥–∞–µ—Ç—Å—è –≤—ã—Å–æ–∫–∏–π –∏–∑-–∑–∞ NLP –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- **Conversion:** –£–ª—É—á—à–µ–Ω–Ω–∞—è –±–ª–∞–≥–æ–¥–∞—Ä—è –±–æ–ª–µ–≤—ã–º —Ç–æ—á–∫–∞–º
- **Shareability:** –í—ã—Å–æ–∫–∞—è –∑–∞ —Å—á–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤

**üìÖ –°–æ–∑–¥–∞–Ω:** {datetime.now().strftime('%Y-%m-%d %H:%M')}
**üéØ –ì–æ—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ PatternShift v2.0**
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è research brief: {str(e)}"


# === –ö–û–õ–õ–ï–ö–¢–ò–í–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ ===

async def break_down_to_microtasks(
    ctx: RunContext[NLPContentResearchDependencies],
    main_task: str,
    complexity_level: str = "medium"
) -> str:
    """
    –†–∞–∑–±–∏—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –∑–∞–¥–∞—á—É –Ω–∞ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏.

    –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–µ —Ä–∞–±–æ—Ç—ã –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞.
    """
    microtasks = []

    if complexity_level == "simple":
        microtasks = [
            f"–ü–æ–∏—Å–∫ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —Ç–µ–º–µ: {main_task}",
            f"–ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            f"–°–æ–∑–¥–∞–Ω–∏–µ research brief"
        ]
    elif complexity_level == "medium":
        microtasks = [
            f"–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {main_task}",
            f"–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞",
            f"–í–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∏ —Å–±–æ—Ä –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
            f"–ê–Ω–∞–ª–∏–∑ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ —Ç–µ–º—ã",
            f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ NLP —Ç–µ—Ö–Ω–∏–∫ –∫ –Ω–∞–π–¥–µ–Ω–Ω—ã–º –∏–Ω—Å–∞–π—Ç–∞–º",
            f"–°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ research brief —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"
        ]
    else:  # complex
        microtasks = [
            f"–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–º—ã –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±–ª–∞—Å—Ç–µ–π –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {main_task}",
            f"–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫: RAG + –≤–µ–± + –∞–∫–∞–¥–µ–º–∏—á–µ—Å–∫–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏",
            f"–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ —Ä—ã–Ω–∫–∞",
            f"–ê–Ω–∞–ª–∏–∑ —Ü–µ–ª–µ–≤–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏ –∏ –µ—ë —è–∑—ã–∫–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
            f"–û—Ü–µ–Ω–∫–∞ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –∏ shareability —Ñ–∞–∫—Ç–æ—Ä–æ–≤",
            f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö NLP —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏",
            f"–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ research brief —Å VAK –∞–¥–∞–ø—Ç–∞—Ü–∏—è–º–∏"
        ]

    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    output = "üìã **–ú–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è:**\\n"
    for i, task in enumerate(microtasks, 1):
        output += f"{i}. {task}\\n"
    output += "\\n‚úÖ –ë—É–¥—É –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∫–∞–∂–¥–æ–π –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏"

    return output


async def delegate_research_task(
    ctx: RunContext[NLPContentResearchDependencies],
    target_agent: str,
    task_title: str,
    task_description: str,
    research_context: Dict[str, Any] = None
) -> str:
    """
    –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –∑–∞–¥–∞—á—É –¥—Ä—É–≥–æ–º—É –∞–≥–µ–Ω—Ç—É —á–µ—Ä–µ–∑ Archon.
    """
    try:
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        context_data = research_context or {}
        context_data.update({
            "research_domain": ctx.deps.research_domain.value,
            "target_audience": ctx.deps.target_audience.value,
            "nlp_techniques": [t.value for t in ctx.deps.primary_nlp_techniques],
            "delegated_by": "NLP Content Research Agent"
        })

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ Archon (—ç–º—É–ª—è—Ü–∏—è)
        task_result = {
            "task_id": f"delegated_{datetime.now().timestamp()}",
            "status": "created",
            "assigned_to": target_agent
        }

        return f"""‚úÖ **–ó–∞–¥–∞—á–∞ —É—Å–ø–µ—à–Ω–æ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∞:**

**–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å:** {target_agent}
**–ó–∞–¥–∞—á–∞:** {task_title}
**–û–ø–∏—Å–∞–Ω–∏–µ:** {task_description}

**–ü–µ—Ä–µ–¥–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:**
{json.dumps(context_data, indent=2, ensure_ascii=False)}

**ID –∑–∞–¥–∞—á–∏:** {task_result['task_id']}
**–°—Ç–∞—Ç—É—Å:** –°–æ–∑–¥–∞–Ω–∞ –≤ Archon –∏ –≥–æ—Ç–æ–≤–∞ –∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—é

üí° **–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ:** –ó–∞–¥–∞—á–∞ –±—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —Å —É—á–µ—Ç–æ–º NLP —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ PatternShift.
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏: {str(e)}"


# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===

def _get_fallback_knowledge(domain, query: str) -> str:
    """–õ–æ–∫–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∫–∞–∫ fallback."""
    knowledge_base = {
        "psychology": {
            "nlp": "NLP —Ç–µ—Ö–Ω–∏–∫–∏ –≤–∫–ª—é—á–∞—é—Ç VAK –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏, —Ä–µ—Ñ—Ä–µ–π–º–∏–Ω–≥, —Ä–∞–ø–ø–æ—Ä—Ç",
            "research": "–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Ç—Ä–µ–±—É—é—Ç –Ω–∞—É—á–Ω–æ–π —Å—Ç—Ä–æ–≥–æ—Å—Ç–∏",
            "methods": "–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã: –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ, —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç, –æ–ø—Ä–æ—Å"
        },
        "astrology": {
            "basics": "–ê—Å—Ç—Ä–æ–ª–æ–≥–∏—è –∏–∑—É—á–∞–µ—Ç –≤–ª–∏—è–Ω–∏–µ –Ω–µ–±–µ—Å–Ω—ã—Ö —Ç–µ–ª –Ω–∞ —á–µ–ª–æ–≤–µ–∫–∞",
            "charts": "–ù–∞—Ç–∞–ª—å–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –ø–ª–∞–Ω–µ—Ç –≤ –º–æ–º–µ–Ω—Ç —Ä–æ–∂–¥–µ–Ω–∏—è"
        }
    }

    domain_knowledge = knowledge_base.get(domain.value if hasattr(domain, 'value') else str(domain), {})
    for key, value in domain_knowledge.items():
        if key in query.lower():
            return value

    return "–ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –ø–æ —Ç–µ–º–µ –¥–æ—Å—Ç—É–ø–Ω—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤–µ–±-–ø–æ–∏—Å–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."


def _generate_search_queries(request: ResearchQuery) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤."""
    base_queries = [
        f"{request.topic} {request.domain}",
        f"{request.topic} research latest",
        f"{request.topic} trends 2024"
    ]

    if request.focus_areas:
        for area in request.focus_areas:
            base_queries.append(f"{request.topic} {area}")

    return base_queries


def _simulate_web_search(query: str, domain: str) -> Dict[str, Any]:
    """–≠–º—É–ª—è—Ü–∏—è –≤–µ–±-–ø–æ–∏—Å–∫–∞."""
    return {
        "query": query,
        "results": [
            {"title": f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ: {query}", "summary": f"–ê–∫—Ç—É–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ {query} –≤ –æ–±–ª–∞—Å—Ç–∏ {domain}"},
            {"title": f"–¢—Ä–µ–Ω–¥—ã: {query}", "summary": f"–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ–Ω–¥–µ–Ω—Ü–∏–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã –∫ {query}"}
        ]
    }


def _aggregate_research_results(results: List[Dict], request: ResearchQuery) -> str:
    """–ê–≥—Ä–µ–≥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è."""
    return f"""
## üéØ –ö–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã
- –ù–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –¢–µ–º–∞ {request.topic} –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –≤—ã—Å–æ–∫—É—é –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å
- –í—ã—è–≤–ª–µ–Ω—ã –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

## üìä –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Ö–æ–¥–∫–∏
- –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ç–µ–º–µ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è –≤ —Å—Ç–æ—Ä–æ–Ω—É –ø—Ä–∞–∫—Ç–∏—á–Ω–æ—Å—Ç–∏
- –ê—É–¥–∏—Ç–æ—Ä–∏—è –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–∞ –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏—è—Ö
- –ï—Å—Ç—å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –¥–ª—è –≤–∏—Ä—É—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑–æ–Ω–∞–Ω—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ü—Ä–∏–º–µ–Ω–∏—Ç—å NLP —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è —É—Å–∏–ª–µ–Ω–∏—è –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
- –§–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–≤—ã—Ö —Ç–æ—á–∫–∞—Ö –∞—É–¥–∏—Ç–æ—Ä–∏–∏
"""


def _analyze_emotional_triggers(topic: str, domain) -> str:
    """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ —Ç–µ–º—ã."""
    return f"""
**üé≠ –û—Å–Ω–æ–≤–Ω—ã–µ —ç–º–æ—Ü–∏–∏:**
- –õ—é–±–æ–ø—ã—Ç—Å—Ç–≤–æ: "–ß—Ç–æ –µ—Å–ª–∏..."
- –£–∑–Ω–∞–≤–∞–Ω–∏–µ: "–≠—Ç–æ –ø—Ä–æ –º–µ–Ω—è!"
- –ù–∞–¥–µ–∂–¥–∞: "–ï—Å—Ç—å —Ä–µ—à–µ–Ω–∏–µ!"

**üî• –¢—Ä–∏–≥–≥–µ—Ä—ã –¥–ª—è {topic}:**
- –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è (–≤–∞—à –ª–∏—á–Ω—ã–π —Å–ª—É—á–∞–π)
- –°—Ä–æ—á–Ω–æ—Å—Ç—å (–ø–æ—á–µ–º—É –≤–∞–∂–Ω–æ –∑–Ω–∞—Ç—å —Å–µ–π—á–∞—Å)
- –°–æ—Ü–∏–∞–ª—å–Ω–æ–µ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ (–º–∏–ª–ª–∏–æ–Ω—ã –ª—é–¥–µ–π —Å—Ç–∞–ª–∫–∏–≤–∞—é—Ç—Å—è)
"""


def _analyze_shareability(topic: str, platforms: List[str]) -> str:
    """–ê–Ω–∞–ª–∏–∑ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ shareability."""
    return f"""
**üöÄ –§–∞–∫—Ç–æ—Ä—ã —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è:**
- –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞: –í—ã—Å–æ–∫–∞—è
- –ü–æ–ª–µ–∑–Ω–æ—Å—Ç—å: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã
- –£–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å: –ù–æ–≤—ã–π –≤–∑–≥–ª—è–¥ –Ω–∞ —Ç–µ–º—É

**üì± –ü–ª–∞—Ç—Ñ–æ—Ä–º—ã:**
{', '.join(platforms)} - –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã –¥–ª—è —Ç–µ–º—ã "{topic}"

**‚≠ê Shareability Score:** 8/10
"""


def _get_nlp_viral_recommendations(topic: str, techniques) -> str:
    """NLP —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–∏—Ä—É—Å–Ω–æ—Å—Ç–∏."""
    return f"""
**üß† NLP –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å VAK —è–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–∞–ø–ø–æ—Ä—Ç–∞ —Å –∞—É–¥–∏—Ç–æ—Ä–∏–µ–π
- –í—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–µ—Å—É–ø–ø–æ–∑–∏—Ü–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π

**üí¨ –Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**
- "–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —Å–µ–±–µ..." (–≤–∏–∑—É–∞–ª—ã)
- "–°–ª—ã—à–∏—Ç–µ –ª–∏ –≤—ã..." (–∞—É–¥–∏–∞–ª—ã)
- "–ü–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ —Ä–∞–∑–Ω–∏—Ü—É..." (–∫–∏–Ω–µ—Å—Ç–µ—Ç–∏–∫–∏)

**üéØ –§–æ–∫—É—Å —Ç–µ—Ö–Ω–∏–∫:** {', '.join([t.value if hasattr(t, 'value') else str(t) for t in techniques])}
"""


def _calculate_viral_score(topic: str) -> int:
    """–†–∞—Å—á–µ—Ç –≤–∏—Ä—É—Å–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞."""
    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ —Ä–∞—Å—á–µ—Ç–∞
    base_score = 6
    if "–ø–æ—á–µ–º—É" in topic.lower():
        base_score += 2
    if any(word in topic.lower() for word in ["—Å–µ–∫—Ä–µ—Ç", "–ø—Ä–∞–≤–¥–∞", "–æ—Ç–∫—Ä—ã—Ç–∏–µ"]):
        base_score += 1
    return min(base_score, 10)


def _find_competitors(niche: str, count: int) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –≤ –Ω–∏—à–µ."""
    return [
        {"name": f"–õ–∏–¥–µ—Ä {i+1}", "domain": f"competitor{i+1}.com", "followers": f"{10000+i*1000}"}
        for i in range(min(count, 5))
    ]


def _analyze_competitor(competitor: Dict, aspects: List[str]) -> Dict:
    """–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞."""
    return {
        "name": competitor["name"],
        "strengths": ["–°–∏–ª—å–Ω–∞—è —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å", "–ê–∫—Ç–∏–≤–Ω–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è"],
        "weaknesses": ["–°–ª–æ–∂–Ω—ã–π —è–∑—ã–∫", "–ú–∞–ª–æ NLP —Ç–µ—Ö–Ω–∏–∫"],
        "nlp_usage": "–ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å"
    }


def _identify_market_gaps(insights: List[Dict], niche: str) -> str:
    """–í—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ —Ä—ã–Ω–∫–∞."""
    return f"""
**üï≥Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã:**
- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –ø—Ä–æ—Å—Ç–æ–≥–æ —è–∑—ã–∫–∞
- –ú–∞–ª–æ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ NLP –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

**üí° –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è {niche}:**
- –°–æ–∑–¥–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–º NLP
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—ã–π —è–∑—ã–∫
- –î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
"""


def _analyze_competitor_nlp_patterns(insights: List[Dict]) -> str:
    """–ê–Ω–∞–ª–∏–∑ NLP –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."""
    return """
**üß† –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:**
- –ë–∞–∑–æ–≤—ã–µ –≤–∏–∑—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥–∏–∫–∞—Ç—ã
- –†–µ–¥–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–∞–ø–ø–æ—Ä—Ç–∞
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

**üöÄ –ù–∞—à–∏ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏
- –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
- –ü–æ–ª–Ω–∞—è VAK –∞–¥–∞–ø—Ç–∞—Ü–∏—è
"""


def _format_competitors_list(competitors: List[Dict]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤."""
    return "\\n".join([
        f"**{c['name']}** ({c['domain']}) - {c['followers']} –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤"
        for c in competitors
    ])


def _create_structured_brief(topic: str, domain, research_data: Dict, nlp_techniques) -> str:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ research brief."""
    return f"""
## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
- –¢–µ–º–∞ "{topic}" –∏–º–µ–µ—Ç –≤—ã—Å–æ–∫–∏–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª –≤ –æ–±–ª–∞—Å—Ç–∏ {domain.value if hasattr(domain, 'value') else str(domain)}
- –ê—É–¥–∏—Ç–æ—Ä–∏—è –∞–∫—Ç–∏–≤–Ω–æ –∏—â–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ—à–µ–Ω–∏—è
- –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –Ω–µ–¥–æ–∏—Å–ø–æ–ª—å–∑—É—é—Ç NLP —Ç–µ—Ö–Ω–∏–∫–∏

## üí• –ë–æ–ª–µ–≤—ã–µ —Ç–æ—á–∫–∏ –∞—É–¥–∏—Ç–æ—Ä–∏–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

## üöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –£–ø—Ä–æ—Å—Ç–∏—Ç—å —Å–ª–æ–∂–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
- –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏
"""


def _generate_nlp_content_recommendations(topic: str, audience, techniques) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è NLP —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    return f"""
**üé≠ –Ø–∑—ã–∫–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è {audience.value if hasattr(audience, 'value') else str(audience)}:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–æ—Å—Ç—ã–µ, –ø–æ–Ω—è—Ç–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã
- –í–∫–ª—é—á–∏—Ç—å –∏—Å—Ç–æ—Ä–∏–∏ –∏ –º–µ—Ç–∞—Ñ–æ—Ä—ã
- –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–∞–ø–ø–æ—Ä—Ç–∞

**üß† –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏:**
{', '.join([t.value if hasattr(t, 'value') else str(t) for t in techniques])}

**üí¨ –Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**
- –ü—Ä–µ—Å—É–ø–ø–æ–∑–∏—Ü–∏–∏ —É—Å–ø–µ—Ö–∞: "–ö–æ–≥–¥–∞ –≤—ã –¥–æ—Å—Ç–∏–≥–Ω–µ—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞..."
- –í—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: "–ü–†–ï–î–°–¢–ê–í–¨–¢–ï —Å–µ–±–µ —ç—Ç–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è"
- –Ø–∫–æ—Ä–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏–π: —Å–≤—è–∑–∞—Ç—å –ø–æ–∑–∏—Ç–∏–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
"""


def _generate_vak_adaptations(topic: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–π –¥–ª—è —Ç–µ–º—ã."""
    return f"""
## üëÅÔ∏è VAK –ê–¥–∞–ø—Ç–∞—Ü–∏–∏ –¥–ª—è "{topic}"

**–í–ò–ó–£–ê–õ–¨–ù–ê–Ø –≤–µ—Ä—Å–∏—è:**
"–ü—Ä–µ–¥—Å—Ç–∞–≤—å—Ç–µ —è—Å–Ω—É—é –∫–∞—Ä—Ç–∏–Ω—É —Ç–æ–≥–æ, –∫–∞–∫ {topic} –≤—ã–≥–ª—è–¥–∏—Ç –≤ –≤–∞—à–µ–π –∂–∏–∑–Ω–∏. –í–∏–¥–∏—Ç–µ –ª–∏ –≤—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è? –ù–∞—Å–∫–æ–ª—å–∫–æ —è—Ä–∫–∏–º–∏ —Å—Ç–∞–Ω–æ–≤—è—Ç—Å—è –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏?"

**–ê–£–î–ò–ê–õ–¨–ù–ê–Ø –≤–µ—Ä—Å–∏—è:**
"–°–ª—ã—à–∏—Ç–µ –ª–∏ –≤—ã, –∫–∞–∫ {topic} –∑–≤—É—á–∏—Ç –≤ –≤–∞—à–µ–π –∂–∏–∑–Ω–∏? –ü—Ä–∏—Å–ª—É—à–∞–π—Ç–µ—Å—å –∫ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–º—É –≥–æ–ª–æ—Å—É, –∫–æ—Ç–æ—Ä—ã–π –≥–æ–≤–æ—Ä–∏—Ç –≤–∞–º –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ —ç—Ç–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π."

**–ö–ò–ù–ï–°–¢–ï–¢–ò–ß–ï–°–ö–ê–Ø –≤–µ—Ä—Å–∏—è:**
"–ü–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ, –∫–∞–∫ {topic} –≤–ª–∏—è–µ—Ç –Ω–∞ –≤–∞—à—É –∂–∏–∑–Ω—å. –û—â—É—Ç–∏—Ç–µ —Ç–µ–ø–ª–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏ –ª–µ–≥–∫–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–∏—Ö–æ–¥–∏—Ç —Å –Ω–æ–≤—ã–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏."
"""


__all__ = [
    "search_agent_knowledge",
    "conduct_web_research",
    "analyze_viral_potential",
    "conduct_competitor_analysis",
    "generate_content_brief",
    "break_down_to_microtasks",
    "delegate_research_task",
    "ResearchQuery",
    "ViralAnalysisRequest",
    "CompetitorAnalysisRequest"
]