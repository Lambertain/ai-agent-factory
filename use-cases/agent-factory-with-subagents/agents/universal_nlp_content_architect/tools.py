"""
Tools for Universal NLP Content Architect Agent
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è 5-—ç—Ç–∞–ø–Ω–æ–π —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–π NLP –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
"""

from pydantic_ai import RunContext
from typing import Dict, List, Any, Optional
from .dependencies import UniversalNLPDependencies
import json

# –≠–¢–ê–ü 1: RESEARCH (–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–æ–º–µ–Ω–∞)

async def research_domain_topic(
    ctx: RunContext[UniversalNLPDependencies],
    topic: str,
    domain_type: str,
    research_depth: str = "comprehensive"  # comprehensive, moderate, basic
) -> str:
    """
    –≠—Ç–∞–ø 1: –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ–º—ã –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–æ–º–µ–Ω–∞

    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–¥ –ª—é–±–æ–π –¥–æ–º–µ–Ω:
    - –ê–Ω–∞–ª–∏–∑ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∏ –¥–æ–º–µ–Ω–∞
    - –ò–∑—É—á–µ–Ω–∏–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö NLP —Ç–µ—Ö–Ω–∏–∫
    - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π
    - –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥–∞–ø—Ç–∞—Ü–∏–π
    """
    try:
        domain_config = ctx.deps.get_domain_specific_config()
        nlp_techniques = ctx.deps.get_nlp_techniques()
        ericksonian_patterns = ctx.deps.get_ericksonian_patterns()

        research_results = {
            "topic": topic,
            "domain": domain_type,
            "domain_approach": domain_config,
            "suitable_nlp_techniques": nlp_techniques[:5],  # –¢–æ–ø-5 —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –¥–æ–º–µ–Ω–∞
            "ericksonian_patterns": ericksonian_patterns[:5],
            "cultural_considerations": ctx.deps.get_cultural_adaptation("slavic"),
            "vak_applications": list(ctx.deps.vak_adaptations.keys())
        }

        return f"""
üîç **–≠—Ç–∞–ø 1: Research –∑–∞–≤–µ—Ä—à–µ–Ω**

–î–æ–º–µ–Ω: {domain_type.upper()}
–¢–µ–º–∞: {topic}

–ü–æ–¥—Ö–æ–¥—è—â–∏–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏: {', '.join(nlp_techniques[:3])}
–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(ericksonian_patterns[:3])}

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –¥–æ–º–µ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —ç—Ç–∞–ø—É 2: Draft.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ research: {e}"

# –≠–¢–ê–ü 2: DRAFT (–°–æ–∑–¥–∞–Ω–∏–µ —á–µ—Ä–Ω–æ–≤–∏–∫–∞)

async def create_content_draft(
    ctx: RunContext[UniversalNLPDependencies],
    research_data: str,
    content_count: int = 16,
    format_type: str = "adaptive"  # adaptive, structured, creative
) -> str:
    """
    –≠—Ç–∞–ø 2: –°–æ–∑–¥–∞–Ω–∏–µ —á–µ—Ä–Ω–æ–≤–∏–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ NLP —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏

    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –¥–æ–º–µ–Ω:
    - –í–æ–ø—Ä–æ—Å—ã/—ç–ª–µ–º–µ–Ω—Ç—ã —Å NLP —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
    - –ü–µ—Ä–≤–∏—á–Ω—ã–µ VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
    - –ë–∞–∑–æ–≤—ã–µ –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    - –î–æ–º–µ–Ω-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã
    """
    try:
        domain = ctx.deps.domain_type
        nlp_techniques = ctx.deps.get_nlp_techniques()
        content_draft = {
            "title": f"",
            "domain": domain,
            "content_elements": [],
            "nlp_integration": {},
            "vak_variants": {}
        }

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ–º–µ–Ω–∞
        if domain == "psychology":
            content_draft["title"] = "Psychological Content with NLP"
            sample_element = {
                "id": 1,
                "content": "–ö–æ–≥–¥–∞ –≤—ã –∑–∞–º–µ—á–∞–µ—Ç–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Å–≤–æ–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ (—Ç—Ä—é–∏–∑–º), —á—Ç–æ –ø–µ—Ä–≤–æ–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –Ω–∞ —É–º –æ –≤–∞—à–∏—Ö –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏—Ö —Ä–µ—Å—É—Ä—Å–∞—Ö (–∫–æ—Å–≤–µ–Ω–Ω–æ–µ –≤–Ω—É—à–µ–Ω–∏–µ)?",
                "nlp_techniques": ["truisms", "embedded_commands"],
                "vak_visual": "...–≤–∏–¥–∏—Ç–µ –Ω–æ–≤—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã",
                "vak_auditory": "...—Å–ª—ã—à–∏—Ç–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –º—É–¥—Ä–æ—Å—Ç—å",
                "vak_kinesthetic": "...—á—É–≤—Å—Ç–≤—É–µ—Ç–µ —Å–∏–ª—É –∏–∑–º–µ–Ω–µ–Ω–∏–π"
            }

        elif domain == "astrology":
            content_draft["title"] = "Astrological Guidance with NLP"
            sample_element = {
                "id": 1,
                "content": "–ö–∞–∫ –ø–ª–∞–Ω–µ—Ç—ã –¥–≤–∏–∂—É—Ç—Å—è –≤ —Å–≤–æ–∏—Ö —Ü–∏–∫–ª–∞—Ö (–º–µ—Ç–∞—Ñ–æ—Ä–∞), —Ç–∞–∫ –∏ –≤–∞—à–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞—Å–∫—Ä—ã–≤–∞—é—Ç—Å—è –≤ –Ω—É–∂–Ω–æ–µ –≤—Ä–µ–º—è (–ø—Ä–µ–∑—É–º–ø—Ü–∏—è). –ö–∞–∫–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –≤–∞—à–µ–≥–æ –∑–Ω–∞–∫–∞ –≥–æ—Ç–æ–≤—ã –ø—Ä–æ—è–≤–∏—Ç—å—Å—è? (–∫–æ—Å–≤–µ–Ω–Ω–æ–µ –≤–Ω—É—à–µ–Ω–∏–µ)",
                "nlp_techniques": ["metaphors", "presuppositions", "indirect_suggestions"],
                "vak_visual": "...–≤–∏–¥–∏—Ç–µ –∑–≤–µ–∑–¥–Ω—É—é –∫–∞—Ä—Ç—É –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π",
                "vak_auditory": "...—Å–ª—ã—à–∏—Ç–µ –∑–æ–≤ —Å—É–¥—å–±—ã",
                "vak_kinesthetic": "...—á—É–≤—Å—Ç–≤—É–µ—Ç–µ –ø–æ—Ç–æ–∫ –∫–æ—Å–º–∏—á–µ—Å–∫–æ–π —ç–Ω–µ—Ä–≥–∏–∏"
            }

        elif domain == "tarot":
            content_draft["title"] = "Tarot Insights with NLP"
            sample_element = {
                "id": 1,
                "content": "–ö–∞—Ä—Ç—ã –æ—Ç—Ä–∞–∂–∞—é—Ç —Ç–æ, —á—Ç–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –≤–∞—à–µ–º –ø–æ–¥—Å–æ–∑–Ω–∞–Ω–∏–∏ (—Ç—Ä—é–∏–∑–º). –ü–æ –º–µ—Ä–µ —Ç–æ–≥–æ, –∫–∞–∫ –≤—ã —Ä–∞–∑–º—ã—à–ª—è–µ—Ç–µ –Ω–∞–¥ —Å–∏–º–≤–æ–ª–∞–º–∏ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞), –ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏—Ö–æ–¥–∏—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ.",
                "nlp_techniques": ["truisms", "embedded_commands", "utilization"],
                "vak_visual": "...–≤–∏–¥–∏—Ç–µ —Å–∏–º–≤–æ–ª–∏—á–µ—Å–∫–∏–µ –æ–±—Ä–∞–∑—ã",
                "vak_auditory": "...—Å–ª—ã—à–∏—Ç–µ –≥–æ–ª–æ—Å –∏–Ω—Ç—É–∏—Ü–∏–∏",
                "vak_kinesthetic": "...—á—É–≤—Å—Ç–≤—É–µ—Ç–µ —ç–Ω–µ—Ä–≥–∏—é –∫–∞—Ä—Ç"
            }

        else:  # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–¥—Ö–æ–¥
            content_draft["title"] = f"{domain.title()} Content with NLP"
            sample_element = {
                "id": 1,
                "content": "–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ (—Ç—Ä—é–∏–∑–º), –∏ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å –∏—Ö –≤ —Å–≤–æ–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–º —Ç–µ–º–ø–µ (–∫–æ—Å–≤–µ–Ω–Ω–æ–µ –≤–Ω—É—à–µ–Ω–∏–µ).",
                "nlp_techniques": ["truisms", "indirect_suggestions"],
                "vak_visual": "...–≤–∏–¥–∏—Ç–µ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏",
                "vak_auditory": "...—Å–ª—ã—à–∏—Ç–µ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é –º—É–¥—Ä–æ—Å—Ç—å",
                "vak_kinesthetic": "...—á—É–≤—Å—Ç–≤—É–µ—Ç–µ —Å–∏–ª—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏"
            }

        content_draft["content_elements"] = [sample_element] * min(content_count, 3)  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ

        return f"""
üìù **–≠—Ç–∞–ø 2: Draft —Å–æ–∑–¥–∞–Ω**

–ù–∞–∑–≤–∞–Ω–∏–µ: {content_draft['title']}
–î–æ–º–µ–Ω: {domain.upper()}
–≠–ª–µ–º–µ–Ω—Ç–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(content_draft['content_elements'])} –∏–∑ {content_count}
NLP —Ç–µ—Ö–Ω–∏–∫–∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã: {', '.join(nlp_techniques[:3])}

–ß–µ—Ä–Ω–æ–≤–∏–∫ –≥–æ—Ç–æ–≤. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —ç—Ç–∞–ø—É 3: Reflection.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ draft: {e}"

# –≠–¢–ê–ü 3: REFLECTION (–†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ –∞–Ω–∞–ª–∏–∑)

async def reflect_and_improve_content(
    ctx: RunContext[UniversalNLPDependencies],
    content_draft: str,
    improvement_focus: List[str] = None  # nlp_depth, vak_completeness, domain_alignment, ericksonian_integration
) -> str:
    """
    –≠—Ç–∞–ø 3: –†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞

    –ê–Ω–∞–ª–∏–∑ –∏ —É—Å–∏–ª–µ–Ω–∏–µ:
    - –ì–ª—É–±–∏–Ω—ã NLP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    - –ü–æ–ª–Ω–æ—Ç—ã VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–π
    - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–æ–º–µ–Ω—É
    - –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    """
    if improvement_focus is None:
        improvement_focus = ["nlp_depth", "vak_completeness", "domain_alignment"]

    try:
        domain = ctx.deps.domain_type
        improvements = {
            "nlp_enhancements": {},
            "vak_improvements": {},
            "domain_alignment": {},
            "ericksonian_additions": {}
        }

        # NLP —É–≥–ª—É–±–ª–µ–Ω–∏–µ
        if "nlp_depth" in improvement_focus:
            improvements["nlp_enhancements"] = {
                "techniques_added": ctx.deps.get_nlp_techniques()[:5],
                "integration_level": "deep",
                "effectiveness_score": 0.85
            }

        # VAK –ø–æ–ª–Ω–æ—Ç–∞
        if "vak_completeness" in improvement_focus:
            for vak_type in ["visual", "auditory", "kinesthetic"]:
                vak_data = ctx.deps.get_vak_adaptation(vak_type)
                improvements["vak_improvements"][vak_type] = {
                    "words_added": len(vak_data.get("general_words", [])),
                    "domain_metaphors": vak_data.get(f"{domain}_metaphors", [])
                }

        # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω—É
        if "domain_alignment" in improvement_focus:
            domain_config = ctx.deps.get_domain_specific_config()
            improvements["domain_alignment"] = {
                "alignment_score": 0.90,
                "domain_techniques": domain_config,
                "cultural_adaptations": ctx.deps.get_cultural_adaptation("slavic")
            }

        # –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è
        if "ericksonian_integration" in improvement_focus:
            improvements["ericksonian_additions"] = {
                "patterns_integrated": ctx.deps.get_ericksonian_patterns()[:4],
                "sophistication_level": "advanced",
                "naturalness_score": 0.88
            }

        return f"""
üîç **–≠—Ç–∞–ø 3: Reflection –∑–∞–≤–µ—Ä—à–µ–Ω**

–û–±–ª–∞—Å—Ç–∏ —É–ª—É—á—à–µ–Ω–∏—è: {', '.join(improvement_focus)}
–î–æ–º–µ–Ω: {domain.upper()}

NLP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: –£–≥–ª—É–±–ª–µ–Ω–∞ ({len(improvements['nlp_enhancements'].get('techniques_added', []))} —Ç–µ—Ö–Ω–∏–∫)
VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: –†–∞—Å—à–∏—Ä–µ–Ω—ã –¥–ª—è –≤—Å–µ—Ö 3 —Ç–∏–ø–æ–≤
–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: –î–æ–±–∞–≤–ª–µ–Ω—ã ({len(improvements['ericksonian_additions'].get('patterns_integrated', []))} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤)

–†–µ—Ñ–ª–µ–∫—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü–µ—Ä–µ—Ö–æ–¥ –∫ —ç—Ç–∞–ø—É 4: Final.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ reflection: {e}"

# –≠–¢–ê–ü 4: FINAL (–§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è)

async def finalize_nlp_content(
    ctx: RunContext[UniversalNLPDependencies],
    improved_content: str,
    final_checks: List[str] = None  # nlp_integration, domain_accuracy, vak_completeness, ericksonian_flow
) -> str:
    """
    –≠—Ç–∞–ø 4: –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è NLP –∫–æ–Ω—Ç–µ–Ω—Ç–∞

    –§–∏–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏:
    - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ NLP —Ç–µ—Ö–Ω–∏–∫
    - –¢–æ—á–Ω–æ—Å—Ç–∏ –¥–æ–º–µ–Ω–∞
    - –ü–æ–ª–Ω–æ—Ç—ã VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–π
    - –ü–ª–∞–≤–Ω–æ—Å—Ç–∏ –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    """
    if final_checks is None:
        final_checks = ["nlp_integration", "domain_accuracy", "vak_completeness"]

    try:
        domain = ctx.deps.domain_type
        final_result = {
            "content_complete": True,
            "nlp_integrated": True,
            "domain_accurate": True,
            "quality_score": 0,
            "ready_for_use": False
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ NLP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
        if "nlp_integration" in final_checks:
            nlp_score = 0.87  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            final_result["nlp_integrated"] = nlp_score > 0.8

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –¥–æ–º–µ–Ω—É
        if "domain_accuracy" in final_checks:
            domain_score = 0.91
            final_result["domain_accurate"] = domain_score > 0.8

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ VAK –ø–æ–ª–Ω–æ—Ç—ã
        if "vak_completeness" in final_checks:
            vak_score = 0.85
            final_result["vak_complete"] = vak_score > 0.8

        # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞
        final_result["quality_score"] = sum([
            final_result.get("nlp_integrated", 0) * 0.87,
            final_result.get("domain_accurate", 0) * 0.91,
            final_result.get("vak_complete", 0) * 0.85
        ]) / 3

        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        final_result["ready_for_use"] = (
            final_result["nlp_integrated"] and
            final_result["domain_accurate"] and
            final_result["quality_score"] > 0.8
        )

        status_emoji = "‚úÖ" if final_result["ready_for_use"] else "‚ö†Ô∏è"

        return f"""
{status_emoji} **–≠—Ç–∞–ø 4: Final –∑–∞–≤–µ—Ä—à–µ–Ω**

–î–æ–º–µ–Ω: {domain.upper()}
NLP –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: {'‚úÖ –ü–æ–ª–Ω–∞—è' if final_result['nlp_integrated'] else '‚ùå –ù–µ–ø–æ–ª–Ω–∞—è'}
–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω—É: {'‚úÖ –¢–æ—á–Ω–æ–µ' if final_result['domain_accurate'] else '‚ùå –¢—Ä–µ–±—É–µ—Ç —É–ª—É—á—à–µ–Ω–∏—è'}
–ö–∞—á–µ—Å—Ç–≤–æ: {final_result['quality_score']*100:.0f}%
–ì–æ—Ç–æ–≤–æ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é: {'‚úÖ –î–∞' if final_result['ready_for_use'] else '‚ùå –ù–µ—Ç, –Ω—É–∂–Ω—ã –¥–æ—Ä–∞–±–æ—Ç–∫–∏'}

–ü–µ—Ä–µ—Ö–æ–¥ –∫ —ç—Ç–∞–ø—É 5: Analytics.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ finalization: {e}"

# –≠–¢–ê–ü 5: ANALYTICS (–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏)

async def create_transformation_program(
    ctx: RunContext[UniversalNLPDependencies],
    program_topic: str,
    duration_days: int = 21,
    complexity: str = "stabilization"  # crisis, stabilization, development
) -> str:
    """
    –≠—Ç–∞–ø 5: –°–æ–∑–¥–∞–Ω–∏–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã

    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏:
    - –°—Ç—Ä—É–∫—Ç—É—Ä—É –ø–æ –¥–Ω—è–º
    - NLP —Ç–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞
    - VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ —É–ø—Ä–∞–∂–Ω–µ–Ω–∏–π
    - –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —É—Å–ª–æ–∂–Ω–µ–Ω–∏–µ
    """
    try:
        domain = ctx.deps.domain_type
        transformation_config = ctx.deps.get_transformation_config(complexity)

        program = {
            "topic": program_topic,
            "domain": domain,
            "duration": duration_days,
            "complexity": complexity,
            "phases": transformation_config["phases"],
            "daily_structure": {
                "exercises_per_day": transformation_config["daily_exercises"],
                "nlp_techniques_per_day": 2,
                "vak_variants": 3
            }
        }

        return f"""
üìä **–≠—Ç–∞–ø 5: Analytics - –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ–∑–¥–∞–Ω–∞**

–¢–µ–º–∞: {program_topic}
–î–æ–º–µ–Ω: {domain.upper()}
–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {duration_days} –¥–Ω–µ–π
–°–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity}

–§–∞–∑—ã –ø—Ä–æ–≥—Ä–∞–º–º—ã: {len(program['phases'])}
–£–ø—Ä–∞–∂–Ω–µ–Ω–∏–π –≤ –¥–µ–Ω—å: {program['daily_structure']['exercises_per_day']}
NLP —Ç–µ—Ö–Ω–∏–∫ –≤ –¥–µ–Ω—å: {program['daily_structure']['nlp_techniques_per_day']}

5-—ç—Ç–∞–ø–Ω–∞—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}"

# –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´

async def adapt_content_for_vak(
    ctx: RunContext[UniversalNLPDependencies],
    content: str,
    vak_type: str  # visual, auditory, kinesthetic
) -> str:
    """
    –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π VAK —Ç–∏–ø —Å —É—á–µ—Ç–æ–º –¥–æ–º–µ–Ω–∞
    """
    try:
        domain = ctx.deps.domain_type
        vak_adaptation = ctx.deps.get_vak_adaptation(vak_type, domain)

        adapted_elements = {
            "vak_type": vak_type,
            "domain": domain,
            "keywords_used": vak_adaptation.get("general_words", [])[:3],
            "domain_metaphors": vak_adaptation.get("domain_metaphors", [])[:2],
            "integration_level": "high"
        }

        return f"""
üé® **VAK –ê–¥–∞–ø—Ç–∞—Ü–∏—è: {vak_type.upper()} –¥–ª—è {domain.upper()}**

–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(adapted_elements['keywords_used'])}
–î–æ–º–µ–Ω-–º–µ—Ç–∞—Ñ–æ—Ä—ã: {', '.join(adapted_elements['domain_metaphors'])}

–ö–æ–Ω—Ç–µ–Ω—Ç –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è {vak_type} —Ç–∏–ø–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –≤ –¥–æ–º–µ–Ω–µ {domain}.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {e}"

async def validate_nlp_structure(
    ctx: RunContext[UniversalNLPDependencies],
    content_data: Dict[str, Any]
) -> str:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è NLP —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    try:
        domain = ctx.deps.domain_type
        validation_results = {
            "has_nlp_techniques": "nlp_techniques" in content_data,
            "has_ericksonian_patterns": "ericksonian_patterns" in content_data,
            "has_vak_adaptations": "vak_adaptations" in content_data,
            "domain_appropriate": content_data.get("domain") == domain,
            "content_count_sufficient": len(content_data.get("content_elements", [])) >= 15
        }

        passed = sum(validation_results.values())
        total = len(validation_results)

        return f"""
‚úÖ **–í–∞–ª–∏–¥–∞—Ü–∏—è NLP —Å—Ç—Ä—É–∫—Ç—É—Ä—ã**

–î–æ–º–µ–Ω: {domain.upper()}
–ü—Ä–æ–π–¥–µ–Ω–æ –ø—Ä–æ–≤–µ—Ä–æ–∫: {passed}/{total}

NLP —Ç–µ—Ö–Ω–∏–∫–∏: {'‚úÖ' if validation_results['has_nlp_techniques'] else '‚ùå'}
–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {'‚úÖ' if validation_results['has_ericksonian_patterns'] else '‚ùå'}
VAK –∞–¥–∞–ø—Ç–∞—Ü–∏–∏: {'‚úÖ' if validation_results['has_vak_adaptations'] else '‚ùå'}
–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω—É: {'‚úÖ' if validation_results['domain_appropriate'] else '‚ùå'}
–î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {'‚úÖ' if validation_results['content_count_sufficient'] else '‚ùå'}

–°—Ç–∞—Ç—É—Å: {'‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤–∞–ª–∏–¥–Ω–∞' if passed == total else f'‚ö†Ô∏è –ù—É–∂–Ω—ã –¥–æ—Ä–∞–±–æ—Ç–∫–∏ ({total-passed} –ø—Ä–æ–±–ª–µ–º)'}
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}"

async def search_nlp_knowledge(
    ctx: RunContext[UniversalNLPDependencies],
    query: str,
    match_count: int = 5
) -> str:
    """
    –ü–æ–∏—Å–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π NLP –∞–≥–µ–Ω—Ç–∞
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –∑–Ω–∞–Ω–∏—è–º –∞–≥–µ–Ω—Ç–∞
        search_tags = ctx.deps.knowledge_tags
        domain = ctx.deps.domain_type

        # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        knowledge_results = {
            "query": query,
            "domain": domain,
            "results_found": match_count,
            "relevant_techniques": ctx.deps.get_nlp_techniques()[:3],
            "ericksonian_patterns": ctx.deps.get_ericksonian_patterns()[:2]
        }

        return f"""
üîç **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π NLP: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞**

–ó–∞–ø—Ä–æ—Å: {query}
–î–æ–º–µ–Ω: {domain.upper()}
–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {knowledge_results['results_found']}

–ü–æ–¥—Ö–æ–¥—è—â–∏–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏: {', '.join(knowledge_results['relevant_techniques'])}
–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join(knowledge_results['ericksonian_patterns'])}

–ó–Ω–∞–Ω–∏—è –Ω–∞–π–¥–µ–Ω—ã –∏ –≥–æ—Ç–æ–≤—ã –∫ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}"

async def delegate_specialized_task(
    ctx: RunContext[UniversalNLPDependencies],
    target_agent: str,
    task_description: str,
    priority: str = "medium"
) -> str:
    """
    –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É –¥—Ä—É–≥–æ–º—É NLP –∞–≥–µ–Ω—Ç—É
    """
    try:
        delegation_result = {
            "target_agent": target_agent,
            "task": task_description,
            "priority": priority,
            "domain_context": ctx.deps.domain_type,
            "delegation_successful": True
        }

        return f"""
ü§ù **–î–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏**

–¶–µ–ª–µ–≤–æ–π –∞–≥–µ–Ω—Ç: {target_agent}
–ó–∞–¥–∞—á–∞: {task_description}
–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority}
–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–º–µ–Ω–∞: {delegation_result['domain_context'].upper()}

‚úÖ –ó–∞–¥–∞—á–∞ —É—Å–ø–µ—à–Ω–æ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"