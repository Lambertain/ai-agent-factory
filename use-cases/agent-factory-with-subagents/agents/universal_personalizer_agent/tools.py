# -*- coding: utf-8 -*-
"""
–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è Universal Personalizer Agent
–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
"""

import json
import asyncio
from typing import Dict, List, Any, Optional
from pydantic_ai import RunContext

try:
    # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ MCP Archon –¥–ª—è RAG
    from mcp_tools import mcp_archon_rag_search_knowledge_base, mcp_archon_manage_task
    MCP_AVAILABLE = True
except ImportError:
    MCP_AVAILABLE = False

from .dependencies import PersonalizerDependencies, AGENT_COMPETENCIES, AGENT_ASSIGNEE_MAP

async def analyze_user_profile(
    ctx: RunContext[PersonalizerDependencies],
    user_data: Dict[str, Any],
    analysis_depth: str = "comprehensive",
    domain_context: str = None
) -> str:
    """
    –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        user_data: –î–∞–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–ø–æ–≤–µ–¥–µ–Ω–∏–µ, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—è)
        analysis_depth: –ì–ª—É–±–∏–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ (basic, comprehensive, deep)
        domain_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–º–µ–Ω–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    try:
        domain_type = domain_context or ctx.deps.domain_type
        personalization_factors = ctx.deps.get_personalization_factors_for_domain()

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ–º–µ–Ω–∞
        if domain_type == "psychology":
            return await _analyze_psychology_profile(user_data, analysis_depth, personalization_factors)
        elif domain_type == "astrology":
            return await _analyze_astrology_profile(user_data, analysis_depth, personalization_factors)
        elif domain_type == "numerology":
            return await _analyze_numerology_profile(user_data, analysis_depth, personalization_factors)
        elif domain_type == "business":
            return await _analyze_business_profile(user_data, analysis_depth, personalization_factors)
        else:
            return await _analyze_universal_profile(user_data, analysis_depth, personalization_factors)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ—Ñ–∏–ª—è: {e}"

async def _analyze_psychology_profile(user_data: Dict[str, Any], depth: str, factors: List[str]) -> str:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    analysis = {
        "psychological_profile": {
            "personality_traits": user_data.get("personality", {}),
            "emotional_state": user_data.get("emotional_indicators", {}),
            "therapeutic_goals": user_data.get("goals", []),
            "cultural_background": user_data.get("culture", "unknown"),
            "previous_interventions": user_data.get("history", [])
        },
        "personalization_recommendations": {
            "content_adaptation": "Evidence-based therapeutic content matching personality and goals",
            "intervention_timing": "Optimize based on emotional state patterns",
            "cultural_sensitivity": "Adapt language and examples to cultural background",
            "progress_tracking": "Personalized metrics based on therapeutic goals"
        },
        "risk_factors": {
            "trauma_indicators": user_data.get("trauma_history", False),
            "crisis_risk": user_data.get("crisis_indicators", "low"),
            "support_system": user_data.get("support_level", "unknown")
        },
        "recommended_approaches": _get_psychology_approaches(user_data),
        "personalization_confidence": _calculate_confidence(user_data, factors)
    }

    return json.dumps(analysis, ensure_ascii=False, indent=2)

async def _analyze_astrology_profile(user_data: Dict[str, Any], depth: str, factors: List[str]) -> str:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    analysis = {
        "astrological_profile": {
            "birth_chart_data": user_data.get("birth_info", {}),
            "astrological_preferences": user_data.get("traditions", ["western"]),
            "experience_level": user_data.get("experience", "beginner"),
            "spiritual_orientation": user_data.get("spirituality", "open"),
            "consultation_history": user_data.get("consultations", [])
        },
        "personalization_recommendations": {
            "chart_emphasis": "Focus on most relevant planetary aspects",
            "interpretation_style": "Match depth to experience level",
            "cultural_context": "Adapt to preferred astrological tradition",
            "timing_sensitivity": "Provide relevant transit information"
        },
        "astrological_insights": {
            "dominant_elements": user_data.get("elements", []),
            "key_aspects": user_data.get("aspects", []),
            "life_themes": user_data.get("themes", [])
        },
        "recommended_content": _get_astrology_content(user_data),
        "personalization_confidence": _calculate_confidence(user_data, factors)
    }

    return json.dumps(analysis, ensure_ascii=False, indent=2)

async def _analyze_numerology_profile(user_data: Dict[str, Any], depth: str, factors: List[str]) -> str:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    analysis = {
        "numerological_profile": {
            "core_numbers": user_data.get("core_numbers", {}),
            "life_path": user_data.get("life_path", None),
            "personal_year": user_data.get("personal_year", None),
            "name_vibrations": user_data.get("name_analysis", {}),
            "system_preference": user_data.get("system", "pythagorean")
        },
        "personalization_recommendations": {
            "calculation_focus": "Emphasize most relevant numbers",
            "interpretation_depth": "Match to user's understanding level",
            "practical_application": "Provide actionable insights",
            "cultural_adaptation": "Adapt meanings to cultural context"
        },
        "numerological_insights": {
            "strengths": user_data.get("strengths", []),
            "challenges": user_data.get("challenges", []),
            "opportunities": user_data.get("opportunities", []),
            "life_cycles": user_data.get("cycles", [])
        },
        "recommended_guidance": _get_numerology_guidance(user_data),
        "personalization_confidence": _calculate_confidence(user_data, factors)
    }

    return json.dumps(analysis, ensure_ascii=False, indent=2)

async def _analyze_business_profile(user_data: Dict[str, Any], depth: str, factors: List[str]) -> str:
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –¥–ª—è –±–∏–∑–Ω–µ—Å-–¥–æ–º–µ–Ω–∞."""
    analysis = {
        "business_profile": {
            "industry_sector": user_data.get("industry", "unknown"),
            "company_size": user_data.get("company_size", "unknown"),
            "role_level": user_data.get("role", "unknown"),
            "business_goals": user_data.get("goals", []),
            "market_position": user_data.get("market_position", "unknown"),
            "technology_adoption": user_data.get("tech_level", "medium")
        },
        "personalization_recommendations": {
            "content_relevance": "Industry-specific insights and benchmarks",
            "complexity_level": "Match to role and experience level",
            "actionability": "Provide implementation-ready recommendations",
            "roi_focus": "Emphasize measurable business outcomes"
        },
        "business_insights": {
            "competitive_advantages": user_data.get("advantages", []),
            "growth_opportunities": user_data.get("opportunities", []),
            "operational_challenges": user_data.get("challenges", []),
            "market_trends": user_data.get("trends", [])
        },
        "recommended_strategies": _get_business_strategies(user_data),
        "personalization_confidence": _calculate_confidence(user_data, factors)
    }

    return json.dumps(analysis, ensure_ascii=False, indent=2)

async def _analyze_universal_profile(user_data: Dict[str, Any], depth: str, factors: List[str]) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è."""
    analysis = {
        "universal_profile": {
            "user_segments": user_data.get("segments", []),
            "behavioral_patterns": user_data.get("behavior", {}),
            "preferences": user_data.get("preferences", {}),
            "engagement_history": user_data.get("engagement", {}),
            "demographic_info": user_data.get("demographics", {})
        },
        "personalization_recommendations": {
            "content_matching": "Align content with user preferences and behavior",
            "engagement_optimization": "Optimize based on interaction patterns",
            "experience_customization": "Adapt interface and flow to user needs",
            "recommendation_tuning": "Fine-tune recommendations based on feedback"
        },
        "insights": {
            "key_interests": user_data.get("interests", []),
            "interaction_patterns": user_data.get("patterns", []),
            "satisfaction_drivers": user_data.get("satisfaction", [])
        },
        "personalization_confidence": _calculate_confidence(user_data, factors)
    }

    return json.dumps(analysis, ensure_ascii=False, indent=2)

def _get_psychology_approaches(user_data: Dict[str, Any]) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–æ–¥—Ö–æ–¥—ã."""
    approaches = []
    personality = user_data.get("personality", {})

    if personality.get("openness", 0) > 0.7:
        approaches.append("mindfulness_based_interventions")
    if personality.get("conscientiousness", 0) > 0.6:
        approaches.append("structured_cognitive_behavioral")
    if user_data.get("culture", "").lower() in ["ukraine", "eastern_european"]:
        approaches.append("culturally_adapted_therapy")

    return approaches

def _get_astrology_content(user_data: Dict[str, Any]) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç."""
    content = []
    experience = user_data.get("experience", "beginner")

    if experience == "beginner":
        content.extend(["basic_chart_reading", "planetary_meanings", "sign_characteristics"])
    elif experience == "intermediate":
        content.extend(["aspect_patterns", "transits", "progressions"])
    else:
        content.extend(["advanced_techniques", "traditional_methods", "predictive_astrology"])

    return content

def _get_numerology_guidance(user_data: Dict[str, Any]) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
    guidance = []
    life_path = user_data.get("life_path")

    if life_path:
        guidance.append(f"life_path_{life_path}_guidance")
    if user_data.get("personal_year"):
        guidance.append("personal_year_insights")
    if user_data.get("business_focus"):
        guidance.append("business_numerology")

    return guidance

def _get_business_strategies(user_data: Dict[str, Any]) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å –±–∏–∑–Ω–µ—Å-—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏."""
    strategies = []
    industry = user_data.get("industry", "")
    size = user_data.get("company_size", "")

    if size in ["startup", "small"]:
        strategies.extend(["growth_hacking", "lean_operations", "agile_development"])
    elif size in ["medium", "large"]:
        strategies.extend(["digital_transformation", "process_optimization", "data_analytics"])

    if industry in ["tech", "software"]:
        strategies.append("innovation_management")

    return strategies

def _calculate_confidence(user_data: Dict[str, Any], factors: List[str]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏."""
    available_factors = len([f for f in factors if f in str(user_data)])
    total_factors = len(factors)

    if total_factors == 0:
        return 0.5

    base_confidence = available_factors / total_factors
    data_completeness = len(user_data) / 10  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 10 –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π

    return min(0.95, (base_confidence * 0.7 + min(1.0, data_completeness) * 0.3))

async def generate_personalized_content(
    ctx: RunContext[PersonalizerDependencies],
    base_content: str,
    user_profile: Dict[str, Any],
    personalization_strategy: str = "adaptive",
    content_type: str = "general"
) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        base_content: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        user_profile: –ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        personalization_strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        content_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

    Returns:
        –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    """
    try:
        domain_type = ctx.deps.domain_type
        content_types = ctx.deps.get_content_types_for_domain()
        adaptation_rules = ctx.deps.get_adaptation_rules_for_domain()

        # –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–æ–º–µ–Ω–∞
        if domain_type == "psychology":
            return await _personalize_psychology_content(base_content, user_profile, adaptation_rules)
        elif domain_type == "astrology":
            return await _personalize_astrology_content(base_content, user_profile, adaptation_rules)
        elif domain_type == "numerology":
            return await _personalize_numerology_content(base_content, user_profile, adaptation_rules)
        elif domain_type == "business":
            return await _personalize_business_content(base_content, user_profile, adaptation_rules)
        else:
            return await _personalize_universal_content(base_content, user_profile, adaptation_rules)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}"

async def _personalize_psychology_content(content: str, profile: Dict[str, Any], rules: List[str]) -> str:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    personalized = content

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∫—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
    if "cultural_sensitivity" in rules:
        culture = profile.get("psychological_profile", {}).get("cultural_background", "")
        if culture:
            personalized = _adapt_cultural_context(personalized, culture)

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏
    goals = profile.get("psychological_profile", {}).get("therapeutic_goals", [])
    if goals and "evidence_based_matching" in rules:
        personalized = _adapt_therapeutic_goals(personalized, goals)

    # –£—á–µ—Ç —Ç—Ä–∞–≤–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞
    trauma = profile.get("risk_factors", {}).get("trauma_indicators", False)
    if trauma and "trauma_informed" in rules:
        personalized = _apply_trauma_informed_approach(personalized)

    return personalized

async def _personalize_astrology_content(content: str, profile: Dict[str, Any], rules: List[str]) -> str:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    personalized = content

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–¥–∏—Ü–∏—é
    traditions = profile.get("astrological_profile", {}).get("astrological_preferences", ["western"])
    if "traditional_accuracy" in rules:
        personalized = _adapt_astrological_tradition(personalized, traditions[0])

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å –æ–ø—ã—Ç–∞
    experience = profile.get("astrological_profile", {}).get("experience_level", "beginner")
    if "depth_level_matching" in rules:
        personalized = _adapt_astrology_depth(personalized, experience)

    return personalized

async def _personalize_numerology_content(content: str, profile: Dict[str, Any], rules: List[str]) -> str:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    personalized = content

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É
    system = profile.get("numerological_profile", {}).get("system_preference", "pythagorean")
    if "system_consistency" in rules:
        personalized = _adapt_numerology_system(personalized, system)

    # –§–æ–∫—É—Å –Ω–∞ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–º –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏
    if "practical_application" in rules:
        personalized = _enhance_practical_application(personalized)

    return personalized

async def _personalize_business_content(content: str, profile: Dict[str, Any], rules: List[str]) -> str:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    personalized = content

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∏–Ω–¥—É—Å—Ç—Ä–∏—é
    industry = profile.get("business_profile", {}).get("industry_sector", "")
    if industry and "industry_relevance" in rules:
        personalized = _adapt_industry_context(personalized, industry)

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ä–æ–ª—å
    role = profile.get("business_profile", {}).get("role_level", "")
    if role and "role_based_filtering" in rules:
        personalized = _adapt_role_level(personalized, role)

    return personalized

async def _personalize_universal_content(content: str, profile: Dict[str, Any], rules: List[str]) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    personalized = content

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
    preferences = profile.get("universal_profile", {}).get("preferences", {})
    if preferences:
        personalized = _adapt_user_preferences(personalized, preferences)

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    patterns = profile.get("universal_profile", {}).get("behavioral_patterns", {})
    if patterns:
        personalized = _adapt_behavioral_patterns(personalized, patterns)

    return personalized

def _adapt_cultural_context(content: str, culture: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ –∫—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã —Å–ª–æ–∂–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º
    cultural_adaptations = {
        "ukraine": "–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è —É–∫—Ä–∞–∏–Ω—Å–∫–æ–≥–æ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞",
        "poland": "–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –ø–æ–ª—å—Å–∫–æ–≥–æ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"
    }

    adaptation = cultural_adaptations.get(culture.lower(), "")
    if adaptation:
        content = f"{content}\n\n[{adaptation}]"

    return content

def _adapt_therapeutic_goals(content: str, goals: List[str]) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–µ —Ü–µ–ª–∏."""
    if "anxiety_reduction" in goals:
        content = content.replace("—Å—Ç—Ä–µ—Å—Å", "—Ç—Ä–µ–≤–æ–≥–∞ –∏ —Å—Ç—Ä–µ—Å—Å")
    if "depression_treatment" in goals:
        content = content.replace("–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ", "–¥–µ–ø—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ")

    return content

def _apply_trauma_informed_approach(content: str) -> str:
    """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ trauma-informed –ø–æ–¥—Ö–æ–¥–∞."""
    return f"[Trauma-informed approach] {content}"

def _adapt_astrological_tradition(content: str, tradition: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Ç—Ä–∞–¥–∏—Ü–∏—é."""
    if tradition == "vedic":
        content = content.replace("–∑–Ω–∞–∫ –∑–æ–¥–∏–∞–∫–∞", "—Ä–∞—à–∏")
    elif tradition == "chinese":
        content = content.replace("–ø–ª–∞–Ω–µ—Ç–∞", "–Ω–µ–±–µ—Å–Ω—ã–π —Å—Ç–≤–æ–ª")

    return content

def _adapt_astrology_depth(content: str, experience: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –≥–ª—É–±–∏–Ω—ã –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    if experience == "beginner":
        content = f"[–î–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö] {content}"
    elif experience == "advanced":
        content = f"[–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑] {content}"

    return content

def _adapt_numerology_system(content: str, system: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–∏—Å—Ç–µ–º—É."""
    system_notes = {
        "pythagorean": "[–ü–∏—Ñ–∞–≥–æ—Ä–µ–π—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞]",
        "chaldean": "[–•–∞–ª–¥–µ–π—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞]",
        "kabbalah": "[–ö–∞–±–±–∞–ª–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Å–∏—Å—Ç–µ–º–∞]"
    }

    note = system_notes.get(system, "")
    if note:
        content = f"{note} {content}"

    return content

def _enhance_practical_application(content: str) -> str:
    """–£—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è."""
    return f"{content}\n\n–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ: [–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏]"

def _adapt_industry_context(content: str, industry: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –æ—Ç—Ä–∞—Å–ª–µ–≤–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç."""
    return f"[{industry.upper()}] {content}"

def _adapt_role_level(content: str, role: str) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ —É—Ä–æ–≤–µ–Ω—å —Ä–æ–ª–∏."""
    if role in ["ceo", "executive"]:
        content = f"[–°—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å] {content}"
    elif role in ["manager", "supervisor"]:
        content = f"[–£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–π —É—Ä–æ–≤–µ–Ω—å] {content}"
    else:
        content = f"[–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å] {content}"

    return content

def _adapt_user_preferences(content: str, preferences: Dict[str, Any]) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è."""
    if preferences.get("detail_level") == "brief":
        content = content[:200] + "..."
    elif preferences.get("detail_level") == "comprehensive":
        content = f"{content}\n\n[–ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞...]"

    return content

def _adapt_behavioral_patterns(content: str, patterns: Dict[str, Any]) -> str:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ø–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã."""
    if patterns.get("engagement_time") == "short":
        content = f"‚ö° {content}"  # –ë—ã—Å—Ç—Ä–æ–µ —á—Ç–µ–Ω–∏–µ
    elif patterns.get("learning_style") == "visual":
        content = f"üìä {content}"  # –í–∏–∑—É–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç

    return content

async def create_personalization_rules(
    ctx: RunContext[PersonalizerDependencies],
    user_segments: List[str],
    content_categories: List[str],
    business_objectives: List[str] = None
) -> str:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        user_segments: –°–µ–≥–º–µ–Ω—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
        content_categories: –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        business_objectives: –ë–∏–∑–Ω–µ—Å-—Ü–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

    Returns:
        –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    try:
        domain_type = ctx.deps.domain_type
        adaptation_rules = ctx.deps.get_adaptation_rules_for_domain()

        rules = {
            "domain": domain_type,
            "segments": user_segments,
            "content_categories": content_categories,
            "adaptation_rules": adaptation_rules,
            "business_objectives": business_objectives or [],
            "personalization_matrix": {},
            "validation_criteria": ctx.deps.get_personalization_criteria()
        }

        # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        for segment in user_segments:
            rules["personalization_matrix"][segment] = {}
            for category in content_categories:
                rules["personalization_matrix"][segment][category] = {
                    "adaptation_level": "medium",
                    "content_filters": [],
                    "presentation_style": "default",
                    "interaction_patterns": [],
                    "success_metrics": []
                }

        # –î–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–∞–≤–∏–ª–∞
        if domain_type == "psychology":
            rules = _enhance_psychology_rules(rules)
        elif domain_type == "astrology":
            rules = _enhance_astrology_rules(rules)
        elif domain_type == "numerology":
            rules = _enhance_numerology_rules(rules)
        elif domain_type == "business":
            rules = _enhance_business_rules(rules)

        return json.dumps(rules, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∞–≤–∏–ª –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"

def _enhance_psychology_rules(rules: Dict[str, Any]) -> Dict[str, Any]:
    """–£—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    rules["ethical_guidelines"] = [
        "therapeutic_alliance_first",
        "do_no_harm",
        "cultural_competence",
        "trauma_informed_care"
    ]
    rules["privacy_requirements"] = [
        "hipaa_compliance",
        "therapeutic_confidentiality",
        "informed_consent"
    ]
    return rules

def _enhance_astrology_rules(rules: Dict[str, Any]) -> Dict[str, Any]:
    """–£—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –∞—Å—Ç—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    rules["cultural_considerations"] = [
        "astrological_tradition_respect",
        "spiritual_sensitivity",
        "cultural_symbol_appropriateness"
    ]
    rules["accuracy_requirements"] = [
        "ephemeris_precision",
        "calculation_verification",
        "traditional_methodology"
    ]
    return rules

def _enhance_numerology_rules(rules: Dict[str, Any]) -> Dict[str, Any]:
    """–£—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –Ω—É–º–µ—Ä–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–º–µ–Ω–∞."""
    rules["calculation_standards"] = [
        "system_consistency",
        "mathematical_accuracy",
        "cultural_number_meanings"
    ]
    rules["practical_focus"] = [
        "actionable_insights",
        "real_world_application",
        "measurable_outcomes"
    ]
    return rules

def _enhance_business_rules(rules: Dict[str, Any]) -> Dict[str, Any]:
    """–£—Å–∏–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –¥–ª—è –±–∏–∑–Ω–µ—Å-–¥–æ–º–µ–Ω–∞."""
    rules["business_alignment"] = [
        "roi_optimization",
        "strategic_relevance",
        "operational_feasibility"
    ]
    rules["performance_metrics"] = [
        "conversion_rates",
        "user_engagement",
        "business_outcomes"
    ]
    return rules

async def adapt_content_to_user(
    ctx: RunContext[PersonalizerDependencies],
    content_items: List[Dict[str, Any]],
    user_context: Dict[str, Any],
    adaptation_mode: str = "automatic"
) -> str:
    """
    –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

    Args:
        content_items: –≠–ª–µ–º–µ–Ω—Ç—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        adaptation_mode: –†–µ–∂–∏–º –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

    Returns:
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    """
    try:
        adapted_items = []

        for item in content_items:
            adapted_item = await _adapt_single_content_item(
                item, user_context, ctx.deps, adaptation_mode
            )
            adapted_items.append(adapted_item)

        result = {
            "adapted_content": adapted_items,
            "adaptation_metadata": {
                "user_context": user_context,
                "adaptation_mode": adaptation_mode,
                "domain": ctx.deps.domain_type,
                "confidence": _calculate_adaptation_confidence(content_items, user_context)
            }
        }

        return json.dumps(result, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}"

async def _adapt_single_content_item(
    item: Dict[str, Any],
    user_context: Dict[str, Any],
    deps: PersonalizerDependencies,
    mode: str
) -> Dict[str, Any]:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞."""
    adapted = item.copy()

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞
    if "title" in adapted:
        adapted["title"] = _personalize_text(adapted["title"], user_context, deps)

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
    if "content" in adapted:
        adapted["content"] = _personalize_text(adapted["content"], user_context, deps)

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
    adapted["personalization_applied"] = True
    adapted["adaptation_mode"] = mode
    adapted["user_segment"] = user_context.get("segment", "default")

    return adapted

def _personalize_text(text: str, user_context: Dict[str, Any], deps: PersonalizerDependencies) -> str:
    """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞."""
    personalized = text

    # –Ø–∑—ã–∫–æ–≤–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è
    language = user_context.get("language", deps.primary_language)
    if language != "ukrainian":
        personalized = f"[{language.upper()}] {personalized}"

    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
    if user_context.get("formal_tone", False):
        personalized = personalized.replace("—Ç—ã", "–í—ã")

    return personalized

def _calculate_adaptation_confidence(content_items: List[Dict[str, Any]], user_context: Dict[str, Any]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏."""
    context_completeness = len(user_context) / 8  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 8 –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–ª–µ–π
    content_adaptability = len([item for item in content_items if "content" in item]) / len(content_items)

    return min(0.95, (context_completeness * 0.6 + content_adaptability * 0.4))

# –û—Å—Ç–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Å–ª–µ–¥—É–µ—Ç...

async def track_personalization_effectiveness(
    ctx: RunContext[PersonalizerDependencies],
    user_interactions: List[Dict[str, Any]],
    personalization_applied: Dict[str, Any],
    success_metrics: List[str] = None
) -> str:
    """
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        user_interactions: –î–∞–Ω–Ω—ã–µ –æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        personalization_applied: –ü—Ä–∏–º–µ–Ω–µ–Ω–Ω–∞—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è
        success_metrics: –ú–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏

    Returns:
        –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    try:
        if success_metrics is None:
            success_metrics = ctx.deps.get_personalization_metrics_for_domain()

        effectiveness = {
            "overall_score": 0.0,
            "metric_scores": {},
            "interaction_analysis": {},
            "recommendations": [],
            "domain": ctx.deps.domain_type
        }

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –º–µ—Ç—Ä–∏–∫—É
        for metric in success_metrics:
            score = _calculate_metric_score(metric, user_interactions, personalization_applied)
            effectiveness["metric_scores"][metric] = score

        # –û–±—â–∏–π —Å—á–µ—Ç
        effectiveness["overall_score"] = sum(effectiveness["metric_scores"].values()) / len(success_metrics)

        # –ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
        effectiveness["interaction_analysis"] = _analyze_user_interactions(user_interactions)

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        effectiveness["recommendations"] = _generate_improvement_recommendations(
            effectiveness["metric_scores"], ctx.deps.domain_type
        )

        return json.dumps(effectiveness, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {e}"

def _calculate_metric_score(metric: str, interactions: List[Dict[str, Any]], personalization: Dict[str, Any]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—á–µ—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏."""
    if metric == "engagement":
        return _calculate_engagement_score(interactions)
    elif metric == "satisfaction":
        return _calculate_satisfaction_score(interactions)
    elif metric == "conversion":
        return _calculate_conversion_score(interactions)
    elif metric == "therapeutic_engagement":
        return _calculate_therapeutic_engagement(interactions)
    elif metric == "accuracy_perception":
        return _calculate_accuracy_perception(interactions)
    else:
        return 0.5  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π —Å—á–µ—Ç –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫

def _calculate_engagement_score(interactions: List[Dict[str, Any]]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—á–µ—Ç –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏."""
    if not interactions:
        return 0.0

    total_time = sum(interaction.get("duration", 0) for interaction in interactions)
    avg_time = total_time / len(interactions)

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ —à–∫–∞–ª–µ 0-1
    return min(1.0, avg_time / 300)  # 5 –º–∏–Ω—É—Ç –∫–∞–∫ –º–∞–∫—Å–∏–º—É–º

def _calculate_satisfaction_score(interactions: List[Dict[str, Any]]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—á–µ—Ç —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω–æ—Å—Ç–∏."""
    ratings = [interaction.get("rating", 0) for interaction in interactions if "rating" in interaction]
    if not ratings:
        return 0.5

    return sum(ratings) / (len(ratings) * 5)  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —à–∫–∞–ª—É 1-5

def _calculate_conversion_score(interactions: List[Dict[str, Any]]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—á–µ—Ç –∫–æ–Ω–≤–µ—Ä—Å–∏–∏."""
    conversions = len([i for i in interactions if i.get("converted", False)])
    return conversions / len(interactions) if interactions else 0.0

def _calculate_therapeutic_engagement(interactions: List[Dict[str, Any]]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫—É—é –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç—å."""
    completed_sessions = len([i for i in interactions if i.get("session_completed", False)])
    return completed_sessions / len(interactions) if interactions else 0.0

def _calculate_accuracy_perception(interactions: List[Dict[str, Any]]) -> float:
    """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏."""
    accuracy_ratings = [i.get("accuracy_rating", 0) for i in interactions if "accuracy_rating" in i]
    if not accuracy_ratings:
        return 0.5

    return sum(accuracy_ratings) / (len(accuracy_ratings) * 5)

def _analyze_user_interactions(interactions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π."""
    if not interactions:
        return {"total_interactions": 0}

    return {
        "total_interactions": len(interactions),
        "avg_duration": sum(i.get("duration", 0) for i in interactions) / len(interactions),
        "completion_rate": len([i for i in interactions if i.get("completed", False)]) / len(interactions),
        "return_rate": len([i for i in interactions if i.get("return_visit", False)]) / len(interactions)
    }

def _generate_improvement_recommendations(metric_scores: Dict[str, float], domain: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é."""
    recommendations = []

    for metric, score in metric_scores.items():
        if score < 0.6:  # –ù–∏–∑–∫–∏–π —Å—á–µ—Ç
            if metric == "engagement":
                recommendations.append("–£–ª—É—á—à–∏—Ç—å –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            elif metric == "satisfaction":
                recommendations.append("–ü–æ–≤—ã—Å–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏")
            elif metric == "conversion":
                recommendations.append("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å call-to-action —ç–ª–µ–º–µ–Ω—Ç—ã")

    return recommendations

# –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Å–ª–µ–¥—É–µ—Ç –≤ —Å–ª–µ–¥—É—é—â–µ–π —á–∞—Å—Ç–∏...

async def search_personalization_patterns(
    ctx: RunContext[PersonalizerDependencies],
    query: str,
    pattern_type: str = "general",
    domain_context: str = None
) -> str:
    """
    –ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ RAG.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        pattern_type: –¢–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞ (behavioral, content, ux, algorithmic)
        domain_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ–º–µ–Ω–∞

    Returns:
        –ù–∞–π–¥–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    try:
        if not MCP_AVAILABLE:
            return "MCP Archon –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–≥–∏ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        search_tags = ctx.deps.knowledge_tags.copy()
        search_tags.append(pattern_type)

        if domain_context:
            search_tags.append(domain_context.replace("_", "-"))

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        enhanced_query = f"{query} personalization patterns {pattern_type} {ctx.deps.domain_type}"

        result = await mcp_archon_rag_search_knowledge_base(
            query=enhanced_query,
            source_domain=ctx.deps.knowledge_domain,
            match_count=5
        )

        if result["success"] and result["results"]:
            patterns = []
            for r in result["results"]:
                pattern_info = {
                    "title": r['metadata'].get('title', 'Unknown'),
                    "content": r['content'][:500] + "..." if len(r['content']) > 500 else r['content'],
                    "relevance_score": r.get('score', 0),
                    "pattern_type": pattern_type,
                    "domain": ctx.deps.domain_type
                }
                patterns.append(pattern_info)

            return json.dumps({
                "success": True,
                "patterns": patterns,
                "query": query,
                "pattern_type": pattern_type
            }, ensure_ascii=False, indent=2)
        else:
            return json.dumps({
                "success": False,
                "message": "–ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
                "suggestion": "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –±–æ–ª–µ–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ç–∏–ø –ø–∞—Ç—Ç–µ—Ä–Ω–∞"
            }, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"

async def validate_personalization_quality(
    ctx: RunContext[PersonalizerDependencies],
    personalization_result: Dict[str, Any],
    validation_criteria: List[str] = None,
    user_feedback: Dict[str, Any] = None
) -> str:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.

    Args:
        personalization_result: –†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_criteria: –ö—Ä–∏—Ç–µ—Ä–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        user_feedback: –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        –û—Ç—á–µ—Ç –æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    """
    try:
        if validation_criteria is None:
            validation_criteria = list(ctx.deps.get_personalization_criteria().keys())

        validation = {
            "overall_quality": 0.0,
            "criteria_scores": {},
            "validation_details": {},
            "recommendations": [],
            "domain": ctx.deps.domain_type
        }

        # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –ø–æ –∫–∞–∂–¥–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é
        for criterion in validation_criteria:
            score = _validate_criterion(criterion, personalization_result, user_feedback, ctx.deps.domain_type)
            validation["criteria_scores"][criterion] = score

        # –û–±—â–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        validation["overall_quality"] = sum(validation["criteria_scores"].values()) / len(validation_criteria)

        # –î–µ—Ç–∞–ª–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation["validation_details"] = _generate_validation_details(
            personalization_result, validation["criteria_scores"]
        )

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        validation["recommendations"] = _generate_quality_recommendations(
            validation["criteria_scores"], ctx.deps.domain_type
        )

        return json.dumps(validation, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"

def _validate_criterion(criterion: str, result: Dict[str, Any], feedback: Dict[str, Any], domain: str) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è."""
    if criterion == "relevance":
        return _validate_relevance(result, feedback)
    elif criterion == "accuracy":
        return _validate_accuracy(result, feedback)
    elif criterion == "engagement":
        return _validate_engagement(result, feedback)
    elif criterion == "therapeutic_value" and domain == "psychology":
        return _validate_therapeutic_value(result, feedback)
    elif criterion == "cultural_sensitivity":
        return _validate_cultural_sensitivity(result, feedback)
    else:
        return 0.7  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤

def _validate_relevance(result: Dict[str, Any], feedback: Dict[str, Any]) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏."""
    if feedback and "relevance_rating" in feedback:
        return feedback["relevance_rating"] / 5.0

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    content_items = result.get("adapted_content", [])
    if not content_items:
        return 0.3

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    personalized_count = len([item for item in content_items if item.get("personalization_applied")])
    return personalized_count / len(content_items)

def _validate_accuracy(result: Dict[str, Any], feedback: Dict[str, Any]) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏."""
    if feedback and "accuracy_rating" in feedback:
        return feedback["accuracy_rating"] / 5.0

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
    adaptation_metadata = result.get("adaptation_metadata", {})
    confidence = adaptation_metadata.get("confidence", 0.5)

    return confidence

def _validate_engagement(result: Dict[str, Any], feedback: Dict[str, Any]) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏."""
    if feedback and "engagement_score" in feedback:
        return feedback["engagement_score"] / 5.0

    # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    content_items = result.get("adapted_content", [])
    interactive_count = len([item for item in content_items if "interactive" in str(item).lower()])

    return interactive_count / len(content_items) if content_items else 0.5

def _validate_therapeutic_value(result: Dict[str, Any], feedback: Dict[str, Any]) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–æ–π —Ü–µ–Ω–Ω–æ—Å—Ç–∏."""
    if feedback and "therapeutic_rating" in feedback:
        return feedback["therapeutic_rating"] / 5.0

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º
    content_items = result.get("adapted_content", [])
    therapeutic_indicators = ["evidence-based", "trauma-informed", "culturally-sensitive"]

    therapeutic_count = 0
    for item in content_items:
        content_text = str(item).lower()
        if any(indicator in content_text for indicator in therapeutic_indicators):
            therapeutic_count += 1

    return therapeutic_count / len(content_items) if content_items else 0.5

def _validate_cultural_sensitivity(result: Dict[str, Any], feedback: Dict[str, Any]) -> float:
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫—É–ª—å—Ç—É—Ä–Ω–æ–π —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏."""
    if feedback and "cultural_appropriateness" in feedback:
        return feedback["cultural_appropriateness"] / 5.0

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã—Ö –∞–¥–∞–ø—Ç–∞—Ü–∏–π
    adaptation_metadata = result.get("adaptation_metadata", {})
    user_context = adaptation_metadata.get("user_context", {})

    has_cultural_context = "culture" in user_context or "cultural_background" in user_context
    return 0.8 if has_cultural_context else 0.6

def _generate_validation_details(result: Dict[str, Any], scores: Dict[str, float]) -> Dict[str, Any]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª–µ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
    return {
        "content_items_count": len(result.get("adapted_content", [])),
        "personalization_applied": any(
            item.get("personalization_applied") for item in result.get("adapted_content", [])
        ),
        "lowest_scoring_criteria": min(scores.items(), key=lambda x: x[1]) if scores else None,
        "highest_scoring_criteria": max(scores.items(), key=lambda x: x[1]) if scores else None
    }

def _generate_quality_recommendations(scores: Dict[str, float], domain: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∫–∞—á–µ—Å—Ç–≤—É."""
    recommendations = []

    for criterion, score in scores.items():
        if score < 0.6:
            if criterion == "relevance":
                recommendations.append("–£–ª—É—á—à–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
            elif criterion == "accuracy":
                recommendations.append("–ü–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ª—É—á—à–∏–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª—è")
            elif criterion == "engagement":
                recommendations.append("–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            elif criterion == "cultural_sensitivity":
                recommendations.append("–£–ª—É—á—à–∏—Ç—å –∫—É–ª—å—Ç—É—Ä–Ω—É—é –∞–¥–∞–ø—Ç–∞—Ü–∏—é –∫–æ–Ω—Ç–µ–Ω—Ç–∞")

    return recommendations

async def optimize_user_experience(
    ctx: RunContext[PersonalizerDependencies],
    user_journey_data: Dict[str, Any],
    optimization_goals: List[str],
    constraints: Dict[str, Any] = None
) -> str:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞ —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—é.

    Args:
        user_journey_data: –î–∞–Ω–Ω—ã–µ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–º –ø—É—Ç–∏
        optimization_goals: –¶–µ–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        constraints: –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

    Returns:
        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ UX
    """
    try:
        if constraints is None:
            constraints = {}

        optimization = {
            "current_ux_analysis": {},
            "optimization_opportunities": [],
            "personalization_strategies": [],
            "implementation_plan": [],
            "expected_outcomes": {},
            "domain": ctx.deps.domain_type
        }

        # –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ UX
        optimization["current_ux_analysis"] = _analyze_current_ux(user_journey_data)

        # –í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        optimization["optimization_opportunities"] = _identify_optimization_opportunities(
            user_journey_data, optimization_goals, ctx.deps.domain_type
        )

        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
        optimization["personalization_strategies"] = _develop_personalization_strategies(
            optimization["optimization_opportunities"], ctx.deps.domain_type
        )

        # –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏
        optimization["implementation_plan"] = _create_implementation_plan(
            optimization["personalization_strategies"], constraints
        )

        # –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        optimization["expected_outcomes"] = _estimate_outcomes(
            optimization["personalization_strategies"], optimization_goals
        )

        return json.dumps(optimization, ensure_ascii=False, indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞: {e}"

def _analyze_current_ux(journey_data: Dict[str, Any]) -> Dict[str, Any]:
    """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞."""
    return {
        "journey_stages": len(journey_data.get("stages", [])),
        "friction_points": journey_data.get("friction_points", []),
        "completion_rate": journey_data.get("completion_rate", 0.5),
        "average_time": journey_data.get("average_time", 0),
        "user_satisfaction": journey_data.get("satisfaction_score", 0.5)
    }

def _identify_optimization_opportunities(journey_data: Dict[str, Any], goals: List[str], domain: str) -> List[Dict[str, Any]]:
    """–í—ã—è–≤–ª–µ–Ω–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
    opportunities = []

    # –û–±—â–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    if journey_data.get("completion_rate", 0) < 0.7:
        opportunities.append({
            "type": "completion_optimization",
            "description": "–£–ª—É—á—à–∏—Ç—å –∫–æ–Ω–≤–µ—Ä—Å–∏—é —á–µ—Ä–µ–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥—Å–∫–∞–∑–∫–∏",
            "priority": "high"
        })

    if "engagement" in goals:
        opportunities.append({
            "type": "engagement_enhancement",
            "description": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –∏–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            "priority": "medium"
        })

    # –î–æ–º–µ–Ω–Ω–æ-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
    if domain == "psychology":
        opportunities.append({
            "type": "therapeutic_personalization",
            "description": "–ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ñ–∏–ª—å",
            "priority": "high"
        })
    elif domain == "business":
        opportunities.append({
            "type": "roi_optimization",
            "description": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ–¥ –±–∏–∑–Ω–µ—Å-—Ü–µ–ª–∏",
            "priority": "high"
        })

    return opportunities

def _develop_personalization_strategies(opportunities: List[Dict[str, Any]], domain: str) -> List[Dict[str, Any]]:
    """–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏."""
    strategies = []

    for opportunity in opportunities:
        if opportunity["type"] == "completion_optimization":
            strategies.append({
                "name": "Adaptive Journey Optimization",
                "description": "–î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—É—Ç–∏",
                "techniques": ["behavioral_triggers", "progress_indicators", "personalized_assistance"],
                "domain_adaptation": _adapt_strategy_to_domain("completion", domain)
            })
        elif opportunity["type"] == "engagement_enhancement":
            strategies.append({
                "name": "Content Personalization Engine",
                "description": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π",
                "techniques": ["content_filtering", "recommendation_systems", "adaptive_interfaces"],
                "domain_adaptation": _adapt_strategy_to_domain("engagement", domain)
            })

    return strategies

def _adapt_strategy_to_domain(strategy_type: str, domain: str) -> Dict[str, Any]:
    """–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–¥ –¥–æ–º–µ–Ω."""
    adaptations = {
        "psychology": {
            "completion": {
                "therapeutic_alliance": "–ü–æ–¥–¥–µ—Ä–∂–∞–Ω–∏–µ —Ç–µ—Ä–∞–ø–µ–≤—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç–Ω–æ—à–µ–Ω–∏–π",
                "trauma_sensitivity": "–£—á–µ—Ç —Ç—Ä–∞–≤–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞",
                "cultural_competence": "–ö—É–ª—å—Ç—É—Ä–Ω–∞—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å"
            },
            "engagement": {
                "evidence_based": "–û—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ –Ω–∞—É—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö",
                "personalized_interventions": "–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–µ–Ω—Ü–∏–∏",
                "progress_tracking": "–û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"
            }
        },
        "business": {
            "completion": {
                "roi_focus": "–§–æ–∫—É—Å –Ω–∞ –≤–æ–∑–≤—Ä–∞—Ç–µ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–π",
                "industry_relevance": "–û—Ç—Ä–∞—Å–ª–µ–≤–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å",
                "scalability": "–ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏–π"
            },
            "engagement": {
                "data_driven": "–û—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö",
                "competitive_advantage": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞",
                "operational_efficiency": "–û–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å"
            }
        }
    }

    return adaptations.get(domain, {}).get(strategy_type, {})

def _create_implementation_plan(strategies: List[Dict[str, Any]], constraints: Dict[str, Any]) -> List[Dict[str, Any]]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏."""
    plan = []

    for i, strategy in enumerate(strategies):
        plan.append({
            "phase": i + 1,
            "strategy": strategy["name"],
            "timeline": constraints.get("timeline", "4-6 weeks"),
            "resources_required": ["development_team", "data_analyst", "ux_designer"],
            "success_metrics": ["user_satisfaction", "completion_rate", "engagement_time"],
            "risk_factors": constraints.get("risks", ["technical_complexity", "user_adoption"])
        })

    return plan

def _estimate_outcomes(strategies: List[Dict[str, Any]], goals: List[str]) -> Dict[str, Any]:
    """–û—Ü–µ–Ω–∫–∞ –æ–∂–∏–¥–∞–µ–º—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤."""
    return {
        "completion_rate_improvement": "15-25%",
        "user_satisfaction_increase": "20-30%",
        "engagement_time_increase": "10-20%",
        "personalization_accuracy": "75-85%",
        "implementation_timeline": "3-6 months",
        "roi_estimate": "200-400%" if "business_value" in goals else "High user satisfaction"
    }

# –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∫–æ–ª–ª–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã

async def break_down_to_microtasks(
    ctx: RunContext[PersonalizerDependencies],
    main_task: str,
    complexity_level: str = "medium"
) -> str:
    """
    –†–∞–∑–±–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—É—é –∑–∞–¥–∞—á—É –Ω–∞ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –∏ –≤—ã–≤–µ—Å—Ç–∏ –∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.
    """
    microtasks = []

    if complexity_level == "simple":
        microtasks = [
            f"–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è: {main_task}",
            f"–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            f"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏",
            f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        ]
    elif complexity_level == "medium":
        microtasks = [
            f"–ê–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {main_task}",
            f"–ü–æ–∏—Å–∫ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π",
            f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è UX/–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
            f"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è",
            f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            f"–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏",
            f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞"
        ]
    else:  # complex
        microtasks = [
            f"–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–¥–∞—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {main_task}",
            f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —á–µ—Ä–µ–∑ RAG –∏ –≤–µ–±-–∏—Å—Ç–æ—á–Ω–∏–∫–∏",
            f"–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–∞–≥–µ–Ω—Ç–Ω–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
            f"–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º)",
            f"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∞–≤–∏–ª –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏",
            f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            f"UX –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è (–¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ UX –∞–≥–µ–Ω—Ç—É)",
            f"–í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ Security –∞–≥–µ–Ω—Ç—É)",
            f"–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ç –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤",
            f"–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –∏ —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
        ]

    output = "üìã **–ú–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏:**\n"
    for i, task in enumerate(microtasks, 1):
        output += f"{i}. {task}\n"
    output += "\n‚úÖ –ë—É–¥—É –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∫–∞–∂–¥–æ–π –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏"

    return output

async def report_microtask_progress(
    ctx: RunContext[PersonalizerDependencies],
    microtask_number: int,
    microtask_description: str,
    status: str = "completed",
    details: str = ""
) -> str:
    """
    –û—Ç—á–∏—Ç–∞—Ç—å—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏.
    """
    status_emoji = {
        "started": "üîÑ",
        "in_progress": "‚è≥",
        "completed": "‚úÖ",
        "blocked": "üö´"
    }

    report = f"{status_emoji.get(status, 'üìù')} **–ú–∏–∫—Ä–æ–∑–∞–¥–∞—á–∞ {microtask_number}** ({status}): {microtask_description}"
    if details:
        report += f"\n   –î–µ—Ç–∞–ª–∏: {details}"

    return report

async def reflect_and_improve(
    ctx: RunContext[PersonalizerDependencies],
    completed_work: str,
    work_type: str = "personalization"
) -> str:
    """
    –í—ã–ø–æ–ª–Ω–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ —É–ª—É—á—à–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç.
    """
    analysis = f"""
üîç **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏:**

**–¢–∏–ø —Ä–∞–±–æ—Ç—ã:** {work_type}
**–î–æ–º–µ–Ω:** {ctx.deps.domain_type}
**–†–µ–∑—É–ª—å—Ç–∞—Ç:** {completed_work[:200]}...

**–ù–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
1. [–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏] - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
2. [–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫—É–ª—å—Ç—É—Ä–Ω—É—é —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å] - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ–¥ –∫—É–ª—å—Ç—É—Ä–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
3. [–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å] - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏
4. [–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é UX] - –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –æ–ø—ã—Ç–∞

**–í–Ω–µ—Å–µ–Ω–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:**
- –ü–æ–≤—ã—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ª—É—á—à–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã
- –£–ª—É—á—à–µ–Ω–∏–µ –∫—É–ª—å—Ç—É—Ä–Ω–æ–π –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
- –£—Å–∏–ª–µ–Ω–∏–µ –∑–∞—â–∏—Ç—ã –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞

**–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏:**
‚úÖ –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (> 80%)
‚úÖ –ö—É–ª—å—Ç—É—Ä–Ω–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
‚úÖ –ó–∞—â–∏—Ç–∞ –ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç–∏
‚úÖ –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–æ–º–µ–Ω–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º ({ctx.deps.domain_type})

üéØ **–§–∏–Ω–∞–ª—å–Ω–∞—è —É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é**
"""

    return analysis

async def check_delegation_need(
    ctx: RunContext[PersonalizerDependencies],
    current_task: str,
    current_agent_type: str = "personalizer"
) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω—É–∂–Ω–æ –ª–∏ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç–∏ –∑–∞–¥–∞—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥—Ä—É–≥–∏–º –∞–≥–µ–Ω—Ç–∞–º.
    """
    keywords = current_task.lower().split()

    delegation_suggestions = []

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏ –¥—Ä—É–≥–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤
    security_keywords = ['–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', 'security', '–ø—Ä–∏–≤–∞—Ç–Ω–æ—Å—Ç—å', 'privacy', 'gdpr', '–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ']
    ui_keywords = ['–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', 'ui', 'ux', '–¥–∏–∑–∞–π–Ω', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –æ–ø—ã—Ç', 'usability']
    performance_keywords = ['–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å', 'performance', '–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è', '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã']

    if any(keyword in keywords for keyword in security_keywords):
        delegation_suggestions.append("Security Audit Agent - –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    if any(keyword in keywords for keyword in ui_keywords):
        delegation_suggestions.append("UI/UX Enhancement Agent - –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞")

    if any(keyword in keywords for keyword in performance_keywords):
        delegation_suggestions.append("Performance Optimization Agent - –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏")

    if delegation_suggestions:
        result = "ü§ù **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏:**\n"
        for suggestion in delegation_suggestions:
            result += f"- {suggestion}\n"
        result += "\n–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ delegate_task_to_agent() –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á."
    else:
        result = "‚úÖ –ó–∞–¥–∞—á–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –±–µ–∑ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."

    return result

async def delegate_task_to_agent(
    ctx: RunContext[PersonalizerDependencies],
    target_agent: str,
    task_title: str,
    task_description: str,
    priority: str = "medium",
    context_data: Dict[str, Any] = None
) -> str:
    """
    –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ –¥—Ä—É–≥–æ–º—É —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É —á–µ—Ä–µ–∑ Archon.
    """
    try:
        if not MCP_AVAILABLE:
            return "‚ùå MCP Archon –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á"

        if context_data is None:
            context_data = {
                "domain_type": ctx.deps.domain_type,
                "personalization_context": "Universal Personalizer Agent"
            }

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ Archon –¥–ª—è —Ü–µ–ª–µ–≤–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        task_result = await mcp_archon_manage_task(
            action="create",
            project_id=ctx.deps.archon_project_id,
            title=task_title,
            description=f"{task_description}\n\n**–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç Universal Personalizer Agent:**\n{json.dumps(context_data, ensure_ascii=False, indent=2)}",
            assignee=AGENT_ASSIGNEE_MAP.get(target_agent, "Archon Analysis Lead"),
            status="todo",
            feature=f"–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è - –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ {target_agent}",
            task_order=50
        )

        return f"‚úÖ –ó–∞–¥–∞—á–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∞ –∞–≥–µ–Ω—Ç—É {target_agent}:\n- –ó–∞–¥–∞—á–∞: {task_title}\n- –°—Ç–∞—Ç—É—Å: —Å–æ–∑–¥–∞–Ω–∞ –≤ Archon\n- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority}"

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏: {e}"