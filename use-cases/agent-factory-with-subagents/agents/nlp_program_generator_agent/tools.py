"""
NLP Program Generator Agent Tools

–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–≥—Ä–∞–º–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
—Å NLP —Ç–µ—Ö–Ω–∏–∫–∞–º–∏ –∏ –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏.
"""

from typing import Dict, Any, List, Optional, Union
import asyncio
import json
from datetime import datetime, timedelta
import re

from pydantic_ai import RunContext
from pydantic import BaseModel, Field

from .dependencies import (
    NLPProgramGeneratorDependencies,
    SeverityLevel,
    VAKType,
    ContentFormat,
    NLPTechnique,
    EricksonianPattern,
    ProgramType
)


# === –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ===

class ProgramSpecification(BaseModel):
    """–°–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    program_id: str = Field(..., description="–£–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –ø—Ä–æ–≥—Ä–∞–º–º—ã")
    title: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
    domain: str = Field(..., description="–î–æ–º–µ–Ω –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
    severity_level: str = Field(..., description="–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏")
    vak_type: str = Field(..., description="–¢–∏–ø –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è")
    duration_days: int = Field(..., description="–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤ –¥–Ω—è—Ö")
    daily_time_minutes: int = Field(..., description="–í—Ä–µ–º—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π")
    content_format: str = Field(..., description="–§–æ—Ä–º–∞—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    target_outcomes: List[str] = Field(..., description="–¶–µ–ª–µ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")


class DailyModule(BaseModel):
    """–ï–∂–µ–¥–Ω–µ–≤–Ω—ã–π –º–æ–¥—É–ª—å –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    day_number: int = Field(..., description="–ù–æ–º–µ—Ä –¥–Ω—è")
    title: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è")
    theme: str = Field(..., description="–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ–º–∞ –¥–Ω—è")

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–µ—Å—Å–∏–∏
    introduction: str = Field(..., description="–ì–∏–ø–Ω–æ—Ç–∏—á–µ—Å–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ")
    main_technique: str = Field(..., description="–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞")
    practical_tasks: str = Field(..., description="–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è")
    reflection: str = Field(..., description="–†–µ—Ñ–ª–µ–∫—Å–∏—è")
    completion: str = Field(..., description="–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ")

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    nlp_techniques: List[str] = Field(default=[], description="–ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏")
    ericksonian_patterns: List[str] = Field(default=[], description="–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã")
    vak_elements: List[str] = Field(default=[], description="VAK —ç–ª–µ–º–µ–Ω—Ç—ã")
    estimated_time: int = Field(..., description="–ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è")


class NLPTechniqueTemplate(BaseModel):
    """–®–∞–±–ª–æ–Ω NLP —Ç–µ—Ö–Ω–∏–∫–∏."""
    technique_name: str = Field(..., description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏")
    technique_type: str = Field(..., description="–¢–∏–ø —Ç–µ—Ö–Ω–∏–∫–∏")
    description: str = Field(..., description="–û–ø–∏—Å–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏")
    instructions: str = Field(..., description="–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è")
    vak_adaptations: Dict[str, str] = Field(..., description="–ê–¥–∞–ø—Ç–∞—Ü–∏–∏ –ø–æ–¥ VAK")
    contraindications: List[str] = Field(default=[], description="–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è")


# === –û–°–ù–û–í–ù–´–ï –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ ===

async def search_program_knowledge(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    query: str,
    match_count: int = 5
) -> str:
    """
    –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∞–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        match_count: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

    Returns:
        –ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–≥–∏ –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
        search_query = f"{query} {' '.join(ctx.deps.knowledge_tags[:5])}"

        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ RAG
        # result = await mcp__archon__rag_search_knowledge_base(
        #     query=search_query,
        #     match_count=match_count
        # )

        # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        knowledge_base = f"""
–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ (–ø–æ–∏—Å–∫: {query}):

**–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è PatternShift 2.0:**
- –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: –ö—Ä–∏–∑–∏—Å (21 –¥–µ–Ω—å) ‚Üí –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è (21 –¥–µ–Ω—å) ‚Üí –†–∞–∑–≤–∏—Ç–∏–µ (14 –¥–Ω–µ–π)
- –ú—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: —Ç–µ–∫—Å—Ç + –∞—É–¥–∏–æ —Å —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–µ–π
- VAK –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∫–∞–Ω–∞–ª –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è

**NLP –¢–µ—Ö–Ω–∏–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º:**
- –ê–Ω–∫–æ—Ä–∏–Ω–≥: —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö —Å–æ—Å—Ç–æ—è–Ω–∏–π —á–µ—Ä–µ–∑ —è–∫–æ—Ä—è
- –°—É–±–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏: –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫ –æ–±—Ä–∞–∑–æ–≤
- –†–µ—Ñ—Ä–µ–π–º–∏–Ω–≥: –ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π
- –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∏–Ω–∏–∏: —Ä–∞–±–æ—Ç–∞ —Å –ø—Ä–æ—à–ª—ã–º –∏ –±—É–¥—É—â–∏–º

**–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:**
- –¢—Ä—é–∏–∑–º—ã: "–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ..."
- –ü—Ä–µ—Å—É–ø–ø–æ–∑–∏—Ü–∏–∏: "–ö–æ–≥–¥–∞ —Ç—ã –ø–æ—á—É–≤—Å—Ç–≤—É–µ—à—å –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å..."
- –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã: "–º–æ–∂–µ—à—å –†–ê–°–°–õ–ê–ë–ò–¢–¨–°–Ø –ü–û–õ–ù–û–°–¢–¨–Æ"
"""
        return knowledge_base

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}"


async def generate_program_specification(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    test_results: Dict[str, Any],
    user_preferences: Dict[str, Any] = None
) -> str:
    """
    –°–æ–∑–¥–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—é –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.

    Args:
        test_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        user_preferences: –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

    Returns:
        JSON —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã
    """
    try:
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        severity = test_results.get("severity_level", ctx.deps.severity_level.value)
        vak_type = test_results.get("vak_type", ctx.deps.vak_type.value)
        problem_areas = test_results.get("problem_areas", [])

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        duration = ctx.deps.get_program_duration()
        session_time = ctx.deps.get_daily_session_time()

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
        spec = ProgramSpecification(
            program_id=f"nlp_prog_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            title=f"–ü—Ä–æ–≥—Ä–∞–º–º–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ - {severity.upper()}",
            domain=ctx.deps.domain.value,
            severity_level=severity,
            vak_type=vak_type,
            duration_days=duration,
            daily_time_minutes=session_time["total_minutes"],
            content_format=ctx.deps.content_format.value,
            target_outcomes=_generate_target_outcomes(problem_areas, severity)
        )

        return spec.model_dump_json(indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}"


async def create_daily_program_module(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    day_number: int,
    program_theme: str,
    vak_type: str,
    severity_level: str
) -> str:
    """
    –°–æ–∑–¥–∞—Ç—å –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –º–æ–¥—É–ª—å –ø—Ä–æ–≥—Ä–∞–º–º—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.

    Args:
        day_number: –ù–æ–º–µ—Ä –¥–Ω—è –≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ
        program_theme: –¢–µ–º–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã
        vak_type: –¢–∏–ø –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è (visual/auditory/kinesthetic)
        severity_level: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏

    Returns:
        JSON –æ–ø–∏—Å–∞–Ω–∏–µ –º–æ–¥—É–ª—è –¥–Ω—è
    """
    try:
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        knowledge = await search_program_knowledge(
            ctx, f"{program_theme} –¥–µ–Ω—å {day_number} {vak_type}"
        )

        # –í—ã–±–æ—Ä —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –¥–Ω—è
        techniques = _select_techniques_for_day(day_number, severity_level, vak_type)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–Ω—è
        module = DailyModule(
            day_number=day_number,
            title=f"–î–µ–Ω—å {day_number}: {_get_daily_theme(day_number, program_theme)}",
            theme=program_theme,
            introduction=_generate_hypnotic_introduction(vak_type, program_theme),
            main_technique=_generate_main_technique(techniques["primary"], vak_type),
            practical_tasks=_generate_practical_tasks(techniques["practical"], vak_type),
            reflection=_generate_reflection_questions(program_theme, day_number),
            completion=_generate_hypnotic_completion(vak_type),
            nlp_techniques=techniques["nlp"],
            ericksonian_patterns=techniques["ericksonian"],
            vak_elements=_get_vak_elements(vak_type),
            estimated_time=ctx.deps.get_daily_session_time()["total_minutes"]
        )

        return module.model_dump_json(indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥—É–ª—è –¥–Ω—è: {e}"


async def adapt_content_for_vak(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    content: str,
    target_vak: str
) -> str:
    """
    –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π VAK —Ç–∏–ø.

    Args:
        content: –ò—Å—Ö–æ–¥–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        target_vak: –¶–µ–ª–µ–≤–æ–π VAK —Ç–∏–ø (visual/auditory/kinesthetic)

    Returns:
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    """
    try:
        vak_adaptations = {
            "visual": {
                "intro_words": ["–ø—Ä–µ–¥—Å—Ç–∞–≤—å", "—É–≤–∏–¥—å", "–≤–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–π", "–ø–æ—Å–º–æ—Ç—Ä–∏"],
                "descriptors": ["—è—Ä–∫–∏–π", "—á–µ—Ç–∫–∏–π", "—Ü–≤–µ—Ç–Ω–æ–π", "–¥–µ—Ç–∞–ª—å–Ω—ã–π"],
                "metaphors": ["–∫–∞—Ä—Ç–∏–Ω–∞", "–æ–±—Ä–∞–∑", "—Å–≤–µ—Ç", "—Ü–≤–µ—Ç", "—Ñ–æ—Ä–º–∞"],
                "instructions": "–°–æ–∑–¥–∞–≤–∞–π —è—Ä–∫–∏–µ, –¥–µ—Ç–∞–ª—å–Ω—ã–µ –æ–±—Ä–∞–∑—ã –≤ —Å–≤–æ–µ–º –≤–æ–æ–±—Ä–∞–∂–µ–Ω–∏–∏..."
            },
            "auditory": {
                "intro_words": ["—É—Å–ª—ã—à—å", "–ø—Ä–∏—Å–ª—É—à–∞–π—Å—è", "—Å–∫–∞–∂–∏ —Å–µ–±–µ", "–ø—Ä–æ–∏–∑–Ω–µ—Å–∏"],
                "descriptors": ["–∑–≤—É—á–∞—â–∏–π", "–º–µ–ª–æ–¥–∏—á–Ω—ã–π", "—Ä–∏—Ç–º–∏—á–Ω—ã–π", "–≥–∞—Ä–º–æ–Ω–∏—á–Ω—ã–π"],
                "metaphors": ["–º–µ–ª–æ–¥–∏—è", "–∑–≤—É–∫", "–≥–æ–ª–æ—Å", "—ç—Ö–æ", "—Ç–æ–Ω"],
                "instructions": "–ù–∞—Å—Ç—Ä–æ–π—Å—è –Ω–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∑–≤—É–∫–∏ –∏ –≥–æ–ª–æ—Å–∞..."
            },
            "kinesthetic": {
                "intro_words": ["–ø–æ—á—É–≤—Å—Ç–≤—É–π", "–æ—â—É—Ç–∏", "–ø—Ä–∏–∫–æ—Å–Ω–∏—Å—å", "–≤–¥–æ—Ö–Ω–∏"],
                "descriptors": ["—Ç–µ–ø–ª—ã–π", "–º—è–≥–∫–∏–π", "—Å–∏–ª—å–Ω—ã–π", "–≥–ª—É–±–æ–∫–∏–π"],
                "metaphors": ["–ø–æ—Ç–æ–∫", "–≤–æ–ª–Ω–∞", "–¥—ã—Ö–∞–Ω–∏–µ", "–¥–≤–∏–∂–µ–Ω–∏–µ", "–ø—Ä–∏–∫–æ—Å–Ω–æ–≤–µ–Ω–∏–µ"],
                "instructions": "–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞ —Ç–µ–ª–µ—Å–Ω—ã—Ö –æ—â—É—â–µ–Ω–∏—è—Ö –∏ –¥–≤–∏–∂–µ–Ω–∏–∏ —ç–Ω–µ—Ä–≥–∏–∏..."
            }
        }

        adaptation = vak_adaptations.get(target_vak, vak_adaptations["kinesthetic"])

        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        adapted_content = content

        # –ó–∞–º–µ–Ω–∞ –æ–±—â–∏—Ö —Å–ª–æ–≤ –Ω–∞ VAK-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ
        for word in adaptation["intro_words"]:
            adapted_content = re.sub(
                r'\b(–æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ|—Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è|–Ω–∞–ø—Ä–∞–≤—å –≤–Ω–∏–º–∞–Ω–∏–µ)\b',
                word,
                adapted_content,
                flags=re.IGNORECASE
            )

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ VAK-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        if "[VAK_INSTRUCTIONS]" in adapted_content:
            adapted_content = adapted_content.replace(
                "[VAK_INSTRUCTIONS]",
                adaptation["instructions"]
            )

        return f"**–ê–¥–∞–ø—Ç–∞—Ü–∏—è –¥–ª—è {target_vak.upper()}:**\n\n{adapted_content}"

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {e}"


async def generate_nlp_technique(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    technique_name: str,
    application_context: str,
    vak_type: str = "mixed"
) -> str:
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è NLP —Ç–µ—Ö–Ω–∏–∫–∏.

    Args:
        technique_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ (anchoring, reframing, etc.)
        application_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        vak_type: –¢–∏–ø –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏

    Returns:
        JSON –æ–ø–∏—Å–∞–Ω–∏–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏
    """
    try:
        # –®–∞–±–ª–æ–Ω—ã NLP —Ç–µ—Ö–Ω–∏–∫
        technique_templates = {
            "anchoring": {
                "description": "–¢–µ—Ö–Ω–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —É—Å–ª–æ–≤–Ω–æ-—Ä–µ—Ñ–ª–µ–∫—Ç–æ—Ä–Ω—ã—Ö —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Å—Ç–∏–º—É–ª–æ–º –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ–º",
                "base_instructions": """
                1. –í–æ–π–¥–∏ –≤ –∂–µ–ª–∞–µ–º–æ–µ —Ä–µ—Å—É—Ä—Å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                2. –ù–∞ –ø–∏–∫–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø—Ä–∏–º–µ–Ω–∏ —è–∫–æ—Ä—å (–∂–µ—Å—Ç/—Å–ª–æ–≤–æ/–æ–±—Ä–∞–∑)
                3. –ü–æ–≤—Ç–æ—Ä–∏—Ç—å —Å–≤—è–∑–∫—É 3-5 —Ä–∞–∑
                4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å —è–∫–æ—Ä—å –≤ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
                """,
                "vak_adaptations": {
                    "visual": "–ò—Å–ø–æ–ª—å–∑—É–π —è—Ä–∫–∏–π –≤–∏–∑—É–∞–ª—å–Ω—ã–π –æ–±—Ä–∞–∑ –∫–∞–∫ —è–∫–æ—Ä—å",
                    "auditory": "–í—ã–±–µ—Ä–∏ –º–µ–ª–æ–¥–∏—é –∏–ª–∏ —Ñ—Ä–∞–∑—É –∫–∞–∫ —è–∫–æ—Ä—å",
                    "kinesthetic": "–°–æ–∑–¥–∞–π —Ç–∞–∫—Ç–∏–ª—å–Ω—ã–π –∂–µ—Å—Ç-—è–∫–æ—Ä—å"
                }
            },
            "reframing": {
                "description": "–¢–µ—Ö–Ω–∏–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Å–º–µ–Ω—É —Ä–∞–º–∫–∏ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏",
                "base_instructions": """
                1. –û–ø—Ä–µ–¥–µ–ª–∏ –ø—Ä–æ–±–ª–µ–º–Ω—É—é —Å–∏—Ç—É–∞—Ü–∏—é
                2. –ù–∞–π–¥–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏
                3. –í—ã–±–µ—Ä–∏ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ—Å—É—Ä—Å–Ω—É—é —Ä–∞–º–∫—É
                4. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π –Ω–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
                """,
                "vak_adaptations": {
                    "visual": "–ü—Ä–µ–¥—Å—Ç–∞–≤—å —Å–∏—Ç—É–∞—Ü–∏—é –∫–∞–∫ –∫–∞—Ä—Ç–∏–Ω—É –≤ —Ä–∞–∑–Ω—ã—Ö —Ä–∞–º–∫–∞—Ö",
                    "auditory": "–†–∞—Å—Å–∫–∞–∂–∏ –∏—Å—Ç–æ—Ä–∏—é —Å —Ä–∞–∑–Ω—ã—Ö —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è",
                    "kinesthetic": "–ü–æ—á—É–≤—Å—Ç–≤—É–π –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Å–∏—Ç—É–∞—Ü–∏–∏"
                }
            }
        }

        template = technique_templates.get(technique_name.lower(), {
            "description": f"–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è NLP —Ç–µ—Ö–Ω–∏–∫–∞: {technique_name}",
            "base_instructions": "–°–ª–µ–¥—É–π—Ç–µ –±–∞–∑–æ–≤—ã–º –ø—Ä–∏–Ω—Ü–∏–ø–∞–º NLP",
            "vak_adaptations": {
                "visual": "–í–∏–∑—É–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫–∏",
                "auditory": "–ê—É–¥–∏–∞–ª—å–Ω–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫–∏",
                "kinesthetic": "–ö–∏–Ω–µ—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –∞–¥–∞–ø—Ç–∞—Ü–∏—è —Ç–µ—Ö–Ω–∏–∫–∏"
            }
        })

        technique = NLPTechniqueTemplate(
            technique_name=technique_name,
            technique_type="NLP",
            description=template["description"],
            instructions=template["base_instructions"],
            vak_adaptations=template["vak_adaptations"],
            contraindications=_get_technique_contraindications(technique_name)
        )

        return technique.model_dump_json(indent=2)

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ NLP —Ç–µ—Ö–Ω–∏–∫–∏: {e}"


# === –ö–û–õ–õ–ï–ö–¢–ò–í–ù–ê–Ø –†–ê–ë–û–¢–ê ===

async def break_down_program_creation(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    program_requirements: str,
    complexity_level: str = "medium"
) -> str:
    """
    –†–∞–∑–±–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º—ã –Ω–∞ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏.

    Args:
        program_requirements: –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –ø—Ä–æ–≥—Ä–∞–º–º–µ
        complexity_level: –£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ (simple/medium/complex)

    Returns:
        –°–ø–∏—Å–æ–∫ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á
    """
    microtasks = []

    if complexity_level == "simple":
        microtasks = [
            f"–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {program_requirements}",
            "–°–æ–∑–¥–∞–Ω–∏–µ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
            "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è 3-7 –∫–ª—é—á–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π",
            "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ VAK —Ç–∏–ø",
            "–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Ñ–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è"
        ]
    elif complexity_level == "medium":
        microtasks = [
            f"–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {program_requirements}",
            "–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ NLP —Ç–µ—Ö–Ω–∏–∫–∞–º",
            "–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
            "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –µ–∂–µ–¥–Ω–µ–≤–Ω—ã—Ö –º–æ–¥—É–ª–µ–π",
            "–°–æ–∑–¥–∞–Ω–∏–µ NLP —Ç–µ—Ö–Ω–∏–∫ –ø–æ–¥ –∑–∞–¥–∞—á–∏",
            "–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –ø–æ–¥ VAK",
            "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
            "–í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ç–∏–∫–∏",
            "–§–∏–Ω–∞–ª—å–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"
        ]
    else:  # complex
        microtasks = [
            f"–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π: {program_requirements}",
            "–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω—ã—Ö –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–π —Ç–µ—Ö–Ω–∏–∫",
            "–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ–∂–º–æ–¥—É–ª—å–Ω—ã—Ö —Å–≤—è–∑–µ–π",
            "–°–æ–∑–¥–∞–Ω–∏–µ –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã",
            "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã",
            "–†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö NLP —Ç–µ—Ö–Ω–∏–∫",
            "–°–æ–∑–¥–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞",
            "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ –∞–≥–µ–Ω—Ç–∞–º–∏ —Å–∏—Å—Ç–µ–º—ã",
            "–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞",
            "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"
        ]

    output = "üìã **–ú–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã:**\n"
    for i, task in enumerate(microtasks, 1):
        output += f"{i}. {task}\n"
    output += "\n‚úÖ –ë—É–¥—É –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∫–∞–∂–¥–æ–π –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏"

    return output


async def delegate_program_task(
    ctx: RunContext[NLPProgramGeneratorDependencies],
    target_agent: str,
    task_title: str,
    task_description: str,
    priority: str = "medium"
) -> str:
    """
    –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –¥—Ä—É–≥–æ–º—É –∞–≥–µ–Ω—Ç—É —á–µ—Ä–µ–∑ Archon.

    Args:
        target_agent: –¶–µ–ª–µ–≤–æ–π –∞–≥–µ–Ω—Ç
        task_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        priority: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    try:
        agent_mapping = {
            "research": "NLP Content Research Agent",
            "quality": "NLP Content Quality Guardian Agent",
            "validation": "Quality Guardian",
            "content": "Content Architect"
        }

        assignee = agent_mapping.get(target_agent, "Implementation Engineer")

        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –≤—ã–∑–æ–≤ Archon API
        # task_result = await mcp__archon__manage_task(...)

        return f"‚úÖ –ó–∞–¥–∞—á–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∞ –∞–≥–µ–Ω—Ç—É {assignee}:\n- –ù–∞–∑–≤–∞–Ω–∏–µ: {task_title}\n- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority}\n- –°—Ç–∞—Ç—É—Å: —Å–æ–∑–¥–∞–Ω–∞ –≤ —Å–∏—Å—Ç–µ–º–µ"

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"


# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===

def _generate_target_outcomes(problem_areas: List[str], severity: str) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ü–µ–ª–µ–≤—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    base_outcomes = [
        "–£–ª—É—á—à–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —Ä–µ–≥—É–ª—è—Ü–∏–∏",
        "–ü–æ–≤—ã—à–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ —Å–µ–±–µ",
        "–†–∞–∑–≤–∏—Ç–∏–µ –Ω–∞–≤—ã–∫–æ–≤ —Å–∞–º–æ–ø–æ–º–æ—â–∏",
        "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–¥–æ—Ä–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –º—ã—à–ª–µ–Ω–∏—è"
    ]

    if severity == "severe":
        base_outcomes.extend([
            "–°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–∑–∏—Å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è",
            "–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"
        ])
    elif severity == "mild":
        base_outcomes.extend([
            "–†–∞—Å–∫—Ä—ã—Ç–∏–µ –ª–∏—á–Ω–æ—Å—Ç–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞",
            "–î–æ—Å—Ç–∏–∂–µ–Ω–∏–µ –ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ü–µ–ª–µ–π"
        ])

    return base_outcomes


def _select_techniques_for_day(day_number: int, severity: str, vak_type: str) -> Dict[str, List[str]]:
    """–í—ã–±–æ—Ä —Ç–µ—Ö–Ω–∏–∫ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–Ω—è."""
    return {
        "primary": f"–î–µ–Ω—å {day_number}: –û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ –¥–ª—è {severity}",
        "practical": f"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è –ø–æ–¥ {vak_type}",
        "nlp": ["anchoring", "reframing"],
        "ericksonian": ["truisms", "presuppositions"]
    }


def _get_daily_theme(day_number: int, program_theme: str) -> str:
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–º—É –¥–Ω—è."""
    themes = [
        "–ó–Ω–∞–∫–æ–º—Å—Ç–≤–æ —Å –ø—Ä–æ–≥—Ä–∞–º–º–æ–π",
        "–°–æ–∑–¥–∞–Ω–∏–µ –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞",
        "–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è",
        "–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –æ—Ç –±–ª–æ–∫–æ–≤",
        "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"
    ]
    return themes[min(day_number - 1, len(themes) - 1)]


def _generate_hypnotic_introduction(vak_type: str, theme: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–Ω–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ –≤–≤–µ–¥–µ–Ω–∏—è."""
    intros = {
        "visual": f"–ü—Ä–µ–¥—Å—Ç–∞–≤—å –ø–µ—Ä–µ–¥ —Å–æ–±–æ–π –¥–≤–µ—Ä—å –≤ –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏... {theme}",
        "auditory": f"–£—Å–ª—ã—à—å –≥–æ–ª–æ—Å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –º—É–¥—Ä–æ—Å—Ç–∏, –≥–æ–≤–æ—Ä—è—â–∏–π –æ... {theme}",
        "kinesthetic": f"–ü–æ—á—É–≤—Å—Ç–≤—É–π –≤–æ–ª–Ω—É —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Å–≤—è–∑–∞–Ω–Ω—É—é —Å... {theme}"
    }
    return intros.get(vak_type, intros["kinesthetic"])


def _generate_main_technique(technique: str, vak_type: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏ –¥–Ω—è."""
    return f"–û—Å–Ω–æ–≤–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ ({technique}) –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –¥–ª—è {vak_type} —Ç–∏–ø–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è"


def _generate_practical_tasks(tasks: str, vak_type: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞–Ω–∏–π."""
    return f"–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞–Ω–∏—è: {tasks} (–∞–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–¥ {vak_type})"


def _generate_reflection_questions(theme: str, day: int) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏."""
    return f"–†–µ—Ñ–ª–µ–∫—Å–∏—è –¥–Ω—è {day}: –ö–∞–∫ –∏–∑–º–µ–Ω–∏–ª–æ—Å—å —Ç–≤–æ–µ –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–º–µ '{theme}'?"


def _generate_hypnotic_completion(vak_type: str) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–Ω–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è."""
    completions = {
        "visual": "–°–æ—Ö—Ä–∞–Ω–∏ —è—Ä–∫–∏–π –æ–±—Ä–∞–∑ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π...",
        "auditory": "–£—Å–ª—ã—à—å –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞...",
        "kinesthetic": "–û—â—É—Ç–∏ –≥–ª—É–±–æ–∫—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –Ω–æ–≤–æ–≥–æ –æ–ø—ã—Ç–∞..."
    }
    return completions.get(vak_type, completions["kinesthetic"])


def _get_vak_elements(vak_type: str) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è VAK —Ç–∏–ø–∞."""
    elements = {
        "visual": ["–æ–±—Ä–∞–∑—ã", "—Ü–≤–µ—Ç–∞", "—Ñ–æ—Ä–º—ã", "—Å–≤–µ—Ç"],
        "auditory": ["–∑–≤—É–∫–∏", "–º–µ–ª–æ–¥–∏–∏", "–≥–æ–ª–æ—Å–∞", "—Ä–∏—Ç–º"],
        "kinesthetic": ["–æ—â—É—â–µ–Ω–∏—è", "–¥–≤–∏–∂–µ–Ω–∏—è", "–¥—ã—Ö–∞–Ω–∏–µ", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞"]
    }
    return elements.get(vak_type, elements["kinesthetic"])


def _get_technique_contraindications(technique_name: str) -> List[str]:
    """–ü–æ–ª—É—á–∏—Ç—å –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ—Ö–Ω–∏–∫–∏."""
    contraindications = {
        "anchoring": ["–æ—Å—Ç—Ä—ã–µ –ø—Å–∏—Ö–æ—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"],
        "timeline": ["–ü–¢–°–† –±–µ–∑ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏"],
        "reframing": ["–±—Ä–µ–¥–æ–≤—ã–µ —Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"]
    }
    return contraindications.get(technique_name.lower(), [])