"""
NLP Content Quality Guardian Agent Tools

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞ NLP –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –ø—Ä–æ–≥—Ä–∞–º–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏.
"""

from typing import Dict, Any, List, Optional
import asyncio
import json
import re
from datetime import datetime

from pydantic_ai import RunContext

from .dependencies import (
    NLPQualityGuardianDependencies,
    ValidationResult, ValidationAspect, QualityLevel,
    CriticalFlag, ContentType, ValidationDomain
)


async def search_quality_knowledge(
    ctx: RunContext[NLPQualityGuardianDependencies],
    query: str,
    validation_aspect: str = "general"
) -> str:
    """
    –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –∞–≥–µ–Ω—Ç–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞.

    Args:
        query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
        validation_aspect: –ê—Å–ø–µ–∫—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –ø–æ–∏—Å–∫–∞

    Returns:
        –ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å —É—á–µ—Ç–æ–º –∞—Å–ø–µ–∫—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        specific_query = f"{query} {validation_aspect} quality validation"

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MCP Archon –¥–ª—è –ø–æ–∏—Å–∫–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        try:
            from mcp__archon__rag_search_knowledge_base import mcp__archon__rag_search_knowledge_base

            result = await mcp__archon__rag_search_knowledge_base(
                query=specific_query,
                source_domain=ctx.deps.knowledge_domain,
                match_count=5
            )

            if result["success"] and result["results"]:
                knowledge = "\n".join([
                    f"**{r['metadata']['title']}:**\n{r['content']}"
                    for r in result["results"]
                ])
                return f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\n{knowledge}"

        except ImportError:
            pass

        # Fallback –ø–æ–∏—Å–∫ –ø–æ —Ç–µ–≥–∞–º –∞–≥–µ–Ω—Ç–∞
        try:
            result = await mcp__archon__rag_search_knowledge_base(
                query=f"nlp quality guardian {validation_aspect}",
                match_count=3
            )

            if result["success"] and result["results"]:
                knowledge = "\n".join([
                    f"**{r['metadata']['title']}:**\n{r['content']}"
                    for r in result["results"]
                ])
                return f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π (–Ω–∞–π–¥–µ–Ω–æ —á–µ—Ä–µ–∑ fallback):\n{knowledge}"

        except Exception:
            pass

        # –ï—Å–ª–∏ –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è
        return f"""
‚ö†Ô∏è **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π NLP Quality Guardian –¥–æ—Å—Ç—É–ø–Ω–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ**

üîç **–ò—Å–∫–∞–ª–∏:** {query} (–∞—Å–ø–µ–∫—Ç: {validation_aspect})

üí° **–ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è {validation_aspect}:**

üìã **–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏:**
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ PatternShift 2.0
- –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞: –ö—Ä–∏–∑–∏—Å ‚Üí –°—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è ‚Üí –†–∞–∑–≤–∏—Ç–∏–µ
- VAK –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è (Visual, Auditory, Kinesthetic)
- –ú—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç (—Ç–µ–∫—Å—Ç + –∞—É–¥–∏–æ)

üß† **–ü—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏:**
- –ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏–∫
- –≠—Ç–∏—á–µ—Å–∫–∞—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- –í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å
- –ö—É–ª—å—Ç—É—Ä–Ω–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

üéØ **NLP –∫—Ä–∏—Ç–µ—Ä–∏–∏:**
- –ü—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
- –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏—è
- –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç–µ—Ö–Ω–∏–∫
- –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –ù–õ–ü

üîê **–ö—Ä–∏—Ç–µ—Ä–∏–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:**
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã—Ö —Ç–µ—Ö–Ω–∏–∫
- –ò–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ
- –£–≤–∞–∂–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
- –ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è

–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –ø–æ–ª–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –≤ Archon –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞.
"""

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π: {e}"


async def validate_content_structure(
    ctx: RunContext[NLPQualityGuardianDependencies],
    content: str,
    content_type: str = "mixed"
) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º.

    Args:
        content: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        content_type: –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (test, program, technique)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏
    """
    try:
        content_length = len(content)

        # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        structure_issues = []
        structure_score = 100.0

        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–æ–≤
        if content_type == "test":
            # –ü–æ–¥—Å—á–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–≤
            questions_pattern = r'(?:^\d+\.|–í–æ–ø—Ä–æ—Å\s+\d+|Question\s+\d+)'
            questions = re.findall(questions_pattern, content, re.MULTILINE | re.IGNORECASE)
            question_count = len(questions)

            if question_count < ctx.deps.validation_criteria.min_test_questions:
                structure_issues.append(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {question_count}, "
                    f"–º–∏–Ω–∏–º—É–º {ctx.deps.validation_criteria.min_test_questions}"
                )
                structure_score -= 30

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
            clinical_terms = [
                '–¥–µ–ø—Ä–µ—Å—Å–∏—è', '—Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å', '—Ñ–æ–±–∏—è', '—Ä–∞—Å—Å—Ç—Ä–æ–π—Å—Ç–≤–æ',
                '—Å–∏–Ω–¥—Ä–æ–º', '–¥–∏–∞–≥–Ω–æ–∑', '—Å–∏–º–ø—Ç–æ–º', '–ø–∞—Ç–æ–ª–æ–≥–∏—è'
            ]
            clinical_found = [term for term in clinical_terms if term.lower() in content.lower()]

            if clinical_found and ctx.deps.validation_criteria.avoid_clinical_terms:
                structure_issues.append(
                    f"–ù–∞–π–¥–µ–Ω—ã –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã: {', '.join(clinical_found)}"
                )
                structure_score -= 20

        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
        elif content_type == "program":
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            level_keywords = ['–∫—Ä–∏–∑–∏—Å', '—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è', '—Ä–∞–∑–≤–∏—Ç–∏–µ', 'crisis', 'stabilization', 'development']
            found_levels = [kw for kw in level_keywords if kw.lower() in content.lower()]

            if len(found_levels) < 3 and ctx.deps.validation_criteria.require_three_level_structure:
                structure_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
                structure_score -= 25

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ VAK –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
            vak_keywords = ['–≤–∏–∑—É–∞–ª—å–Ω—ã–π', '–∞—É–¥–∏–∞–ª—å–Ω—ã–π', '–∫–∏–Ω–µ—Å—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π', 'visual', 'auditory', 'kinesthetic']
            found_vak = [kw for kw in vak_keywords if kw.lower() in content.lower()]

            if len(found_vak) < 2 and ctx.deps.validation_criteria.require_vak_personalization:
                structure_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç VAK –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏—è")
                structure_score -= 20

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω–æ—Å—Ç–∏
            format_keywords = ['–∞—É–¥–∏–æ', '—Ç–µ–∫—Å—Ç', '—É–ø—Ä–∞–∂–Ω–µ–Ω–∏–µ', 'audio', 'text', 'exercise']
            found_formats = [kw for kw in format_keywords if kw.lower() in content.lower()]

            if len(found_formats) < 2 and ctx.deps.validation_criteria.require_multiformat_content:
                structure_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –º—É–ª—å—Ç–∏—Ñ–æ—Ä–º–∞—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç")
                structure_score -= 15

        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è NLP —Ç–µ—Ö–Ω–∏–∫
        elif content_type == "technique":
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–µ—Ö–Ω–∏–∫–∏
            technique_elements = [
                '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è', '—à–∞–≥–∏', '–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ', '–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è',
                'instruction', 'steps', 'application', 'contraindications'
            ]
            found_elements = [elem for elem in technique_elements if elem.lower() in content.lower()]

            if len(found_elements) < 2:
                structure_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –±–∞–∑–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Ç–µ—Ö–Ω–∏–∫–∏")
                structure_score -= 30

        # –û–±—â–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        if content_length < 500:
            structure_issues.append("–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç")
            structure_score -= 10
        elif content_length > ctx.deps.max_content_length:
            structure_issues.append(f"–°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç: {content_length} —Å–∏–º–≤–æ–ª–æ–≤")
            structure_score -= 5

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        headers = re.findall(r'^#+\s+(.+)', content, re.MULTILINE)
        if len(headers) < 2:
            structure_issues.append("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ (–∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)")
            structure_score -= 10

        return {
            "structure_score": max(0, structure_score),
            "issues": structure_issues,
            "question_count": question_count if content_type == "test" else None,
            "content_length": content_length,
            "headers_found": len(headers),
            "analysis_timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        return {
            "structure_score": 0,
            "issues": [f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã: {e}"],
            "error": str(e)
        }


async def check_safety_and_ethics(
    ctx: RunContext[NLPQualityGuardianDependencies],
    content: str,
    target_audience: str = "adults"
) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ç–∏—á–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        content: –ö–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        target_audience: –¶–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è (adults, teens, children)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
    """
    try:
        safety_score = 100.0
        critical_flags = []
        safety_issues = []
        warnings = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
        harmful_patterns = [
            '–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ', '–¥–∞–≤–ª–µ–Ω–∏–µ', '–∑–∞—Å—Ç–∞–≤–∏—Ç—å', '–¥–æ–ª–∂–µ–Ω –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ',
            '–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç', '—Å—Ç–æ–ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏—Å—Ü–µ–ª–µ–Ω–∏–µ',
            '–∑–∞–º–µ–Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –ª–µ—á–µ–Ω–∏—è', '–æ—Ç–∫–∞–∑ –æ—Ç –≤—Ä–∞—á–∞'
        ]

        for pattern in harmful_patterns:
            if pattern.lower() in content.lower():
                critical_flags.append(CriticalFlag.POTENTIALLY_HARMFUL)
                safety_issues.append(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –≤—Ä–µ–¥–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞: '{pattern}'")
                safety_score -= 20
                break

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏
        manipulative_patterns = [
            '—Å–∫—Ä—ã—Ç–æ–µ –≤–ª–∏—è–Ω–∏–µ', '–ø–æ–¥—Å–æ–∑–Ω–∞—Ç–µ–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ –±–µ–∑ —Å–æ–≥–ª–∞—Å–∏—è',
            '–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ –∫ –ø–æ–∫—É–ø–∫–µ', '—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –º–∞–Ω–∏–ø—É–ª—è—Ü–∏—è',
            '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç —Ç–µ—Ö–Ω–∏–∫–∏'
        ]

        for pattern in manipulative_patterns:
            if pattern.lower() in content.lower():
                critical_flags.append(CriticalFlag.MANIPULATIVE_CONTENT)
                safety_issues.append(f"–ú–∞–Ω–∏–ø—É–ª—è—Ç–∏–≤–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç: '{pattern}'")
                safety_score -= 25
                break

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤
        ethical_violations = []

        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è
        consent_keywords = ['—Å–æ–≥–ª–∞—Å–∏–µ', '–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ', '–ø–æ –∂–µ–ª–∞–Ω–∏—é', 'consent', 'voluntary']
        if not any(kw.lower() in content.lower() for kw in consent_keywords):
            ethical_violations.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø –∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ–≥–ª–∞—Å–∏—è")

        # –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≥—Ä–∞–Ω–∏—Ü –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        boundary_keywords = ['–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è', '–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è', '–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è', '–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ']
        if not any(kw.lower() in content.lower() for kw in boundary_keywords):
            ethical_violations.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è")

        if ethical_violations:
            critical_flags.append(CriticalFlag.UNETHICAL_PRACTICE)
            safety_issues.extend(ethical_violations)
            safety_score -= 15 * len(ethical_violations)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã—Ö —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π
        pseudoscience_patterns = [
            '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è', '–∫–≤–∞–Ω—Ç–æ–≤–æ–µ –∏—Å—Ü–µ–ª–µ–Ω–∏–µ', '–∫–æ—Å–º–∏—á–µ—Å–∫–∏–µ –≤–∏–±—Ä–∞—Ü–∏–∏',
            '–º–≥–Ω–æ–≤–µ–Ω–Ω–æ–µ –∏—Å—Ü–µ–ª–µ–Ω–∏–µ', '—á—É–¥–µ—Å–Ω–æ–µ –≤—ã–∑–¥–æ—Ä–æ–≤–ª–µ–Ω–∏–µ', '–º–∞–≥–∏—á–µ—Å–∫–∞—è —Å–∏–ª–∞'
        ]

        found_pseudo = [p for p in pseudoscience_patterns if p.lower() in content.lower()]
        if found_pseudo:
            critical_flags.append(CriticalFlag.PSEUDOSCIENTIFIC_CLAIM)
            safety_issues.append(f"–ü—Å–µ–≤–¥–æ–Ω–∞—É—á–Ω—ã–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è: {', '.join(found_pseudo)}")
            safety_score -= 30

        # –í–æ–∑—Ä–∞—Å—Ç–Ω–∞—è –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å
        if target_audience == "children" or target_audience == "teens":
            adult_content_markers = [
                '–∏–Ω—Ç–∏–º–Ω—ã–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è', '—Å–µ–∫—Å—É–∞–ª—å–Ω–æ—Å—Ç—å', '–∞–ª–∫–æ–≥–æ–ª—å', '–Ω–∞—Ä–∫–æ—Ç–∏–∫–∏',
                '—Å—É–∏—Ü–∏–¥', '—Å–∞–º–æ–ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–µ', '–Ω–∞—Å–∏–ª–∏–µ'
            ]

            found_adult_content = [m for m in adult_content_markers if m.lower() in content.lower()]
            if found_adult_content:
                critical_flags.append(CriticalFlag.AGE_INAPPROPRIATE)
                safety_issues.append(f"–ù–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–π –¥–ª—è –≤–æ–∑—Ä–∞—Å—Ç–∞ –∫–æ–Ω—Ç–µ–Ω—Ç: {', '.join(found_adult_content)}")
                safety_score -= 40

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤—É
        legal_issues = []

        # –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –±–µ–∑ –ª–∏—Ü–µ–Ω–∑–∏–∏
        medical_claims = [
            '–ª–µ—á–∏—Ç—å –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–µ', '–º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑', '–∑–∞–º–µ–Ω–∞ —Ç–µ—Ä–∞–ø–∏–∏',
            '–ø—Å–∏—Ö–∏–∞—Ç—Ä–∏—á–µ—Å–∫–æ–µ –ª–µ—á–µ–Ω–∏–µ', '–∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ'
        ]

        found_medical = [c for c in medical_claims if c.lower() in content.lower()]
        if found_medical:
            critical_flags.append(CriticalFlag.LEGAL_VIOLATION)
            legal_issues.append(f"–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞: {', '.join(found_medical)}")
            safety_score -= 35

        if legal_issues:
            safety_issues.extend(legal_issues)

        # –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã (–±–æ–Ω—É—Å—ã –∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
        positive_elements = [
            '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '—ç—Ç–∏—á–Ω–æ—Å—Ç—å', '–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ—Å—Ç—å', '—É–≤–∞–∂–µ–Ω–∏–µ',
            '–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ', '–ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è'
        ]

        found_positive = [p for p in positive_elements if p.lower() in content.lower()]
        if found_positive:
            safety_score = min(100, safety_score + len(found_positive) * 2)

        return {
            "safety_score": max(0, safety_score),
            "critical_flags": [flag.value for flag in critical_flags],
            "safety_issues": safety_issues,
            "warnings": warnings,
            "positive_elements": found_positive,
            "is_safe_for_audience": len(critical_flags) == 0 and safety_score >= 70,
            "analysis_timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        return {
            "safety_score": 0,
            "critical_flags": [CriticalFlag.POTENTIALLY_HARMFUL.value],
            "safety_issues": [f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏: {e}"],
            "error": str(e),
            "is_safe_for_audience": False
        }


async def validate_nlp_techniques(
    ctx: RunContext[NLPQualityGuardianDependencies],
    content: str,
    technique_type: str = "general"
) -> Dict[str, Any]:
    """
    –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ NLP —Ç–µ—Ö–Ω–∏–∫ –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ.

    Args:
        content: –ö–æ–Ω—Ç–µ–Ω—Ç —Å NLP —Ç–µ—Ö–Ω–∏–∫–∞–º–∏
        technique_type: –¢–∏–ø —Ç–µ—Ö–Ω–∏–∫ (rapport, reframing, anchoring, etc.)

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏ NLP —Ç–µ—Ö–Ω–∏–∫
    """
    try:
        nlp_score = 100.0
        technique_issues = []
        found_techniques = []

        # –°–ª–æ–≤–∞—Ä—å NLP —Ç–µ—Ö–Ω–∏–∫ –∏ –∏—Ö –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π
        nlp_techniques = {
            "rapport": {
                "keywords": ["—Ä–∞–ø–ø–æ—Ä—Ç", "–ø–æ–¥—Å—Ç—Ä–æ–π–∫–∞", "rapport", "matching", "mirroring"],
                "correct_usage": ["–≤–∑–∞–∏–º–Ω–æ–µ —É–≤–∞–∂–µ–Ω–∏–µ", "–¥–æ–≤–µ—Ä–∏–µ", "–ø–æ–Ω–∏–º–∞–Ω–∏–µ"],
                "incorrect_usage": ["–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ", "–º–∞–Ω–∏–ø—É–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–æ–±–º–∞–Ω"]
            },
            "reframing": {
                "keywords": ["—Ä–µ—Ñ—Ä–µ–π–º–∏–Ω–≥", "–ø–µ—Ä–µ–æ—Å–º—ã—Å–ª–µ–Ω–∏–µ", "reframing", "recontextualization"],
                "correct_usage": ["–Ω–æ–≤–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞", "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–π –≤–∑–≥–ª—è–¥", "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ"],
                "incorrect_usage": ["–æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏", "–ª–æ–∂–Ω—ã–µ –æ–±–µ—â–∞–Ω–∏—è", "–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º"]
            },
            "anchoring": {
                "keywords": ["—è–∫–æ—Ä–µ–Ω–∏–µ", "—è–∫–æ—Ä—å", "anchoring", "anchor", "conditioning"],
                "correct_usage": ["—Ä–µ—Å—É—Ä—Å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ", "–ø–æ–∑–∏—Ç–∏–≤–Ω–∞—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏—è", "—ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞"],
                "incorrect_usage": ["–ø—Ä–∏–Ω—É–∂–¥–µ–Ω–∏–µ", "–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å", "–Ω–∞–≤—è–∑—ã–≤–∞–Ω–∏–µ"]
            },
            "submodalities": {
                "keywords": ["—Å—É–±–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–∏", "submodalities", "sensory"],
                "correct_usage": ["–∏–∑–º–µ–Ω–µ–Ω–∏–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è", "—Ä–µ—Å—É—Ä—Å–Ω—ã–µ –æ–±—Ä–∞–∑—ã", "–ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –æ—â—É—â–µ–Ω–∏—è"],
                "incorrect_usage": ["–±–æ–ª–µ–∑–Ω–µ–Ω–Ω—ã–µ –æ–±—Ä–∞–∑—ã", "—Ç—Ä–∞–≤–º–∞—Ç–∏–∑–∞—Ü–∏—è", "–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ"]
            },
            "timeline": {
                "keywords": ["–≤—Ä–µ–º–µ–Ω–Ω–∞—è –ª–∏–Ω–∏—è", "timeline", "past", "future", "–≤—Ä–µ–º—è"],
                "correct_usage": ["–∏—Å—Ü–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ–≥–æ", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –±—É–¥—É—â–µ–≥–æ", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –æ–ø—ã—Ç–∞"],
                "incorrect_usage": ["–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤", "–ª–æ–∂–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è", "–æ—Ç—Ä–∏—Ü–∞–Ω–∏–µ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏"]
            }
        }

        # –ü–æ–∏—Å–∫ –∏ –∞–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏–∫
        for technique_name, technique_info in nlp_techniques.items():
            technique_found = False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏
            for keyword in technique_info["keywords"]:
                if keyword.lower() in content.lower():
                    technique_found = True
                    found_techniques.append(technique_name)
                    break

            if technique_found:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
                correct_usage_found = any(
                    usage.lower() in content.lower()
                    for usage in technique_info["correct_usage"]
                )

                incorrect_usage_found = any(
                    usage.lower() in content.lower()
                    for usage in technique_info["incorrect_usage"]
                )

                if incorrect_usage_found:
                    technique_issues.append(
                        f"–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏–∫–∏ {technique_name}: "
                        f"–Ω–∞–π–¥–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã"
                    )
                    nlp_score -= 25
                elif not correct_usage_found:
                    technique_issues.append(
                        f"–¢–µ—Ö–Ω–∏–∫–∞ {technique_name} –Ω–µ –∏–º–µ–µ—Ç —á–µ—Ç–∫–æ–≥–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è"
                    )
                    nlp_score -= 10

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        ericksonian_patterns = {
            "truisms": ["–∫–∞–∂–¥—ã–π —á–µ–ª–æ–≤–µ–∫", "–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ", "–∫–∞–∫ –ø—Ä–∞–≤–∏–ª–æ", "–≤—Å–µ –∑–Ω–∞—é—Ç"],
            "presuppositions": ["–∫–æ–≥–¥–∞ –≤—ã", "–ø–æ –º–µ—Ä–µ —Ç–æ–≥–æ –∫–∞–∫", "–ø–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫"],
            "embedded_commands": ["–º–æ–∂–µ—Ç–µ —Ä–∞—Å—Å–ª–∞–±–∏—Ç—å—Å—è", "–ø–æ—á—É–≤—Å—Ç–≤—É–π—Ç–µ —Å–ø–æ–∫–æ–π—Å—Ç–≤–∏–µ", "–Ω–∞–π–¥–∏—Ç–µ –∫–æ–º—Ñ–æ—Ä—Ç"],
            "metaphors": ["–∫–∞–∫ –¥–µ—Ä–µ–≤–æ —Ä–∞—Å—Ç–µ—Ç", "—Å–ª–æ–≤–Ω–æ —Ä–µ–∫–∞ —Ç–µ—á–µ—Ç", "–ø–æ–¥–æ–±–Ω–æ —Å–æ–ª–Ω—Ü—É"]
        }

        found_ericksonian = []
        for pattern_type, patterns in ericksonian_patterns.items():
            for pattern in patterns:
                if pattern.lower() in content.lower():
                    found_ericksonian.append(pattern_type)
                    break

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø—Ä–∏–Ω—Ü–∏–ø–∞–º –≠—Ä–∏–∫—Å–æ–Ω–∞
        ericksonian_principles = [
            "utilization", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤", "indirect approach", "–Ω–µ–ø—Ä—è–º–æ–π –ø–æ–¥—Ö–æ–¥",
            "respect", "—É–≤–∞–∂–µ–Ω–∏–µ", "client pace", "—Ç–µ–º–ø –∫–ª–∏–µ–Ω—Ç–∞"
        ]

        ericksonian_compliance = any(
            principle.lower() in content.lower()
            for principle in ericksonian_principles
        )

        if found_ericksonian and not ericksonian_compliance:
            technique_issues.append(
                "–≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–µ–∑ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤"
            )
            nlp_score -= 20

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ç–µ—Ö–Ω–∏–∫
        evidence_keywords = [
            "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç", "–Ω–∞—É—á–Ω–æ –¥–æ–∫–∞–∑–∞–Ω–æ", "—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞",
            "–∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—ã—Ç–∞–Ω–∏—è", "peer-reviewed", "evidence-based"
        ]

        has_evidence_base = any(
            keyword.lower() in content.lower()
            for keyword in evidence_keywords
        )

        if found_techniques and not has_evidence_base and ctx.deps.validation_criteria.require_scientific_basis:
            technique_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ç–µ—Ö–Ω–∏–∫")
            nlp_score -= 15

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
        ethical_nlp_markers = [
            "–∏–Ω—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–≥–ª–∞—Å–∏–µ", "–¥–æ–±—Ä–æ–≤–æ–ª—å–Ω–æ–µ —É—á–∞—Å—Ç–∏–µ", "–ø—Ä–∞–≤–æ –æ—Ç–∫–∞–∑–∞",
            "–±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã", "—ç—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã", "—É–≤–∞–∂–µ–Ω–∏–µ –ª–∏—á–Ω–æ—Å—Ç–∏"
        ]

        has_ethical_framework = any(
            marker.lower() in content.lower()
            for marker in ethical_nlp_markers
        )

        if found_techniques and not has_ethical_framework:
            technique_issues.append("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —ç—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–º–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è NLP —Ç–µ—Ö–Ω–∏–∫")
            nlp_score -= 20

        return {
            "nlp_score": max(0, nlp_score),
            "found_techniques": found_techniques,
            "found_ericksonian_patterns": found_ericksonian,
            "technique_issues": technique_issues,
            "has_evidence_base": has_evidence_base,
            "has_ethical_framework": has_ethical_framework,
            "ericksonian_compliance": ericksonian_compliance,
            "techniques_count": len(found_techniques),
            "analysis_timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        return {
            "nlp_score": 0,
            "technique_issues": [f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ NLP —Ç–µ—Ö–Ω–∏–∫: {e}"],
            "error": str(e)
        }


async def generate_quality_report(
    ctx: RunContext[NLPQualityGuardianDependencies],
    validation_results: Dict[str, Any],
    content_info: Dict[str, Any]
) -> str:
    """
    –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞.

    Args:
        validation_results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        content_info: –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω—Ç–µ–Ω—Ç–µ

    Returns:
        –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown
    """
    try:
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        structure = validation_results.get("structure", {})
        safety = validation_results.get("safety", {})
        nlp = validation_results.get("nlp", {})

        # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π –±–∞–ª–ª
        scores = []
        if "structure_score" in structure:
            scores.append(structure["structure_score"])
        if "safety_score" in safety:
            scores.append(safety["safety_score"])
        if "nlp_score" in nlp:
            scores.append(nlp["nlp_score"])

        overall_score = sum(scores) / len(scores) if scores else 0

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞
        if overall_score >= 90:
            quality_level = "üü¢ –û–¢–õ–ò–ß–ù–û"
            recommendation = "–ì–æ—Ç–æ–≤–æ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"
        elif overall_score >= 70:
            quality_level = "üü° –•–û–†–û–®–û"
            recommendation = "–ù–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –¥–æ—Ä–∞–±–æ—Ç–∫–∏"
        elif overall_score >= 50:
            quality_level = "üü† –¢–†–ï–ë–£–ï–¢ –£–õ–£–ß–®–ï–ù–ò–Ø"
            recommendation = "–°—É—â–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–∞–≤–∫–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã"
        else:
            quality_level = "üî¥ –ù–ï–ü–†–ò–ï–ú–õ–ï–ú–û"
            recommendation = "–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è"

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report = f"""# üõ°Ô∏è –û—Ç—á–µ—Ç –æ –∫–∞—á–µ—Å—Ç–≤–µ NLP –∫–æ–Ω—Ç–µ–Ω—Ç–∞

## üìä –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞

**–ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª:** {overall_score:.1f}/100
**–£—Ä–æ–≤–µ–Ω—å –∫–∞—á–µ—Å—Ç–≤–∞:** {quality_level}
**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** {recommendation}

---

## üèóÔ∏è –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è

**–ë–∞–ª–ª:** {structure.get('structure_score', 0):.1f}/100

### –ù–∞–π–¥–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:
- **–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞:** {structure.get('content_length', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')} —Å–∏–º–≤–æ–ª–æ–≤
- **–ó–∞–≥–æ–ª–æ–≤–∫–æ–≤ –Ω–∞–π–¥–µ–Ω–æ:** {structure.get('headers_found', 0)}
"""

        if content_info.get("content_type") == "test":
            report += f"- **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤:** {structure.get('question_count', 0)}\n"

        if structure.get('issues'):
            report += f"\n### ‚ö†Ô∏è –°—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:\n"
            for issue in structure['issues']:
                report += f"- {issue}\n"

        report += f"""
---

## üîê –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —ç—Ç–∏—á–Ω–æ—Å—Ç—å

**–ë–∞–ª–ª:** {safety.get('safety_score', 0):.1f}/100
**–ë–µ–∑–æ–ø–∞—Å–Ω–æ –¥–ª—è –∞—É–¥–∏—Ç–æ—Ä–∏–∏:** {'‚úÖ –î–∞' if safety.get('is_safe_for_audience', False) else '‚ùå –ù–µ—Ç'}

"""

        if safety.get('critical_flags'):
            report += "### üö® –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´:\n"
            for flag in safety['critical_flags']:
                report += f"- ‚ö†Ô∏è {flag}\n"
            report += "\n"

        if safety.get('safety_issues'):
            report += "### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏:\n"
            for issue in safety['safety_issues']:
                report += f"- {issue}\n"
            report += "\n"

        if safety.get('positive_elements'):
            report += "### ‚úÖ –ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã:\n"
            for element in safety['positive_elements']:
                report += f"- {element}\n"
            report += "\n"

        report += f"""---

## üéØ –ö–∞—á–µ—Å—Ç–≤–æ NLP —Ç–µ—Ö–Ω–∏–∫

**–ë–∞–ª–ª:** {nlp.get('nlp_score', 0):.1f}/100
**–ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ö–Ω–∏–∫:** {nlp.get('techniques_count', 0)}

"""

        if nlp.get('found_techniques'):
            report += "### –û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ NLP —Ç–µ—Ö–Ω–∏–∫–∏:\n"
            for technique in nlp['found_techniques']:
                report += f"- {technique}\n"
            report += "\n"

        if nlp.get('found_ericksonian_patterns'):
            report += "### –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã:\n"
            for pattern in nlp['found_ericksonian_patterns']:
                report += f"- {pattern}\n"
            report += "\n"

        report += f"""**–ù–∞—É—á–Ω–∞—è –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç—å:** {'‚úÖ –ï—Å—Ç—å' if nlp.get('has_evidence_base', False) else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}
**–≠—Ç–∏—á–µ—Å–∫–∞—è —Ä–∞–º–∫–∞:** {'‚úÖ –ï—Å—Ç—å' if nlp.get('has_ethical_framework', False) else '‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç'}
**–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–∞–º –≠—Ä–∏–∫—Å–æ–Ω–∞:** {'‚úÖ –î–∞' if nlp.get('ericksonian_compliance', False) else '‚ùå –ù–µ—Ç'}

"""

        if nlp.get('technique_issues'):
            report += "### ‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º—ã —Å —Ç–µ—Ö–Ω–∏–∫–∞–º–∏:\n"
            for issue in nlp['technique_issues']:
                report += f"- {issue}\n"
            report += "\n"

        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        all_issues = []
        all_issues.extend(structure.get('issues', []))
        all_issues.extend(safety.get('safety_issues', []))
        all_issues.extend(nlp.get('technique_issues', []))

        if all_issues:
            report += """---

## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è:
"""
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
            critical_issues = [issue for issue in all_issues if any(
                word in issue.lower() for word in ['–∫—Ä–∏—Ç–∏—á–Ω–æ', '–æ–ø–∞—Å–Ω–æ', '–≤—Ä–µ–¥–Ω–æ', '–∑–∞–ø—Ä–µ—â–µ–Ω–æ']
            )]

            for issue in critical_issues[:5]:  # –¢–æ–ø-5 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö
                report += f"1. **–ö–†–ò–¢–ò–ß–ù–û:** {issue}\n"

            report += "\n### –í–∞–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:\n"
            regular_issues = [issue for issue in all_issues if issue not in critical_issues]
            for i, issue in enumerate(regular_issues[:10], 1):  # –¢–æ–ø-10 –æ–±—ã—á–Ω—ã—Ö
                report += f"{i}. {issue}\n"

        report += f"""
---

## üìà –î–µ—Ç–∞–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞

### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∞–ª–ª–æ–≤:
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞:** {structure.get('structure_score', 0):.1f}%
- **–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å:** {safety.get('safety_score', 0):.1f}%
- **NLP —Ç–µ—Ö–Ω–∏–∫–∏:** {nlp.get('nlp_score', 0):.1f}%

### –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–µ—Ç–∞–ª–∏:
- **–î–æ–º–µ–Ω –≤–∞–ª–∏–¥–∞—Ü–∏–∏:** {ctx.deps.validation_domain.value}
- **–¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞:** {content_info.get('content_type', 'mixed')}
- **–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **–ê–≥–µ–Ω—Ç:** NLP Content Quality Guardian v1.0.0

---

## üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏

"""

        if overall_score >= 90:
            report += """‚úÖ **–ö–æ–Ω—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤ –∫ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏!**
- –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
- –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
"""
        elif overall_score >= 70:
            report += """‚ö†Ô∏è **–¢—Ä–µ–±—É–µ—Ç—Å—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞:**
1. –ò—Å–ø—Ä–∞–≤–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –≤—ã—à–µ –ø—Ä–æ–±–ª–µ–º—ã
2. –ü—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–≤—Ç–æ—Ä–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
3. –ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
"""
        elif overall_score >= 50:
            report += """üîß **–ù–µ–æ–±—Ö–æ–¥–∏–º–∞ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞:**
1. –£—Å—Ç—Ä–∞–Ω–∏—Ç—å –≤—Å–µ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
2. –ü–µ—Ä–µ—Ä–∞–±–æ—Ç–∞—Ç—å –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã
3. –î–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
4. –ü—Ä–æ–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω—É—é –ø–æ–≤—Ç–æ—Ä–Ω—É—é –≤–∞–ª–∏–¥–∞—Ü–∏—é
"""
        else:
            report += """üö´ **–ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é:**
1. –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –∑–∞–º–µ—á–∞–Ω–∏–π
2. –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –ø–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –∏ NLP
3. –ü–µ—Ä–µ—Å–º–æ—Ç—Ä —Ü–µ–ª–µ–π –∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏
4. –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –Ω–∞ –±–∞–∑–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞
"""

        report += f"""
---

*–û—Ç—á–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω NLP Content Quality Guardian Agent*
*–î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y %H:%M')}*
"""

        return report

    except Exception as e:
        return f"""# ‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–∞

–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞ –æ –∫–∞—á–µ—Å—Ç–≤–µ: {e}

## –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
- **–û–±—â–∏–π –±–∞–ª–ª:** {overall_score:.1f}/100 (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
- **–í—Ä–µ–º—è –∞–Ω–∞–ª–∏–∑–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—É.
"""


async def break_down_validation_tasks(
    ctx: RunContext[NLPQualityGuardianDependencies],
    content_description: str,
    validation_scope: str = "full"
) -> str:
    """
    –†–∞–∑–±–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –Ω–∞ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –∏ –ø–æ–∫–∞–∑–∞—Ç—å –∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.

    Args:
        content_description: –û–ø–∏—Å–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        validation_scope: –û–±—ä–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (basic, full, expert)

    Returns:
        –°–ø–∏—Å–æ–∫ –º–∏–∫—Ä–æ–∑–∞–¥–∞—á –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    """
    try:
        if validation_scope == "basic":
            microtasks = [
                "–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏ –±–∞–∑–æ–≤—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π",
                "–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–Ω–¥–∞—Ä—Ç–æ–≤",
                "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"
            ]
        elif validation_scope == "expert":
            microtasks = [
                f"–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_description}",
                "–ü–æ–∏—Å–∫ –≤ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏",
                "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ PatternShift 2.0",
                "–î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è NLP —Ç–µ—Ö–Ω–∏–∫ –∏ –≠—Ä–∏–∫—Å–æ–Ω–æ–≤—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤",
                "–ê–Ω–∞–ª–∏–∑ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ –∏ –Ω–∞—É—á–Ω–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏",
                "–ü—Ä–æ–≤–µ—Ä–∫–∞ —ç—Ç–∏—á–µ—Å–∫–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –≤–æ–∑—Ä–∞—Å—Ç–Ω–æ–π –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç–∏",
                "–ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∫—É–ª—å—Ç—É—Ä–Ω–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å",
                "–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –∑–∞–∫–ª—é—á–µ–Ω–∏—è",
                "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ",
                "–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
            ]
        else:  # full
            microtasks = [
                f"–ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ —Ç–∏–ø–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {content_description}",
                "–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏",
                "–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è PatternShift –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏",
                "–í–∞–ª–∏–¥–∞—Ü–∏—è NLP —Ç–µ—Ö–Ω–∏–∫ –∏ –∏—Ö –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è",
                "–ê–Ω–∞–ª–∏–∑ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ —ç—Ç–∏—á–µ—Å–∫–∏—Ö –∞—Å–ø–µ–∫—Ç–æ–≤",
                "–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—É—á–Ω–æ–π –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π",
                "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏",
                "–†–µ—Ñ–ª–µ–∫—Å–∏—è –∏ —É–ª—É—á—à–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏"
            ]

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤—ã–≤–æ–¥ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        output = "üìã **–ú–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞:**\n"
        for i, task in enumerate(microtasks, 1):
            output += f"{i}. {task}\n"

        output += f"\n‚úÖ –ë—É–¥—É –æ—Ç—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –∫–∞–∂–¥–æ–π –º–∏–∫—Ä–æ–∑–∞–¥–∞—á–∏"
        output += f"\nüéØ **–û–±—ä–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏:** {validation_scope}"
        output += f"\nüîç **–î–æ–º–µ–Ω:** {ctx.deps.validation_domain.value}"

        return output

    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}"


async def delegate_validation_task(
    ctx: RunContext[NLPQualityGuardianDependencies],
    target_agent: str,
    task_title: str,
    task_description: str,
    content_sample: str = "",
    priority: str = "medium"
) -> str:
    """
    –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å –∑–∞–¥–∞—á—É –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥—Ä—É–≥–æ–º—É —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞–≥–µ–Ω—Ç—É.

    Args:
        target_agent: –¶–µ–ª–µ–≤–æ–π –∞–≥–µ–Ω—Ç (security_audit, nlp_research, etc.)
        task_title: –ù–∞–∑–≤–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        task_description: –û–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        content_sample: –û–±—Ä–∞–∑–µ—Ü –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        priority: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∑–∞–¥–∞—á–∏

    Returns:
        –†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    """
    try:
        # –ú–∞–ø–∏–Ω–≥ –∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        agent_mapping = {
            "security_audit": "Security Audit Agent",
            "nlp_research": "NLP Content Research Agent",
            "psychology_expert": "Psychology Content Architect Agent",
            "quality_guardian": "Archon Quality Guardian",
            "analysis_lead": "Archon Analysis Lead"
        }

        assignee = agent_mapping.get(target_agent, "Archon Analysis Lead")

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        context_data = {
            "validation_domain": ctx.deps.validation_domain.value,
            "content_type": ctx.deps.target_content_type.value,
            "delegated_by": "NLP Content Quality Guardian Agent",
            "content_sample": content_sample[:500] if content_sample else "–ù–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–æ",
            "validation_criteria": "–°–º. –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π NLP Quality Guardian"
        }

        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –≤ Archon (–∏–º–∏—Ç–∞—Ü–∏—è, —Ç–∞–∫ –∫–∞–∫ MCP –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
        task_result = {
            "task_id": f"validation_delegate_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "assigned_to": assignee,
            "status": "created",
            "priority": priority
        }

        return f"""‚úÖ –ó–∞–¥–∞—á–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —É—Å–ø–µ—à–Ω–æ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∞

**–¶–µ–ª–µ–≤–æ–π –∞–≥–µ–Ω—Ç:** {target_agent} ({assignee})
**–ó–∞–¥–∞—á–∞:** {task_title}
**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç:** {priority}
**–ö–æ–Ω—Ç–µ–∫—Å—Ç:** {task_description}

**–ü–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**
- –î–æ–º–µ–Ω –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {context_data['validation_domain']}
- –¢–∏–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {context_data['content_type']}
- –û–±—Ä–∞–∑–µ—Ü –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {'–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω' if content_sample else '–ù–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω'}

üìã –ó–∞–¥–∞—á–∞ —Å–æ–∑–¥–∞–Ω–∞ –≤ —Å–∏—Å—Ç–µ–º–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á–∞–º–∏
üîÑ –û–∂–∏–¥–∞–µ—Ç—Å—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º
üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ –æ–±—â–∏–π –æ—Ç—á–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏
"""

    except Exception as e:
        return f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}"