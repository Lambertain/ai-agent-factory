# ===== LLM Configuration =====
# Provider: openai, anthropic, gemini, ollama, etc.
LLM_PROVIDER=openai
# Your LLM API key (for Qwen via OpenAI-compatible API)
LLM_API_KEY=your-qwen-api-key-here
# LLM to use for the agents (Qwen3 Coder - latest model)
LLM_CHOICE=qwen3-coder
# Base URL for Qwen API (compatible with OpenAI API format)
LLM_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# ===== Cost-Optimized Task-specific Model Configuration =====
# Complex planning and architectural analysis - Premium Qwen
LLM_ARCHITECTURE_MODEL=qwen2.5-72b-instruct
LLM_ARCHITECTURE_PROVIDER=openai
LLM_ARCHITECTURE_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# Code implementation - Latest Qwen3 Coder (480B MoE, 1M context)
LLM_CODING_MODEL=qwen3-coder
LLM_CODING_PROVIDER=openai
LLM_CODING_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# Testing and code validation - Lightweight Qwen Coder
LLM_TESTING_MODEL=qwen2.5-coder-7b-instruct
LLM_TESTING_PROVIDER=openai
LLM_TESTING_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# ===== DeepSeek V3 for Fast Operations (Testing) =====
# Fast coding tasks - DeepSeek V3 (60 tokens/sec, low cost)
LLM_FAST_CODING_MODEL=deepseek-v3
LLM_FAST_CODING_PROVIDER=openai
LLM_FAST_CODING_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1

# ===== COST-OPTIMIZED: Gemini for Text Tasks =====
# Basic planning, documentation, prompt engineering - Ultra-cheap Gemini
LLM_PLANNING_MODEL=gemini-2.5-flash-lite
LLM_PLANNING_PROVIDER=gemini
LLM_PLANNING_API_KEY=your-gemini-api-key-here

# Documentation generation - Ultra-cheap Gemini
LLM_DOCS_MODEL=gemini-2.5-flash-lite
LLM_DOCS_PROVIDER=gemini
LLM_DOCS_API_KEY=your-gemini-api-key-here

# Validation and QA - Ultra-cheap Gemini
LLM_VALIDATION_MODEL=gemini-2.5-flash-lite
LLM_VALIDATION_PROVIDER=gemini
LLM_VALIDATION_API_KEY=your-gemini-api-key-here

# Prompt engineering - Ultra-cheap Gemini
LLM_PROMPTS_MODEL=gemini-2.5-flash-lite
LLM_PROMPTS_PROVIDER=gemini
LLM_PROMPTS_API_KEY=your-gemini-api-key-here

# ===== QUALITY-FIRST COST OPTIMIZATION =====
# PRINCIPLE: Quality > Cost Savings
# Only optimize costs if quality >= 95% of baseline

# Quality monitoring and automatic fallback
ENABLE_QUALITY_MONITORING=true
MIN_QUALITY_THRESHOLD=95
AUTO_FALLBACK_ON_QUALITY_DROP=true

# A/B testing mode (compare Gemini vs Qwen quality)
AB_TESTING_MODE=true
AB_TESTING_SAMPLE_RATE=0.1
QUALITY_BASELINE_MODEL=qwen2.5-72b-instruct

# Conservative optimization settings
# Enable Batch API for Gemini (only after quality validation)
GEMINI_USE_BATCH_API=false  # Start with false for testing
GEMINI_BATCH_ENDPOINT=https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:batchGenerate

# Enable context caching for Qwen (safe optimization)
QWEN_ENABLE_CONTEXT_CACHE=true

# Response streaming (quality neutral)
ENABLE_STREAMING=true

# Conservative context compression (quality first)
AUTO_COMPRESS_CONTEXT=false  # Start disabled for quality validation
MAX_CONTEXT_TOKENS=8000  # Higher limit to preserve quality
COMPRESSION_RATIO=0.8  # More conservative compression

# Request caching (quality neutral)
ENABLE_REQUEST_CACHE=true
CACHE_TTL_MINUTES=15  # Shorter cache to ensure freshness

# Quality-aware smart routing
ENABLE_SMART_ROUTING=true
QUALITY_THRESHOLD_RATIO=0.95  # Only route if 95% quality maintained
COST_THRESHOLD_RATIO=0.8

# ===== QUALITY-FIRST FALLBACK CONFIGURATION =====
# Conservative approach: Start with proven models, fallback to alternatives
ENABLE_MODEL_FALLBACK=true

# Documentation fallback: Conservative progression
# Phase 1: Test Gemini quality vs Qwen baseline
DOCS_FALLBACK_MODELS=qwen2.5-32b-instruct,gemini-2.5-flash-lite,gemini-1.5-flash
DOCS_AB_TEST_PRIMARY=gemini-2.5-flash-lite
DOCS_AB_TEST_BASELINE=qwen2.5-32b-instruct

# Planning fallback: Keep quality-critical on Qwen initially
# Only switch to Gemini if A/B testing shows >=95% quality
PLANNING_FALLBACK_MODELS=qwen2.5-72b-instruct,qwen2.5-32b-instruct,gemini-2.5-flash-lite
PLANNING_AB_TEST_PRIMARY=gemini-2.5-flash-lite
PLANNING_AB_TEST_BASELINE=qwen2.5-72b-instruct

# Validation fallback: Conservative - start with Qwen
VALIDATION_FALLBACK_MODELS=qwen2.5-32b-instruct,gemini-2.5-flash-lite,gemini-2.5-flash
VALIDATION_AB_TEST_PRIMARY=gemini-2.5-flash-lite
VALIDATION_AB_TEST_BASELINE=qwen2.5-32b-instruct

# Coding tasks: NEVER optimize - keep on specialized models
CODING_FALLBACK_MODELS=qwen3-coder,deepseek-v3,qwen2.5-coder-32b-instruct,qwen2.5-coder-7b-instruct
ARCHITECTURE_FALLBACK_MODELS=qwen2.5-72b-instruct,qwen2.5-32b-instruct

# ===== A/B TESTING CONFIGURATION (1M FREE TOKENS) =====
# Perfect opportunity to test and optimize model selection
ENABLE_AB_TESTING=true
AB_TEST_SAMPLE_RATE=0.3

# Model comparison pairs for testing
CODING_AB_TEST_PAIRS=qwen3-coder,deepseek-v3
FAST_TASKS_AB_TEST_PAIRS=deepseek-v3,qwen2.5-coder-7b-instruct
ARCHITECTURE_AB_TEST_PAIRS=qwen2.5-72b-instruct,qwen3-coder

# Quality thresholds for model selection
MIN_QUALITY_SCORE=85
AUTO_PROMOTE_WINNER=true
AB_TEST_METRICS_LOG=true